{
    "repository": "HiveRunner/HiveRunner",
    "clone_url": "https://github.com/HiveRunner/HiveRunner.git",
    "timestamp": "2023-05-28T18:01:06.914412Z",
    "clone_success": true,
    "number of actions": 3,
    "number_of_test_actions": 1,
    "actions_successful": false,
    "actions_stdout": "[build/Package and run all tests] \ud83d\ude80  Start image=crawlergpt:latest\n[build/Package and run all tests]   \ud83d\udc33  docker pull image=crawlergpt:latest platform= username= forcePull=false\n[build/Package and run all tests]   \ud83d\udc33  docker create image=crawlergpt:latest platform= entrypoint=[\"tail\" \"-f\" \"/dev/null\"] cmd=[]\n[build/Package and run all tests]   \ud83d\udc33  docker run image=crawlergpt:latest platform= entrypoint=[\"tail\" \"-f\" \"/dev/null\"] cmd=[]\n[build/Package and run all tests]   \ud83d\udc33  docker exec cmd=[chown -R 1012:1000 /tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner] user=0 workdir=\n[build/Package and run all tests]   \u2601  git clone 'https://github.com/actions/setup-java' # ref=v2\n[build/Package and run all tests] Non-terminating error while running 'git clone': some refs were not updated\n[build/Package and run all tests] \u2b50 Run Main actions/checkout@v2\n[build/Package and run all tests]   \u2705  Success - Main actions/checkout@v2\n[build/Package and run all tests] \u2b50 Run Main Set up JDK\n[build/Package and run all tests]   \ud83d\udc33  docker cp src=/home/andre-silva/.cache/act/actions-setup-java@v2/ dst=/var/run/act/actions/actions-setup-java@v2/\n[build/Package and run all tests]   \ud83d\udc33  docker exec cmd=[chown -R 1012:1000 /var/run/act/actions/actions-setup-java@v2/] user=0 workdir=\n[build/Package and run all tests]   \ud83d\udc33  docker exec cmd=[node /var/run/act/actions/actions-setup-java@v2/dist/setup/index.js] user= workdir=\n[build/Package and run all tests]   \ud83d\udcac  ::debug::isExplicit: 11.0.11-9\n[build/Package and run all tests]   \ud83d\udcac  ::debug::explicit? true\n[build/Package and run all tests]   \ud83d\udcac  ::debug::isExplicit: 8.0.292-1\n[build/Package and run all tests]   \ud83d\udcac  ::debug::explicit? true\n[build/Package and run all tests]   | Resolved Java 8.0.292+1 from tool-cache\n[build/Package and run all tests]   | Setting Java 8.0.292+1 as the default\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | Java configuration:\n[build/Package and run all tests]   |   Distribution: adopt\n[build/Package and run all tests]   |   Version: 8.0.292+1\n[build/Package and run all tests]   |   Path: /opt/hostedtoolcache/Java_Adopt_jdk/8.0.292-1/x64\n[build/Package and run all tests]   | \n[build/Package and run all tests]   \u2753 add-matcher /run/act/actions/actions-setup-java@v2/.github/java.json\n[build/Package and run all tests]   | Creating settings.xml with server-id: github\n[build/Package and run all tests]   | Writing to /home/runneradmin/.m2/settings.xml\n[build/Package and run all tests]   \u2705  Success - Main Set up JDK\n[build/Package and run all tests]   \u2699  ::set-env:: JAVA_HOME=/opt/hostedtoolcache/Java_Adopt_jdk/8.0.292-1/x64\n[build/Package and run all tests]   \u2699  ::set-output:: distribution=Adopt-Hotspot\n[build/Package and run all tests]   \u2699  ::set-output:: path=/opt/hostedtoolcache/Java_Adopt_jdk/8.0.292-1/x64\n[build/Package and run all tests]   \u2699  ::set-output:: version=8.0.292+1\n[build/Package and run all tests]   \u2699  ::add-path:: /opt/hostedtoolcache/Java_Adopt_jdk/8.0.292-1/x64/bin\n[build/Package and run all tests] \u2b50 Run Main Run Maven Targets\n[build/Package and run all tests]   \ud83d\udc33  docker exec cmd=[bash --noprofile --norc -e -o pipefail /var/run/act/workflow/2] user= workdir=\n[build/Package and run all tests]   | Apache Maven 3.8.2 (ea98e05a04480131370aa0c110b8c54cf726c06f)\n[build/Package and run all tests]   | Maven home: /usr/share/apache-maven-3.8.2\n[build/Package and run all tests]   | Java version: 1.8.0_292, vendor: AdoptOpenJDK, runtime: /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre\n[build/Package and run all tests]   | Default locale: en, platform encoding: UTF-8\n[build/Package and run all tests]   | OS name: \"linux\", version: \"5.4.0-146-generic\", arch: \"amd64\", family: \"unix\"\n[build/Package and run all tests]   | [INFO] Scanning for projects...\n[build/Package and run all tests]   | [INFO] Inspecting build with total of 1 modules...\n[build/Package and run all tests]   | [INFO] Installing Nexus Staging features:\n[build/Package and run all tests]   | [INFO]   ... total of 1 executions of maven-deploy-plugin replaced with nexus-staging-maven-plugin\n[build/Package and run all tests]   | [INFO] \n[build/Package and run all tests]   | [INFO] ------------------< io.github.hiverunner:hiverunner >-------------------\n[build/Package and run all tests]   | [INFO] Building HiveRunner 6.1.1-SNAPSHOT\n[build/Package and run all tests]   | [INFO] --------------------------------[ jar ]---------------------------------\n[build/Package and run all tests]   | [INFO] \n[build/Package and run all tests]   | [INFO] --- license-maven-plugin:3.0:format (default) @ hiverunner ---\n[build/Package and run all tests]   | [INFO] Updating license headers...\n[build/Package and run all tests]   | [INFO] \n[build/Package and run all tests]   | [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hiverunner ---\n[build/Package and run all tests]   | [INFO] Using 'UTF-8' encoding to copy filtered resources.\n[build/Package and run all tests]   | [INFO] skip non existing resourceDirectory /tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/src/main/resources\n[build/Package and run all tests]   | [INFO] \n[build/Package and run all tests]   | [INFO] --- maven-compiler-plugin:3.7.0:compile (default-compile) @ hiverunner ---\n[build/Package and run all tests]   | [INFO] Changes detected - recompiling the module!\n[build/Package and run all tests]   | [INFO] Compiling 58 source files to /tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/classes\n[build/Package and run all tests]   | [INFO] /tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/src/main/java/com/klarna/hiverunner/data/Converters.java: Some input files use or override a deprecated API.\n[build/Package and run all tests]   | [INFO] /tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/src/main/java/com/klarna/hiverunner/data/Converters.java: Recompile with -Xlint:deprecation for details.\n[build/Package and run all tests]   | [INFO] /tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/src/main/java/com/klarna/hiverunner/builder/HiveShellBuilder.java: Some input files use unchecked or unsafe operations.\n[build/Package and run all tests]   | [INFO] /tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/src/main/java/com/klarna/hiverunner/builder/HiveShellBuilder.java: Recompile with -Xlint:unchecked for details.\n[build/Package and run all tests]   | [INFO] \n[build/Package and run all tests]   | [INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hiverunner ---\n[build/Package and run all tests]   | [INFO] Using 'UTF-8' encoding to copy filtered resources.\n[build/Package and run all tests]   | [INFO] Copying 32 resources\n[build/Package and run all tests]   | [INFO] \n[build/Package and run all tests]   | [INFO] --- maven-compiler-plugin:3.7.0:testCompile (default-testCompile) @ hiverunner ---\n[build/Package and run all tests]   | [INFO] Changes detected - recompiling the module!\n[build/Package and run all tests]   | [INFO] Compiling 79 source files to /tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/test-classes\n[build/Package and run all tests]   | [INFO] /tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/src/test/java/com/klarna/hiverunner/sql/cli/beeline/SqlLineCommandRuleTest.java: Some input files use or override a deprecated API.\n[build/Package and run all tests]   | [INFO] /tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/src/test/java/com/klarna/hiverunner/sql/cli/beeline/SqlLineCommandRuleTest.java: Recompile with -Xlint:deprecation for details.\n[build/Package and run all tests]   | [INFO] /tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/src/test/java/com/klarna/hiverunner/examples/junit4/HelloAnnotatedHiveRunnerTest.java: Some input files use unchecked or unsafe operations.\n[build/Package and run all tests]   | [INFO] /tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/src/test/java/com/klarna/hiverunner/examples/junit4/HelloAnnotatedHiveRunnerTest.java: Recompile with -Xlint:unchecked for details.\n[build/Package and run all tests]   | [INFO] \n[build/Package and run all tests]   | [INFO] --- maven-surefire-plugin:2.22.2:test (default-test) @ hiverunner ---\n[build/Package and run all tests]   | [INFO] \n[build/Package and run all tests]   | [INFO] -------------------------------------------------------\n[build/Package and run all tests]   | [INFO]  T E S T S\n[build/Package and run all tests]   | [INFO] -------------------------------------------------------\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.ExecuteScriptIntegrationTest\n[build/Package and run all tests]   | 2023-05-28T18:02:08,585 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | 2023-05-28T18:02:08,607 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.ExecuteScriptIntegrationTest in /\n[build/Package and run all tests]   | Hive Session ID = 2bb44861-3a39-4f81-8ffc-d8aed128b840\n[build/Package and run all tests]   | 2023-05-28T18:02:09,697 INFO  SessionState:1227 - Hive Session ID = 2bb44861-3a39-4f81-8ffc-d8aed128b840\n[build/Package and run all tests]   | 2023-05-28T18:02:10,341 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:02:11,324 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:02:12,925 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:02:16,568 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:02:16,569 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:02:16,741 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 7c8f03d9-f0b0-459d-a9b3-84d6a1eff753\n[build/Package and run all tests]   | 2023-05-28T18:02:17,357 INFO  SessionState:1227 - Hive Session ID = 7c8f03d9-f0b0-459d-a9b3-84d6a1eff753\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:02:19,246 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.test_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:02:20,271 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180219_ac569287-58d3-4653-938f-9bb9cf7c0745\n[build/Package and run all tests]   | Total jobs = 1\n[build/Package and run all tests]   | Launching Job 1 out of 1\n[build/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[build/Package and run all tests]   | 2023-05-28T18:02:20,642 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[build/Package and run all tests]   | 2023-05-28T18:02:20,663 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[build/Package and run all tests]   | 2023-05-28T18:02:20,681 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[build/Package and run all tests]   | 2023-05-28T18:02:20,682 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[build/Package and run all tests]   | 2023-05-28T18:02:20,715 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:02:20,805 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:02:21,081 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:02:21,120 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:02:21,160 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:02:21,234 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local1734675315_0001\n[build/Package and run all tests]   | 2023-05-28T18:02:21,235 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:02:21,569 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:02:21,573 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:02:21,575 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:02:21,586 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:02:21,591 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local1734675315_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:02:21,640 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:02:21,650 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_tests2004428780449793508/localscratchdir2181151082362953430/98d5fe49-8d3d-4c92-8007-6b779e740736/hive_2023-05-28_18-02-19_617_6797832094938533985-1/dummy_path/dummy_file:0+1InputFormatClass: org.apache.hadoop.hive.ql.io.NullRowsInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:02:21,726 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[build/Package and run all tests]   | 2023-05-28T18:02:21,727 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[build/Package and run all tests]   | 2023-05-28T18:02:21,727 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[build/Package and run all tests]   | 2023-05-28T18:02:21,727 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[build/Package and run all tests]   | 2023-05-28T18:02:21,751 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[build/Package and run all tests]   | 2023-05-28T18:02:21,752 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[build/Package and run all tests]   | 2023-05-28T18:02:21,771 INFO  org.apache.orc.impl.MemoryManagerImpl:85 - orc.rows.between.memory.checks=5000\n[build/Package and run all tests]   | 2023-05-28T18:02:21,886 INFO  org.apache.orc.impl.PhysicalFsWriter:92 - ORC writer created for path: file:/tmp/hiverunner_tests2004428780449793508/warehouse5875778672626774546/test_db.db/test_table/.hive-staging_hive_2023-05-28_18-02-19_617_6797832094938533985-1/_task_tmp.-ext-10002/_tmp.000000_0 with stripeSize: 67108864 blockSize: 268435456 compression: ZLIB bufferSize: 262144\n[build/Package and run all tests]   | 2023-05-28T18:02:21,904 INFO  org.apache.orc.impl.OrcCodecPool:56 - Got brand-new codec ZLIB\n[build/Package and run all tests]   | 2023-05-28T18:02:21,941 INFO  org.apache.orc.impl.WriterImpl:188 - ORC writer created for path: file:/tmp/hiverunner_tests2004428780449793508/warehouse5875778672626774546/test_db.db/test_table/.hive-staging_hive_2023-05-28_18-02-19_617_6797832094938533985-1/_task_tmp.-ext-10002/_tmp.000000_0 with stripeSize: 67108864 blockSize: 268435456 compression: ZLIB bufferSize: 262144\n[build/Package and run all tests]   | 2023-05-28T18:02:22,012 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:02:22,020 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1734675315_0001_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:02:22,023 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - map\n[build/Package and run all tests]   | 2023-05-28T18:02:22,023 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1734675315_0001_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:02:22,028 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1734675315_0001_m_000000_0: Counters: 25\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=40624366\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=41754664\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=4\n[build/Package and run all tests]   | \t\tMap output records=0\n[build/Package and run all tests]   | \t\tInput split bytes=353\n[build/Package and run all tests]   | \t\tSpilled Records=0\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=64\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2001207296\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_1_test_db.test_table=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_3=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_1=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_UDTF_2=1\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:02:22,028 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local1734675315_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:02:22,031 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:02:22,591 Stage-1 map = 100%,  reduce = 0%\n[build/Package and run all tests]   | Ended Job = job_local1734675315_0001\n[build/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[build/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[build/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_tests2004428780449793508/warehouse5875778672626774546/test_db.db/test_table/.hive-staging_hive_2023-05-28_18-02-19_617_6797832094938533985-1/-ext-10000\n[build/Package and run all tests]   | Loading data to table test_db.test_table\n[build/Package and run all tests]   | 2023-05-28T18:02:22,621 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:02:22,819 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:02:22,883 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:02:22,894 ERROR org.apache.hadoop.hive.ql.io.AcidUtils:1003 - Failed to get files with ID; using regular API: Only supported for DFS; got class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem\n[build/Package and run all tests]   | 2023-05-28T18:02:22,991 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.ExecuteScriptIntegrationTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:02:23,040 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.543 s - in com.klarna.hiverunner.ExecuteScriptIntegrationTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.HiveShellBeeLineEmulationTest\n[build/Package and run all tests]   | 2023-05-28T18:02:25,612 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | Hive Session ID = 64619d4f-3b14-42aa-a6d1-362664d78201\n[build/Package and run all tests]   | 2023-05-28T18:02:26,111 INFO  SessionState:1227 - Hive Session ID = 64619d4f-3b14-42aa-a6d1-362664d78201\n[build/Package and run all tests]   | 2023-05-28T18:02:26,673 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:02:27,592 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:02:29,175 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:02:30,349 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:02:33,756 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:02:33,757 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:02:33,914 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 9cfcef0b-8cab-49df-8cf4-3d8ec209622b\n[build/Package and run all tests]   | 2023-05-28T18:02:34,488 INFO  SessionState:1227 - Hive Session ID = 9cfcef0b-8cab-49df-8cf4-3d8ec209622b\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:02:35,964 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellBeeLineEmulationTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:02:36,025 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = a8939c9a-536a-4c1a-a428-2b8671e62557\n[build/Package and run all tests]   | 2023-05-28T18:02:36,093 INFO  SessionState:1227 - Hive Session ID = a8939c9a-536a-4c1a-a428-2b8671e62557\n[build/Package and run all tests]   | 2023-05-28T18:02:36,115 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:02:36,119 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:02:36,530 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:02:38,997 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 9a47e52c-8212-452a-a9ad-8ffeca659342\n[build/Package and run all tests]   | 2023-05-28T18:02:39,171 INFO  SessionState:1227 - Hive Session ID = 9a47e52c-8212-452a-a9ad-8ffeca659342\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:02:39,262 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellBeeLineEmulationTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:02:39,280 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 28ea16f3-a321-43ef-a902-8476d1882384\n[build/Package and run all tests]   | 2023-05-28T18:02:39,331 INFO  SessionState:1227 - Hive Session ID = 28ea16f3-a321-43ef-a902-8476d1882384\n[build/Package and run all tests]   | 2023-05-28T18:02:39,355 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:02:39,358 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:02:39,705 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:02:42,214 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = c7b33bcf-3414-443b-a5f5-d89f84da40b7\n[build/Package and run all tests]   | 2023-05-28T18:02:42,372 INFO  SessionState:1227 - Hive Session ID = c7b33bcf-3414-443b-a5f5-d89f84da40b7\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:02:42,457 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellBeeLineEmulationTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:02:42,475 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 73a4c600-ccda-4160-9247-d6521afa0680\n[build/Package and run all tests]   | 2023-05-28T18:02:42,533 INFO  SessionState:1227 - Hive Session ID = 73a4c600-ccda-4160-9247-d6521afa0680\n[build/Package and run all tests]   | 2023-05-28T18:02:42,554 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:02:42,557 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:02:42,869 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:02:45,469 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 607a33c1-8516-4c83-8b32-2e2646f5b619\n[build/Package and run all tests]   | 2023-05-28T18:02:45,626 INFO  SessionState:1227 - Hive Session ID = 607a33c1-8516-4c83-8b32-2e2646f5b619\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:02:45,691 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellBeeLineEmulationTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:02:45,709 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 9515e41d-5f03-4bb3-bcbf-7d75245aec13\n[build/Package and run all tests]   | 2023-05-28T18:02:45,759 INFO  SessionState:1227 - Hive Session ID = 9515e41d-5f03-4bb3-bcbf-7d75245aec13\n[build/Package and run all tests]   | 2023-05-28T18:02:45,779 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:02:45,782 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:02:46,061 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:02:48,250 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 2ad954a7-c8ae-4e6a-8fb2-65507e7b38e8\n[build/Package and run all tests]   | 2023-05-28T18:02:48,365 INFO  SessionState:1227 - Hive Session ID = 2ad954a7-c8ae-4e6a-8fb2-65507e7b38e8\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:02:48,429 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellBeeLineEmulationTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:02:48,443 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = ba37ec09-d90d-4e03-85b7-0885ea295cdd\n[build/Package and run all tests]   | 2023-05-28T18:02:48,493 INFO  SessionState:1227 - Hive Session ID = ba37ec09-d90d-4e03-85b7-0885ea295cdd\n[build/Package and run all tests]   | 2023-05-28T18:02:48,514 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:02:48,516 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:02:48,874 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:02:50,939 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = bf5f175a-5225-4b63-a280-26f7b51bda20\n[build/Package and run all tests]   | 2023-05-28T18:02:51,038 INFO  SessionState:1227 - Hive Session ID = bf5f175a-5225-4b63-a280-26f7b51bda20\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:02:51,100 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellBeeLineEmulationTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:02:51,117 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 2631a037-f991-4b49-904e-0e09a5fc9940\n[build/Package and run all tests]   | 2023-05-28T18:02:51,169 INFO  SessionState:1227 - Hive Session ID = 2631a037-f991-4b49-904e-0e09a5fc9940\n[build/Package and run all tests]   | 2023-05-28T18:02:51,190 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:02:51,193 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:02:51,522 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:02:53,906 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 56264dc3-dfc8-4d8c-9dc6-4bf029444731\n[build/Package and run all tests]   | 2023-05-28T18:02:54,068 INFO  SessionState:1227 - Hive Session ID = 56264dc3-dfc8-4d8c-9dc6-4bf029444731\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | NoViableAltException(-1@[])\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1387)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:220)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:74)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:616)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:197)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:260)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:247)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:541)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:510)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:267)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.HiveServerContainer.executeStatement(HiveServerContainer.java:127)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatementsWithCommandShellEmulation(HiveShellBase.java:116)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatementWithCommandShellEmulation(HiveShellBase.java:110)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatement(HiveShellBase.java:100)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:89)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:82)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.HiveShellBeeLineEmulationTest.lambda$testQueryStripFullLineComment$0(HiveShellBeeLineEmulationTest.java:63)\n[build/Package and run all tests]   | \tat org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:55)\n[build/Package and run all tests]   | \tat org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:37)\n[build/Package and run all tests]   | \tat org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3007)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.HiveShellBeeLineEmulationTest.testQueryStripFullLineComment(HiveShellBeeLineEmulationTest.java:63)\n[build/Package and run all tests]   | \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n[build/Package and run all tests]   | \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n[build/Package and run all tests]   | \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n[build/Package and run all tests]   | \tat java.lang.reflect.Method.invoke(Method.java:498)\n[build/Package and run all tests]   | \tat org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[build/Package and run all tests]   | \tat java.util.ArrayList.forEach(ArrayList.java:1259)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[build/Package and run all tests]   | \tat java.util.ArrayList.forEach(ArrayList.java:1259)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\n[build/Package and run all tests]   | FAILED: ParseException line 1:0 cannot recognize input near '<EOF>' '<EOF>' '<EOF>'\n[build/Package and run all tests]   | 2023-05-28T18:02:54,174 ERROR org.apache.hadoop.hive.ql.Driver:1250 - FAILED: ParseException line 1:0 cannot recognize input near '<EOF>' '<EOF>' '<EOF>'\n[build/Package and run all tests]   | org.apache.hadoop.hive.ql.parse.ParseException: line 1:0 cannot recognize input near '<EOF>' '<EOF>' '<EOF>'\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:223)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:74)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:616)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:197)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:260)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:247)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:541)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:510)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:267)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.HiveServerContainer.executeStatement(HiveServerContainer.java:127)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatementsWithCommandShellEmulation(HiveShellBase.java:116)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatementWithCommandShellEmulation(HiveShellBase.java:110)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatement(HiveShellBase.java:100)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:89)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:82)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.HiveShellBeeLineEmulationTest.lambda$testQueryStripFullLineComment$0(HiveShellBeeLineEmulationTest.java:63)\n[build/Package and run all tests]   | \tat org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:55)\n[build/Package and run all tests]   | \tat org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:37)\n[build/Package and run all tests]   | \tat org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3007)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.HiveShellBeeLineEmulationTest.testQueryStripFullLineComment(HiveShellBeeLineEmulationTest.java:63)\n[build/Package and run all tests]   | \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n[build/Package and run all tests]   | \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n[build/Package and run all tests]   | \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n[build/Package and run all tests]   | \tat java.lang.reflect.Method.invoke(Method.java:498)\n[build/Package and run all tests]   | \tat org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[build/Package and run all tests]   | \tat java.util.ArrayList.forEach(ArrayList.java:1259)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[build/Package and run all tests]   | \tat java.util.ArrayList.forEach(ArrayList.java:1259)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:02:54,187 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellBeeLineEmulationTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:02:54,201 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 29.336 s - in com.klarna.hiverunner.HiveShellBeeLineEmulationTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.examples.HelloHiveRunnerParamaterizedTest\n[build/Package and run all tests]   | 2023-05-28T18:02:56,974 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | Hive Session ID = 200a976b-3b20-48ef-ae67-c063429d8784\n[build/Package and run all tests]   | 2023-05-28T18:02:57,392 INFO  SessionState:1227 - Hive Session ID = 200a976b-3b20-48ef-ae67-c063429d8784\n[build/Package and run all tests]   | 2023-05-28T18:02:57,940 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:02:58,837 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:03:00,492 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:03:01,686 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:03:04,758 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:03:04,759 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:03:04,937 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 774378f8-44d6-493f-b2dd-a71a3a77a312\n[build/Package and run all tests]   | 2023-05-28T18:03:05,513 INFO  SessionState:1227 - Hive Session ID = 774378f8-44d6-493f-b2dd-a71a3a77a312\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:07,683 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:08,102 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:03:08,129 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:03:08,506 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:03:08,637 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[build/Package and run all tests]   | 2023-05-28T18:03:08,723 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:03:08,767 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:08,768 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:08,769 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[build/Package and run all tests]   | 2023-05-28T18:03:08,783 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:08,784 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:08,799 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[build/Package and run all tests]   | 2023-05-28T18:03:08,800 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[build/Package and run all tests]   | 2023-05-28T18:03:08,812 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:08,813 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:08,819 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:08,820 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:08,829 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:08,830 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:08,831 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[build/Package and run all tests]   | 2023-05-28T18:03:08,832 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[build/Package and run all tests]   | 2023-05-28T18:03:08,899 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:08,900 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:08,902 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:08,903 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:08,906 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_1882407624' to file:/tmp/hiverunner_test3886932221830180654/warehouse2067463725992121439/source_db.db/test_table/_SCRATCH0.7037664189558517\n[build/Package and run all tests]   | 2023-05-28T18:03:08,915 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:08,915 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:08,919 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:08,919 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:08,969 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:03:09,027 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:03:09,582 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180309_865ebfd1-034c-4fe6-8dbb-3af8dfef569a\n[build/Package and run all tests]   | Total jobs = 1\n[build/Package and run all tests]   | Launching Job 1 out of 1\n[build/Package and run all tests]   | Number of reduce tasks not specified. Estimated from input data size: 1\n[build/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[build/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[build/Package and run all tests]   | In order to limit the maximum number of reducers:\n[build/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[build/Package and run all tests]   | In order to set a constant number of reducers:\n[build/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[build/Package and run all tests]   | 2023-05-28T18:03:09,919 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[build/Package and run all tests]   | 2023-05-28T18:03:09,941 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[build/Package and run all tests]   | 2023-05-28T18:03:09,970 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[build/Package and run all tests]   | 2023-05-28T18:03:09,971 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[build/Package and run all tests]   | 2023-05-28T18:03:10,000 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:03:10,072 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:03:10,338 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:03:10,372 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:03:10,459 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:03:10,528 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local1709668529_0001\n[build/Package and run all tests]   | 2023-05-28T18:03:10,529 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:03:10,831 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | 2023-05-28T18:03:10,832 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:03:10,833 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:03:10,849 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:03:10,850 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local1709668529_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:10,902 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:03:10,906 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test3886932221830180654/warehouse2067463725992121439/source_db.db/test_table/part-m-1882407624:0+146InputFormatClass: org.apache.hadoop.mapred.SequenceFileInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:03:11,004 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[build/Package and run all tests]   | 2023-05-28T18:03:11,005 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[build/Package and run all tests]   | 2023-05-28T18:03:11,005 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[build/Package and run all tests]   | 2023-05-28T18:03:11,006 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[build/Package and run all tests]   | 2023-05-28T18:03:11,027 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[build/Package and run all tests]   | 2023-05-28T18:03:11,028 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[build/Package and run all tests]   | 2023-05-28T18:03:11,028 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[build/Package and run all tests]   | 2023-05-28T18:03:11,028 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:03:11,028 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[build/Package and run all tests]   | 2023-05-28T18:03:11,032 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[build/Package and run all tests]   | 2023-05-28T18:03:11,101 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:03:11,101 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[build/Package and run all tests]   | 2023-05-28T18:03:11,101 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[build/Package and run all tests]   | 2023-05-28T18:03:11,101 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 34; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:03:11,102 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n[build/Package and run all tests]   | 2023-05-28T18:03:11,155 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[build/Package and run all tests]   | 2023-05-28T18:03:11,177 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1709668529_0001_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:03:11,179 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test3886932221830180654/warehouse2067463725992121439/source_db.db/test_table/part-m-1882407624:0+146\n[build/Package and run all tests]   | 2023-05-28T18:03:11,180 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1709668529_0001_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:03:11,183 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1709668529_0001_m_000000_0: Counters: 25\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=40624607\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=41757998\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=4\n[build/Package and run all tests]   | \t\tMap output records=2\n[build/Package and run all tests]   | \t\tMap output bytes=34\n[build/Package and run all tests]   | \t\tMap output materialized bytes=44\n[build/Package and run all tests]   | \t\tInput split bytes=277\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tSpilled Records=2\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=1998585856\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_8=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_9=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_7=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | 2023-05-28T18:03:11,184 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local1709668529_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:11,186 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28T18:03:11,191 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[build/Package and run all tests]   | 2023-05-28T18:03:11,191 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local1709668529_0001_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:11,205 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:03:11,209 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@858106f\n[build/Package and run all tests]   | 2023-05-28T18:03:11,211 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:03:11,232 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1399010048, maxSingleShuffleLimit=349752512, mergeThreshold=923346688, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[build/Package and run all tests]   | 2023-05-28T18:03:11,237 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local1709668529_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[build/Package and run all tests]   | 2023-05-28T18:03:11,272 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#1 about to shuffle output of map attempt_local1709668529_0001_m_000000_0 decomp: 40 len: 44 to MEMORY\n[build/Package and run all tests]   | 2023-05-28T18:03:11,276 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 40 bytes from map-output for attempt_local1709668529_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:11,276 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 40, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->40\n[build/Package and run all tests]   | 2023-05-28T18:03:11,278 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[build/Package and run all tests]   | 2023-05-28T18:03:11,279 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:03:11,280 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[build/Package and run all tests]   | 2023-05-28T18:03:11,368 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:03:11,369 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[build/Package and run all tests]   | 2023-05-28T18:03:11,374 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 40 bytes to disk to satisfy reduce memory limit\n[build/Package and run all tests]   | 2023-05-28T18:03:11,375 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 44 bytes from disk\n[build/Package and run all tests]   | 2023-05-28T18:03:11,377 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[build/Package and run all tests]   | 2023-05-28T18:03:11,377 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:03:11,378 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[build/Package and run all tests]   | 2023-05-28T18:03:11,379 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:03:11,382 INFO  ExecReducer:99 - conf classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter8728343627266601750.jar]\n[build/Package and run all tests]   | 2023-05-28T18:03:11,383 INFO  ExecReducer:101 - thread classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter8728343627266601750.jar]\n[build/Package and run all tests]   | 2023-05-28T18:03:11,396 INFO  ExecReducer:147 - \n[build/Package and run all tests]   | <GBY>Id =4\n[build/Package and run all tests]   |   <Children>\n[build/Package and run all tests]   |     <FS>Id =6\n[build/Package and run all tests]   |       <Children>\n[build/Package and run all tests]   |       <\\Children>\n[build/Package and run all tests]   |       <Parent>Id = 4 null<\\Parent>\n[build/Package and run all tests]   |     <\\FS>\n[build/Package and run all tests]   |   <\\Children>\n[build/Package and run all tests]   |   <Parent><\\Parent>\n[build/Package and run all tests]   | <\\GBY>\n[build/Package and run all tests]   | 2023-05-28T18:03:11,404 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[build/Package and run all tests]   | 2023-05-28T18:03:11,428 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1709668529_0001_r_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:03:11,430 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[build/Package and run all tests]   | 2023-05-28T18:03:11,430 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1709668529_0001_r_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:03:11,431 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1709668529_0001_r_000000_0: Counters: 29\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=40624727\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=41758179\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tCombine output records=0\n[build/Package and run all tests]   | \t\tReduce input groups=2\n[build/Package and run all tests]   | \t\tReduce shuffle bytes=44\n[build/Package and run all tests]   | \t\tReduce input records=2\n[build/Package and run all tests]   | \t\tReduce output records=0\n[build/Package and run all tests]   | \t\tSpilled Records=2\n[build/Package and run all tests]   | \t\tShuffled Maps =1\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=1\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=1998585856\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_0=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_4=2\n[build/Package and run all tests]   | \tShuffle Errors\n[build/Package and run all tests]   | \t\tBAD_ID=0\n[build/Package and run all tests]   | \t\tCONNECTION=0\n[build/Package and run all tests]   | \t\tIO_ERROR=0\n[build/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[build/Package and run all tests]   | \t\tWRONG_MAP=0\n[build/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:03:11,431 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local1709668529_0001_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:11,432 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:03:11,859 Stage-1 map = 100%,  reduce = 100%\n[build/Package and run all tests]   | Ended Job = job_local1709668529_0001\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:11,884 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:03:11,892 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:03:11,912 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.HelloHiveRunnerParamaterizedTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:11,967 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = f6fc7a94-1592-4924-b667-df023838cdbb\n[build/Package and run all tests]   | 2023-05-28T18:03:12,034 INFO  SessionState:1227 - Hive Session ID = f6fc7a94-1592-4924-b667-df023838cdbb\n[build/Package and run all tests]   | 2023-05-28T18:03:12,058 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:03:12,061 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:03:12,475 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:03:14,731 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 1022bb37-e9ae-4dd5-aa99-96cf2fad13a3\n[build/Package and run all tests]   | 2023-05-28T18:03:14,831 INFO  SessionState:1227 - Hive Session ID = 1022bb37-e9ae-4dd5-aa99-96cf2fad13a3\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:14,916 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:15,147 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:03:15,295 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:03:15,418 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:03:15,431 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:15,432 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:15,434 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:15,435 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:15,455 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:15,456 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:15,460 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:15,460 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:15,465 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:15,466 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:15,481 INFO  org.apache.orc.impl.MemoryManagerImpl:85 - orc.rows.between.memory.checks=5000\n[build/Package and run all tests]   | 2023-05-28T18:03:15,528 INFO  org.apache.orc.impl.PhysicalFsWriter:92 - ORC writer created for path: file:/tmp/hiverunner_test7435914403431094610/warehouse769700794746447981/source_db.db/test_table/_SCRATCH0.9109710712478621/_temporary/0/_temporary/attempt__0000_m_000000_101504250/part-m-101504250 with stripeSize: 67108864 blockSize: 268435456 compression: ZLIB bufferSize: 262144\n[build/Package and run all tests]   | 2023-05-28T18:03:15,548 INFO  org.apache.orc.impl.OrcCodecPool:56 - Got brand-new codec ZLIB\n[build/Package and run all tests]   | 2023-05-28T18:03:15,594 INFO  org.apache.orc.impl.WriterImpl:188 - ORC writer created for path: file:/tmp/hiverunner_test7435914403431094610/warehouse769700794746447981/source_db.db/test_table/_SCRATCH0.9109710712478621/_temporary/0/_temporary/attempt__0000_m_000000_101504250/part-m-101504250 with stripeSize: 67108864 blockSize: 268435456 compression: ZLIB bufferSize: 262144\n[build/Package and run all tests]   | 2023-05-28T18:03:15,646 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:15,647 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:15,649 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:15,650 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:15,652 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_101504250' to file:/tmp/hiverunner_test7435914403431094610/warehouse769700794746447981/source_db.db/test_table/_SCRATCH0.9109710712478621\n[build/Package and run all tests]   | 2023-05-28T18:03:15,658 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:15,659 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:15,663 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:15,663 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:15,708 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:03:15,748 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:03:15,818 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180315_6ea7289c-70e0-4c70-9de7-a064a917fbfc\n[build/Package and run all tests]   | Total jobs = 1\n[build/Package and run all tests]   | Launching Job 1 out of 1\n[build/Package and run all tests]   | Number of reduce tasks not specified. Estimated from input data size: 1\n[build/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[build/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[build/Package and run all tests]   | In order to limit the maximum number of reducers:\n[build/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[build/Package and run all tests]   | In order to set a constant number of reducers:\n[build/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[build/Package and run all tests]   | 2023-05-28T18:03:15,859 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:03:15,866 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:03:15,878 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:03:16,050 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:03:16,051 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:03:16,079 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:03:16,115 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local665206029_0002\n[build/Package and run all tests]   | 2023-05-28T18:03:16,116 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:03:16,438 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:03:16,438 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:03:16,439 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:03:16,442 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:03:16,442 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local665206029_0002_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:16,447 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:03:16,449 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test7435914403431094610/warehouse769700794746447981/source_db.db/test_table/part-m-101504250:0+322InputFormatClass: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:03:16,512 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[build/Package and run all tests]   | 2023-05-28T18:03:16,522 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[build/Package and run all tests]   | 2023-05-28T18:03:16,522 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[build/Package and run all tests]   | 2023-05-28T18:03:16,523 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[build/Package and run all tests]   | 2023-05-28T18:03:16,523 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:03:16,523 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[build/Package and run all tests]   | 2023-05-28T18:03:16,524 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[build/Package and run all tests]   | 2023-05-28T18:03:16,530 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:03:16,530 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[build/Package and run all tests]   | 2023-05-28T18:03:16,530 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[build/Package and run all tests]   | 2023-05-28T18:03:16,530 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 34; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:03:16,530 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n[build/Package and run all tests]   | 2023-05-28T18:03:16,613 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[build/Package and run all tests]   | 2023-05-28T18:03:16,621 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local665206029_0002_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:03:16,622 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test7435914403431094610/warehouse769700794746447981/source_db.db/test_table/part-m-101504250:0+322\n[build/Package and run all tests]   | 2023-05-28T18:03:16,623 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local665206029_0002_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:03:16,623 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local665206029_0002_m_000000_0: Counters: 25\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=81250167\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=83512511\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=1\n[build/Package and run all tests]   | \t\tMap output records=2\n[build/Package and run all tests]   | \t\tMap output bytes=34\n[build/Package and run all tests]   | \t\tMap output materialized bytes=44\n[build/Package and run all tests]   | \t\tInput split bytes=274\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tSpilled Records=2\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2085617664\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_8=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_9=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_7=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | 2023-05-28T18:03:16,623 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local665206029_0002_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:16,623 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28T18:03:16,625 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[build/Package and run all tests]   | 2023-05-28T18:03:16,626 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local665206029_0002_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:16,629 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:03:16,629 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2bf8af6e\n[build/Package and run all tests]   | 2023-05-28T18:03:16,630 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:03:16,631 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1459932288, maxSingleShuffleLimit=364983072, mergeThreshold=963555328, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[build/Package and run all tests]   | 2023-05-28T18:03:16,632 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local665206029_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[build/Package and run all tests]   | 2023-05-28T18:03:16,636 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#2 about to shuffle output of map attempt_local665206029_0002_m_000000_0 decomp: 40 len: 44 to MEMORY\n[build/Package and run all tests]   | 2023-05-28T18:03:16,637 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 40 bytes from map-output for attempt_local665206029_0002_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:16,637 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 40, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->40\n[build/Package and run all tests]   | 2023-05-28T18:03:16,638 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[build/Package and run all tests]   | 2023-05-28T18:03:16,639 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:03:16,639 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[build/Package and run all tests]   | 2023-05-28T18:03:16,719 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:03:16,719 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[build/Package and run all tests]   | 2023-05-28T18:03:16,723 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 40 bytes to disk to satisfy reduce memory limit\n[build/Package and run all tests]   | 2023-05-28T18:03:16,723 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 44 bytes from disk\n[build/Package and run all tests]   | 2023-05-28T18:03:16,723 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[build/Package and run all tests]   | 2023-05-28T18:03:16,724 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:03:16,724 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[build/Package and run all tests]   | 2023-05-28T18:03:16,725 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:03:16,725 INFO  ExecReducer:99 - conf classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter8728343627266601750.jar]\n[build/Package and run all tests]   | 2023-05-28T18:03:16,725 INFO  ExecReducer:101 - thread classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter8728343627266601750.jar]\n[build/Package and run all tests]   | 2023-05-28T18:03:16,731 INFO  ExecReducer:147 - \n[build/Package and run all tests]   | <GBY>Id =4\n[build/Package and run all tests]   |   <Children>\n[build/Package and run all tests]   |     <FS>Id =6\n[build/Package and run all tests]   |       <Children>\n[build/Package and run all tests]   |       <\\Children>\n[build/Package and run all tests]   |       <Parent>Id = 4 null<\\Parent>\n[build/Package and run all tests]   |     <\\FS>\n[build/Package and run all tests]   |   <\\Children>\n[build/Package and run all tests]   |   <Parent><\\Parent>\n[build/Package and run all tests]   | <\\GBY>\n[build/Package and run all tests]   | 2023-05-28T18:03:16,746 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local665206029_0002_r_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:03:16,748 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[build/Package and run all tests]   | 2023-05-28T18:03:16,748 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local665206029_0002_r_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:03:16,748 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local665206029_0002_r_000000_0: Counters: 29\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=81250287\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=83512692\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tCombine output records=0\n[build/Package and run all tests]   | \t\tReduce input groups=2\n[build/Package and run all tests]   | \t\tReduce shuffle bytes=44\n[build/Package and run all tests]   | \t\tReduce input records=2\n[build/Package and run all tests]   | \t\tReduce output records=0\n[build/Package and run all tests]   | \t\tSpilled Records=2\n[build/Package and run all tests]   | \t\tShuffled Maps =1\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=1\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2085617664\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_0=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_4=2\n[build/Package and run all tests]   | \tShuffle Errors\n[build/Package and run all tests]   | \t\tBAD_ID=0\n[build/Package and run all tests]   | \t\tCONNECTION=0\n[build/Package and run all tests]   | \t\tIO_ERROR=0\n[build/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[build/Package and run all tests]   | \t\tWRONG_MAP=0\n[build/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:03:16,749 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local665206029_0002_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:16,749 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:03:17,448 Stage-1 map = 100%,  reduce = 100%\n[build/Package and run all tests]   | Ended Job = job_local665206029_0002\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:17,466 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:03:17,469 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.HelloHiveRunnerParamaterizedTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:17,489 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 6c7ddc75-b44b-4a27-b9b5-fb4a70c60070\n[build/Package and run all tests]   | 2023-05-28T18:03:17,543 INFO  SessionState:1227 - Hive Session ID = 6c7ddc75-b44b-4a27-b9b5-fb4a70c60070\n[build/Package and run all tests]   | 2023-05-28T18:03:17,571 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:03:17,573 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:03:17,985 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:03:20,034 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 0921111a-e3c0-4a2b-92c7-39dd85d4d070\n[build/Package and run all tests]   | 2023-05-28T18:03:20,189 INFO  SessionState:1227 - Hive Session ID = 0921111a-e3c0-4a2b-92c7-39dd85d4d070\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:20,275 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:20,476 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:03:20,608 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:03:20,746 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:03:20,758 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:20,758 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:20,760 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:20,761 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:20,777 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:20,777 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:20,781 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:20,781 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:20,785 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:20,786 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:20,800 INFO  org.apache.parquet.hadoop.codec.CodecConfig:91 - Compression set to false\n[build/Package and run all tests]   | 2023-05-28T18:03:20,802 INFO  org.apache.parquet.hadoop.codec.CodecConfig:95 - Compression: UNCOMPRESSED\n[build/Package and run all tests]   | 2023-05-28T18:03:20,816 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:376 - Parquet block size to 134217728\n[build/Package and run all tests]   | 2023-05-28T18:03:20,817 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:377 - Parquet page size to 1048576\n[build/Package and run all tests]   | 2023-05-28T18:03:20,817 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:378 - Parquet dictionary page size to 1048576\n[build/Package and run all tests]   | 2023-05-28T18:03:20,818 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:379 - Dictionary is on\n[build/Package and run all tests]   | 2023-05-28T18:03:20,818 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:380 - Validation is off\n[build/Package and run all tests]   | 2023-05-28T18:03:20,819 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:381 - Writer version is: PARQUET_1_0\n[build/Package and run all tests]   | 2023-05-28T18:03:20,819 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:382 - Maximum row group padding size is 8388608 bytes\n[build/Package and run all tests]   | 2023-05-28T18:03:20,820 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:383 - Page size checking is: estimated\n[build/Package and run all tests]   | 2023-05-28T18:03:20,820 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:384 - Min row count for page size check is: 100\n[build/Package and run all tests]   | 2023-05-28T18:03:20,820 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:385 - Max row count for page size check is: 10000\n[build/Package and run all tests]   | 2023-05-28T18:03:21,175 INFO  org.apache.parquet.hadoop.InternalParquetRecordWriter:165 - Flushing mem columnStore to file. allocated memory: 64\n[build/Package and run all tests]   | 2023-05-28T18:03:21,390 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:21,391 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:21,393 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:21,393 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:21,395 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_2138221389' to file:/tmp/hiverunner_test5565585434440319736/warehouse4975675859910668408/source_db.db/test_table/_SCRATCH0.821452517677056\n[build/Package and run all tests]   | 2023-05-28T18:03:21,400 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:21,401 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:21,405 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:21,405 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:21,447 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:03:21,490 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:03:21,555 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180321_7ba53630-7170-465a-a400-e760c8f8ce34\n[build/Package and run all tests]   | Total jobs = 1\n[build/Package and run all tests]   | Launching Job 1 out of 1\n[build/Package and run all tests]   | Number of reduce tasks not specified. Estimated from input data size: 1\n[build/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[build/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[build/Package and run all tests]   | In order to limit the maximum number of reducers:\n[build/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[build/Package and run all tests]   | In order to set a constant number of reducers:\n[build/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[build/Package and run all tests]   | 2023-05-28T18:03:21,581 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:03:21,596 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:03:21,603 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:03:21,759 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:03:21,760 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:03:21,788 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:03:21,821 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local111632147_0003\n[build/Package and run all tests]   | 2023-05-28T18:03:21,822 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:03:22,119 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | 2023-05-28T18:03:22,119 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:03:22,120 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:03:22,123 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:03:22,123 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local111632147_0003_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:22,128 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:03:22,129 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test5565585434440319736/warehouse4975675859910668408/source_db.db/test_table/part-m-2138221389:0+452InputFormatClass: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:03:22,173 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[build/Package and run all tests]   | 2023-05-28T18:03:22,183 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[build/Package and run all tests]   | 2023-05-28T18:03:22,183 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[build/Package and run all tests]   | 2023-05-28T18:03:22,183 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[build/Package and run all tests]   | 2023-05-28T18:03:22,183 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:03:22,183 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[build/Package and run all tests]   | 2023-05-28T18:03:22,184 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[build/Package and run all tests]   | 2023-05-28T18:03:22,221 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:03:22,222 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[build/Package and run all tests]   | 2023-05-28T18:03:22,222 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[build/Package and run all tests]   | 2023-05-28T18:03:22,222 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 34; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:03:22,222 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n[build/Package and run all tests]   | 2023-05-28T18:03:22,414 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[build/Package and run all tests]   | 2023-05-28T18:03:22,422 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local111632147_0003_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:03:22,424 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - map\n[build/Package and run all tests]   | 2023-05-28T18:03:22,424 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local111632147_0003_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:03:22,425 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local111632147_0003_m_000000_0: Counters: 25\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=121876121\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=125267148\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=1\n[build/Package and run all tests]   | \t\tMap output records=2\n[build/Package and run all tests]   | \t\tMap output bytes=34\n[build/Package and run all tests]   | \t\tMap output materialized bytes=44\n[build/Package and run all tests]   | \t\tInput split bytes=290\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tSpilled Records=2\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2084569088\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_8=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_9=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_7=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | 2023-05-28T18:03:22,425 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local111632147_0003_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:22,425 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28T18:03:22,427 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[build/Package and run all tests]   | 2023-05-28T18:03:22,427 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local111632147_0003_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:22,431 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:03:22,431 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4949c7a4\n[build/Package and run all tests]   | 2023-05-28T18:03:22,431 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:03:22,433 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1459198336, maxSingleShuffleLimit=364799584, mergeThreshold=963070912, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[build/Package and run all tests]   | 2023-05-28T18:03:22,434 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local111632147_0003_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[build/Package and run all tests]   | 2023-05-28T18:03:22,438 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#3 about to shuffle output of map attempt_local111632147_0003_m_000000_0 decomp: 40 len: 44 to MEMORY\n[build/Package and run all tests]   | 2023-05-28T18:03:22,438 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 40 bytes from map-output for attempt_local111632147_0003_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:22,439 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 40, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->40\n[build/Package and run all tests]   | 2023-05-28T18:03:22,439 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[build/Package and run all tests]   | 2023-05-28T18:03:22,440 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:03:22,440 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[build/Package and run all tests]   | 2023-05-28T18:03:22,542 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:03:22,543 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[build/Package and run all tests]   | 2023-05-28T18:03:22,548 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 40 bytes to disk to satisfy reduce memory limit\n[build/Package and run all tests]   | 2023-05-28T18:03:22,548 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 44 bytes from disk\n[build/Package and run all tests]   | 2023-05-28T18:03:22,549 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[build/Package and run all tests]   | 2023-05-28T18:03:22,549 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:03:22,549 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[build/Package and run all tests]   | 2023-05-28T18:03:22,550 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:03:22,550 INFO  ExecReducer:99 - conf classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter8728343627266601750.jar]\n[build/Package and run all tests]   | 2023-05-28T18:03:22,550 INFO  ExecReducer:101 - thread classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter8728343627266601750.jar]\n[build/Package and run all tests]   | 2023-05-28T18:03:22,556 INFO  ExecReducer:147 - \n[build/Package and run all tests]   | <GBY>Id =4\n[build/Package and run all tests]   |   <Children>\n[build/Package and run all tests]   |     <FS>Id =6\n[build/Package and run all tests]   |       <Children>\n[build/Package and run all tests]   |       <\\Children>\n[build/Package and run all tests]   |       <Parent>Id = 4 null<\\Parent>\n[build/Package and run all tests]   |     <\\FS>\n[build/Package and run all tests]   |   <\\Children>\n[build/Package and run all tests]   |   <Parent><\\Parent>\n[build/Package and run all tests]   | <\\GBY>\n[build/Package and run all tests]   | 2023-05-28T18:03:22,574 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local111632147_0003_r_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:03:22,575 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[build/Package and run all tests]   | 2023-05-28T18:03:22,575 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local111632147_0003_r_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:03:22,576 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local111632147_0003_r_000000_0: Counters: 29\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=121876241\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=125267329\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tCombine output records=0\n[build/Package and run all tests]   | \t\tReduce input groups=2\n[build/Package and run all tests]   | \t\tReduce shuffle bytes=44\n[build/Package and run all tests]   | \t\tReduce input records=2\n[build/Package and run all tests]   | \t\tReduce output records=0\n[build/Package and run all tests]   | \t\tSpilled Records=2\n[build/Package and run all tests]   | \t\tShuffled Maps =1\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=1\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2084569088\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_0=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_4=2\n[build/Package and run all tests]   | \tShuffle Errors\n[build/Package and run all tests]   | \t\tBAD_ID=0\n[build/Package and run all tests]   | \t\tCONNECTION=0\n[build/Package and run all tests]   | \t\tIO_ERROR=0\n[build/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[build/Package and run all tests]   | \t\tWRONG_MAP=0\n[build/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:03:22,576 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local111632147_0003_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:22,576 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:03:23,126 Stage-1 map = 100%,  reduce = 100%\n[build/Package and run all tests]   | Ended Job = job_local111632147_0003\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:23,141 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:03:23,144 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.HelloHiveRunnerParamaterizedTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:23,162 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 26.943 s - in com.klarna.hiverunner.examples.HelloHiveRunnerParamaterizedTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.examples.HelloHiveRunnerTest\n[build/Package and run all tests]   | 2023-05-28T18:03:25,834 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | Hive Session ID = d2b4c31e-d8d2-4ecb-8124-1f5ceb6093d6\n[build/Package and run all tests]   | 2023-05-28T18:03:26,359 INFO  SessionState:1227 - Hive Session ID = d2b4c31e-d8d2-4ecb-8124-1f5ceb6093d6\n[build/Package and run all tests]   | 2023-05-28T18:03:27,033 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:03:27,988 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:03:29,759 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:03:30,982 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:03:34,448 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:03:34,449 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:03:34,620 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 9b129726-4ad2-4c72-9175-3b6d8a50f0be\n[build/Package and run all tests]   | 2023-05-28T18:03:35,299 INFO  SessionState:1227 - Hive Session ID = 9b129726-4ad2-4c72-9175-3b6d8a50f0be\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:37,414 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:37,778 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.my_schema, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:37,928 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:03:37,952 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:03:38,279 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:03:38,398 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[build/Package and run all tests]   | 2023-05-28T18:03:38,476 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:03:38,516 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:38,517 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:38,518 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[build/Package and run all tests]   | 2023-05-28T18:03:38,530 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:38,531 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:38,543 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[build/Package and run all tests]   | 2023-05-28T18:03:38,543 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[build/Package and run all tests]   | 2023-05-28T18:03:38,553 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:38,554 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:38,561 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:38,561 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:38,570 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:38,570 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:38,571 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[build/Package and run all tests]   | 2023-05-28T18:03:38,572 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[build/Package and run all tests]   | 2023-05-28T18:03:38,622 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:38,622 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:38,625 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:38,625 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:38,628 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_1322239019' to file:/tmp/hiverunner_test5934128242227264168/warehouse3847445277246667278/source_db.db/test_table/_SCRATCH0.11816008927323729\n[build/Package and run all tests]   | 2023-05-28T18:03:38,637 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:38,637 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:38,641 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:03:38,641 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:03:38,702 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:03:38,762 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180338_617d1c1a-e56f-4b17-872d-23e58d529ada\n[build/Package and run all tests]   | Total jobs = 1\n[build/Package and run all tests]   | 2023-05-28T18:03:39,272 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Launching Job 1 out of 1\n[build/Package and run all tests]   | Number of reduce tasks not specified. Estimated from input data size: 1\n[build/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[build/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[build/Package and run all tests]   | In order to limit the maximum number of reducers:\n[build/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[build/Package and run all tests]   | In order to set a constant number of reducers:\n[build/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[build/Package and run all tests]   | 2023-05-28T18:03:39,685 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[build/Package and run all tests]   | 2023-05-28T18:03:39,712 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[build/Package and run all tests]   | 2023-05-28T18:03:39,734 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[build/Package and run all tests]   | 2023-05-28T18:03:39,735 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[build/Package and run all tests]   | 2023-05-28T18:03:39,756 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:03:39,824 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:03:40,104 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:03:40,123 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:03:40,161 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:03:40,289 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local1841981809_0001\n[build/Package and run all tests]   | 2023-05-28T18:03:40,292 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:03:40,564 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | 2023-05-28T18:03:40,565 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:03:40,566 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:03:40,583 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:03:40,583 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local1841981809_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:40,633 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:03:40,642 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test5934128242227264168/warehouse3847445277246667278/source_db.db/test_table/part-m-1322239019:0+28InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:03:40,737 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[build/Package and run all tests]   | 2023-05-28T18:03:40,738 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[build/Package and run all tests]   | 2023-05-28T18:03:40,738 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[build/Package and run all tests]   | 2023-05-28T18:03:40,738 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[build/Package and run all tests]   | 2023-05-28T18:03:40,758 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[build/Package and run all tests]   | 2023-05-28T18:03:40,759 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[build/Package and run all tests]   | 2023-05-28T18:03:40,759 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[build/Package and run all tests]   | 2023-05-28T18:03:40,759 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:03:40,760 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[build/Package and run all tests]   | 2023-05-28T18:03:40,764 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[build/Package and run all tests]   | 2023-05-28T18:03:40,852 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:03:40,853 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[build/Package and run all tests]   | 2023-05-28T18:03:40,853 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[build/Package and run all tests]   | 2023-05-28T18:03:40,853 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 34; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:03:40,854 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n[build/Package and run all tests]   | 2023-05-28T18:03:40,901 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[build/Package and run all tests]   | 2023-05-28T18:03:40,918 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1841981809_0001_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:03:40,920 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test5934128242227264168/warehouse3847445277246667278/source_db.db/test_table/part-m-1322239019:0+28\n[build/Package and run all tests]   | 2023-05-28T18:03:40,920 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1841981809_0001_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:03:40,924 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1841981809_0001_m_000000_0: Counters: 25\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=40624326\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=41762289\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=4\n[build/Package and run all tests]   | \t\tMap output records=2\n[build/Package and run all tests]   | \t\tMap output bytes=34\n[build/Package and run all tests]   | \t\tMap output materialized bytes=44\n[build/Package and run all tests]   | \t\tInput split bytes=269\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tSpilled Records=2\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=1999110144\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_8=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_9=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_7=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | 2023-05-28T18:03:40,924 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local1841981809_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:40,926 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28T18:03:40,932 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[build/Package and run all tests]   | 2023-05-28T18:03:40,932 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local1841981809_0001_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:40,945 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:03:40,949 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@620df90f\n[build/Package and run all tests]   | 2023-05-28T18:03:40,951 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:03:40,971 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1399377024, maxSingleShuffleLimit=349844256, mergeThreshold=923588864, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[build/Package and run all tests]   | 2023-05-28T18:03:40,976 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local1841981809_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[build/Package and run all tests]   | 2023-05-28T18:03:41,014 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#1 about to shuffle output of map attempt_local1841981809_0001_m_000000_0 decomp: 40 len: 44 to MEMORY\n[build/Package and run all tests]   | 2023-05-28T18:03:41,019 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 40 bytes from map-output for attempt_local1841981809_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:41,022 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 40, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->40\n[build/Package and run all tests]   | 2023-05-28T18:03:41,024 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[build/Package and run all tests]   | 2023-05-28T18:03:41,025 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:03:41,026 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[build/Package and run all tests]   | 2023-05-28T18:03:41,113 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:03:41,114 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[build/Package and run all tests]   | 2023-05-28T18:03:41,118 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 40 bytes to disk to satisfy reduce memory limit\n[build/Package and run all tests]   | 2023-05-28T18:03:41,119 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 44 bytes from disk\n[build/Package and run all tests]   | 2023-05-28T18:03:41,121 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[build/Package and run all tests]   | 2023-05-28T18:03:41,121 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:03:41,121 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[build/Package and run all tests]   | 2023-05-28T18:03:41,122 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:03:41,125 INFO  ExecReducer:99 - conf classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter2701882928777577745.jar]\n[build/Package and run all tests]   | 2023-05-28T18:03:41,125 INFO  ExecReducer:101 - thread classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter2701882928777577745.jar]\n[build/Package and run all tests]   | 2023-05-28T18:03:41,136 INFO  ExecReducer:147 - \n[build/Package and run all tests]   | <GBY>Id =4\n[build/Package and run all tests]   |   <Children>\n[build/Package and run all tests]   |     <FS>Id =6\n[build/Package and run all tests]   |       <Children>\n[build/Package and run all tests]   |       <\\Children>\n[build/Package and run all tests]   |       <Parent>Id = 4 null<\\Parent>\n[build/Package and run all tests]   |     <\\FS>\n[build/Package and run all tests]   |   <\\Children>\n[build/Package and run all tests]   |   <Parent><\\Parent>\n[build/Package and run all tests]   | <\\GBY>\n[build/Package and run all tests]   | 2023-05-28T18:03:41,142 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[build/Package and run all tests]   | 2023-05-28T18:03:41,180 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1841981809_0001_r_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:03:41,182 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[build/Package and run all tests]   | 2023-05-28T18:03:41,182 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1841981809_0001_r_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:03:41,183 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1841981809_0001_r_000000_0: Counters: 29\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=40624446\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=41762470\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tCombine output records=0\n[build/Package and run all tests]   | \t\tReduce input groups=2\n[build/Package and run all tests]   | \t\tReduce shuffle bytes=44\n[build/Package and run all tests]   | \t\tReduce input records=2\n[build/Package and run all tests]   | \t\tReduce output records=0\n[build/Package and run all tests]   | \t\tSpilled Records=2\n[build/Package and run all tests]   | \t\tShuffled Maps =1\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=1\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=1999110144\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_1_my_schema.result=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_4=2\n[build/Package and run all tests]   | \tShuffle Errors\n[build/Package and run all tests]   | \t\tBAD_ID=0\n[build/Package and run all tests]   | \t\tCONNECTION=0\n[build/Package and run all tests]   | \t\tIO_ERROR=0\n[build/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[build/Package and run all tests]   | \t\tWRONG_MAP=0\n[build/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:03:41,183 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local1841981809_0001_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:41,184 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:03:41,590 Stage-1 map = 100%,  reduce = 100%\n[build/Package and run all tests]   | Ended Job = job_local1841981809_0001\n[build/Package and run all tests]   | Loading data to table my_schema.result\n[build/Package and run all tests]   | 2023-05-28T18:03:41,612 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:41,819 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:41,892 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:03:41,907 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:03:41,940 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.HelloHiveRunnerTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:41,994 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.859 s - in com.klarna.hiverunner.examples.HelloHiveRunnerTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.examples.HelloAnnotatedHiveRunnerTest\n[build/Package and run all tests]   | 2023-05-28T18:03:44,669 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | Hive Session ID = 321cbf2c-ceb2-47fe-b532-be2c33afbbdc\n[build/Package and run all tests]   | 2023-05-28T18:03:45,108 INFO  SessionState:1227 - Hive Session ID = 321cbf2c-ceb2-47fe-b532-be2c33afbbdc\n[build/Package and run all tests]   | 2023-05-28T18:03:45,528 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:03:46,507 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:03:48,182 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:03:49,414 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:03:52,289 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:03:52,290 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:03:52,433 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 3a6d0227-7c55-4167-bd71-376b61ccfb2f\n[build/Package and run all tests]   | 2023-05-28T18:03:52,880 INFO  SessionState:1227 - Hive Session ID = 3a6d0227-7c55-4167-bd71-376b61ccfb2f\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:54,581 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.bar, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:55,490 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180354_f52b49d4-2e77-4b73-a872-0f4e5fd3effc\n[build/Package and run all tests]   | Total jobs = 3\n[build/Package and run all tests]   | Launching Job 1 out of 3\n[build/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[build/Package and run all tests]   | 2023-05-28T18:03:55,790 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[build/Package and run all tests]   | 2023-05-28T18:03:55,805 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[build/Package and run all tests]   | 2023-05-28T18:03:55,819 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[build/Package and run all tests]   | 2023-05-28T18:03:55,820 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[build/Package and run all tests]   | 2023-05-28T18:03:55,848 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:03:55,922 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:03:56,198 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[build/Package and run all tests]   | 2023-05-28T18:03:56,217 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:03:56,253 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:03:56,321 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local1368667398_0001\n[build/Package and run all tests]   | 2023-05-28T18:03:56,321 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:03:56,640 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:03:56,644 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:03:56,646 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:03:56,661 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:03:56,668 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local1368667398_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:56,717 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:03:56,728 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test7141032865892398401/hadooptmp6182287022571871592/foo/data_from_string.csv:0+11,/tmp/hiverunner_test7141032865892398401/hadooptmp6182287022571871592/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:03:56,811 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[build/Package and run all tests]   | 2023-05-28T18:03:56,811 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[build/Package and run all tests]   | 2023-05-28T18:03:56,812 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[build/Package and run all tests]   | 2023-05-28T18:03:56,812 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[build/Package and run all tests]   | 2023-05-28T18:03:56,921 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[build/Package and run all tests]   | 2023-05-28T18:03:56,924 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[build/Package and run all tests]   | 2023-05-28T18:03:56,959 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:03:56,968 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1368667398_0001_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:03:56,970 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test7141032865892398401/hadooptmp6182287022571871592/foo/data_from_file.csv:0+12\n[build/Package and run all tests]   | 2023-05-28T18:03:56,970 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1368667398_0001_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:03:56,973 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1368667398_0001_m_000000_0: Counters: 24\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=40624401\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=41752058\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=4\n[build/Package and run all tests]   | \t\tMap output records=0\n[build/Package and run all tests]   | \t\tInput split bytes=365\n[build/Package and run all tests]   | \t\tSpilled Records=0\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=58\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2000683008\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_1_bar.foo_prim=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:03:56,974 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local1368667398_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:03:56,975 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:03:57,664 Stage-1 map = 100%,  reduce = 0%\n[build/Package and run all tests]   | Ended Job = job_local1368667398_0001\n[build/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[build/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[build/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test7141032865892398401/warehouse8405188666705352903/bar.db/.hive-staging_hive_2023-05-28_18-03-54_941_112270953701907150-1/-ext-10001\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test7141032865892398401/warehouse8405188666705352903/bar.db/foo_prim\n[build/Package and run all tests]   | 2023-05-28T18:03:57,697 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:57,802 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | WARNING: Order/Sort by without limit in sub query or view [a] is removed, as it's pointless and bad for performance.\n[build/Package and run all tests]   | 2023-05-28T18:03:57,906 WARN  org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer:531 - Cannot determine basic stats for table: bar@foo_prim from metastore. Falling back.\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:57,929 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:03:57,937 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:03:57,963 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.HelloAnnotatedHiveRunnerTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:03:58,027 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = cbe58066-4e80-47d3-9d73-7231ab6cfb38\n[build/Package and run all tests]   | 2023-05-28T18:03:58,097 INFO  SessionState:1227 - Hive Session ID = cbe58066-4e80-47d3-9d73-7231ab6cfb38\n[build/Package and run all tests]   | 2023-05-28T18:03:58,168 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:03:58,171 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:03:58,591 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:04:00,919 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 90198c50-f0da-44ca-a560-d568609dfb83\n[build/Package and run all tests]   | 2023-05-28T18:04:01,055 INFO  SessionState:1227 - Hive Session ID = 90198c50-f0da-44ca-a560-d568609dfb83\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:01,137 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.bar, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:01,557 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180401_af9ef3d3-197f-4cad-92cf-0f0416f5cd4e\n[build/Package and run all tests]   | Total jobs = 3\n[build/Package and run all tests]   | Launching Job 1 out of 3\n[build/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[build/Package and run all tests]   | 2023-05-28T18:04:01,608 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:04:01,625 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:04:01,636 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:04:01,803 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[build/Package and run all tests]   | 2023-05-28T18:04:01,805 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:04:01,852 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:04:01,913 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local2012659325_0002\n[build/Package and run all tests]   | 2023-05-28T18:04:01,914 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:04:02,117 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | 2023-05-28T18:04:02,118 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:04:02,119 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:04:02,122 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:04:02,123 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local2012659325_0002_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:02,129 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:04:02,131 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test3005261630718438482/hadooptmp7892884512023068058/foo/data_from_string.csv:0+11,/tmp/hiverunner_test3005261630718438482/hadooptmp7892884512023068058/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:04:02,140 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[build/Package and run all tests]   | 2023-05-28T18:04:02,163 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:04:02,165 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local2012659325_0002_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:04:02,167 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test3005261630718438482/hadooptmp7892884512023068058/foo/data_from_file.csv:0+12\n[build/Package and run all tests]   | 2023-05-28T18:04:02,167 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local2012659325_0002_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:04:02,167 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local2012659325_0002_m_000000_0: Counters: 24\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=81248845\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=83504164\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=4\n[build/Package and run all tests]   | \t\tMap output records=0\n[build/Package and run all tests]   | \t\tInput split bytes=365\n[build/Package and run all tests]   | \t\tSpilled Records=0\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2087190528\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_1_bar.foo_prim=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:04:02,168 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local2012659325_0002_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:02,168 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:04:03,125 Stage-1 map = 100%,  reduce = 0%\n[build/Package and run all tests]   | Ended Job = job_local2012659325_0002\n[build/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[build/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[build/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test3005261630718438482/warehouse6421155463306711925/bar.db/.hive-staging_hive_2023-05-28_18-04-01_376_2014057844870937167-1/-ext-10001\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test3005261630718438482/warehouse6421155463306711925/bar.db/foo_prim\n[build/Package and run all tests]   | 2023-05-28T18:04:03,138 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:03,194 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:03,248 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 2\n[build/Package and run all tests]   | 2023-05-28T18:04:03,252 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.HelloAnnotatedHiveRunnerTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:03,271 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = eb2a3f4d-9add-4186-9d90-0ee76eacaeb4\n[build/Package and run all tests]   | 2023-05-28T18:04:03,323 INFO  SessionState:1227 - Hive Session ID = eb2a3f4d-9add-4186-9d90-0ee76eacaeb4\n[build/Package and run all tests]   | 2023-05-28T18:04:03,373 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:04:03,376 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:04:03,835 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:04:05,641 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 7f4f077d-6941-46b2-93e8-a160295b8d1a\n[build/Package and run all tests]   | 2023-05-28T18:04:05,794 INFO  SessionState:1227 - Hive Session ID = 7f4f077d-6941-46b2-93e8-a160295b8d1a\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:05,868 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.bar, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:06,227 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180406_64b25478-c10a-4897-936a-2176eaed51e8\n[build/Package and run all tests]   | Total jobs = 3\n[build/Package and run all tests]   | Launching Job 1 out of 3\n[build/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[build/Package and run all tests]   | 2023-05-28T18:04:06,267 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:04:06,274 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:04:06,283 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:04:06,442 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[build/Package and run all tests]   | 2023-05-28T18:04:06,445 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:04:06,510 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:04:06,542 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local2053878521_0003\n[build/Package and run all tests]   | 2023-05-28T18:04:06,543 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:04:06,716 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | 2023-05-28T18:04:06,717 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:04:06,718 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:04:06,721 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:04:06,721 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local2053878521_0003_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:06,725 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:04:06,727 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test3593023818158195820/hadooptmp963923936652365669/foo/data_from_string.csv:0+11,/tmp/hiverunner_test3593023818158195820/hadooptmp963923936652365669/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:04:06,735 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[build/Package and run all tests]   | 2023-05-28T18:04:06,757 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:04:06,760 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local2053878521_0003_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:04:06,761 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test3593023818158195820/hadooptmp963923936652365669/foo/data_from_file.csv:0+12\n[build/Package and run all tests]   | 2023-05-28T18:04:06,761 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local2053878521_0003_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:04:06,762 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local2053878521_0003_m_000000_0: Counters: 24\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=121873267\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=125256238\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=4\n[build/Package and run all tests]   | \t\tMap output records=0\n[build/Package and run all tests]   | \t\tInput split bytes=363\n[build/Package and run all tests]   | \t\tSpilled Records=0\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2087190528\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_1_bar.foo_prim=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:04:06,762 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local2053878521_0003_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:06,763 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:04:07,722 Stage-1 map = 100%,  reduce = 0%\n[build/Package and run all tests]   | Ended Job = job_local2053878521_0003\n[build/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[build/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[build/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test3593023818158195820/warehouse4561929330531115452/bar.db/.hive-staging_hive_2023-05-28_18-04-06_058_7130270802968761350-1/-ext-10001\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test3593023818158195820/warehouse4561929330531115452/bar.db/foo_prim\n[build/Package and run all tests]   | 2023-05-28T18:04:07,733 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:07,800 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:07,858 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:04:07,861 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.HelloAnnotatedHiveRunnerTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:07,878 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 9046d2b6-acac-4882-b926-3ec1eac9557d\n[build/Package and run all tests]   | 2023-05-28T18:04:07,935 INFO  SessionState:1227 - Hive Session ID = 9046d2b6-acac-4882-b926-3ec1eac9557d\n[build/Package and run all tests]   | 2023-05-28T18:04:07,975 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:04:07,978 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:04:08,330 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:04:10,377 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = ad6c66e0-a2e4-442c-88aa-f237cd224bc3\n[build/Package and run all tests]   | 2023-05-28T18:04:10,531 INFO  SessionState:1227 - Hive Session ID = ad6c66e0-a2e4-442c-88aa-f237cd224bc3\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:10,594 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.bar, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:10,968 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180410_81db5a32-e2dc-4f0a-944f-30625a10dc3d\n[build/Package and run all tests]   | Total jobs = 3\n[build/Package and run all tests]   | Launching Job 1 out of 3\n[build/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[build/Package and run all tests]   | 2023-05-28T18:04:10,995 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:04:11,005 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:04:11,041 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:04:11,216 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[build/Package and run all tests]   | 2023-05-28T18:04:11,218 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:04:11,281 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:04:11,316 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local1586303796_0004\n[build/Package and run all tests]   | 2023-05-28T18:04:11,316 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:04:11,449 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | 2023-05-28T18:04:11,450 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:04:11,451 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:04:11,454 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:04:11,454 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local1586303796_0004_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:11,458 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:04:11,459 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test3869280240417179012/hadooptmp8640705252287655941/foo/data_from_string.csv:0+11,/tmp/hiverunner_test3869280240417179012/hadooptmp8640705252287655941/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:04:11,466 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[build/Package and run all tests]   | 2023-05-28T18:04:11,515 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:04:11,517 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1586303796_0004_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:04:11,518 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test3869280240417179012/hadooptmp8640705252287655941/foo/data_from_file.csv:0+12\n[build/Package and run all tests]   | 2023-05-28T18:04:11,518 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1586303796_0004_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:04:11,518 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1586303796_0004_m_000000_0: Counters: 24\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=162497697\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=167008345\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=4\n[build/Package and run all tests]   | \t\tMap output records=0\n[build/Package and run all tests]   | \t\tInput split bytes=365\n[build/Package and run all tests]   | \t\tSpilled Records=0\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2086141952\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_1_bar.foo_prim=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:04:11,519 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local1586303796_0004_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:11,519 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:04:12,456 Stage-1 map = 100%,  reduce = 0%\n[build/Package and run all tests]   | Ended Job = job_local1586303796_0004\n[build/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[build/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[build/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test3869280240417179012/warehouse2506192061998207805/bar.db/.hive-staging_hive_2023-05-28_18-04-10_805_3662053472090881221-1/-ext-10001\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test3869280240417179012/warehouse2506192061998207805/bar.db/foo_prim\n[build/Package and run all tests]   | 2023-05-28T18:04:12,467 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:12,525 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:04:12,608 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180412_eb0f5210-0c22-4311-a22a-5f46e50a5b44\n[build/Package and run all tests]   | Total jobs = 1\n[build/Package and run all tests]   | Launching Job 1 out of 1\n[build/Package and run all tests]   | Number of reduce tasks determined at compile time: 1\n[build/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[build/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[build/Package and run all tests]   | In order to limit the maximum number of reducers:\n[build/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[build/Package and run all tests]   | In order to set a constant number of reducers:\n[build/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[build/Package and run all tests]   | 2023-05-28T18:04:12,641 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:04:12,649 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:04:12,669 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:04:12,882 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[build/Package and run all tests]   | 2023-05-28T18:04:12,884 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:04:12,917 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:04:13,008 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local368395857_0005\n[build/Package and run all tests]   | 2023-05-28T18:04:13,009 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:04:13,149 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | 2023-05-28T18:04:13,149 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:04:13,150 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:04:13,165 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:04:13,165 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local368395857_0005_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:13,171 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:04:13,173 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test3869280240417179012/hadooptmp8640705252287655941/foo/data_from_string.csv:0+11,/tmp/hiverunner_test3869280240417179012/hadooptmp8640705252287655941/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:04:13,182 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[build/Package and run all tests]   | 2023-05-28T18:04:13,203 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[build/Package and run all tests]   | 2023-05-28T18:04:13,203 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[build/Package and run all tests]   | 2023-05-28T18:04:13,203 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[build/Package and run all tests]   | 2023-05-28T18:04:13,204 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:04:13,204 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[build/Package and run all tests]   | 2023-05-28T18:04:13,213 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[build/Package and run all tests]   | 2023-05-28T18:04:13,231 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:04:13,231 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[build/Package and run all tests]   | 2023-05-28T18:04:13,232 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[build/Package and run all tests]   | 2023-05-28T18:04:13,232 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 74; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:04:13,232 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214384(104857536); length = 13/6553600\n[build/Package and run all tests]   | 2023-05-28T18:04:13,299 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[build/Package and run all tests]   | 2023-05-28T18:04:13,314 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local368395857_0005_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:04:13,316 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test3869280240417179012/hadooptmp8640705252287655941/foo/data_from_file.csv:0+12\n[build/Package and run all tests]   | 2023-05-28T18:04:13,316 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local368395857_0005_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:04:13,317 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local368395857_0005_m_000000_0: Counters: 24\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=203122098\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=208760786\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=4\n[build/Package and run all tests]   | \t\tMap output records=4\n[build/Package and run all tests]   | \t\tMap output bytes=74\n[build/Package and run all tests]   | \t\tMap output materialized bytes=88\n[build/Package and run all tests]   | \t\tInput split bytes=365\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tSpilled Records=4\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2086141952\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_6=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | 2023-05-28T18:04:13,317 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local368395857_0005_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:13,317 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28T18:04:13,322 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[build/Package and run all tests]   | 2023-05-28T18:04:13,322 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local368395857_0005_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:13,331 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:04:13,335 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3f037eb4\n[build/Package and run all tests]   | 2023-05-28T18:04:13,337 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:04:13,363 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1460299392, maxSingleShuffleLimit=365074848, mergeThreshold=963797632, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[build/Package and run all tests]   | 2023-05-28T18:04:13,373 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local368395857_0005_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[build/Package and run all tests]   | 2023-05-28T18:04:13,407 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#1 about to shuffle output of map attempt_local368395857_0005_m_000000_0 decomp: 84 len: 88 to MEMORY\n[build/Package and run all tests]   | 2023-05-28T18:04:13,411 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 84 bytes from map-output for attempt_local368395857_0005_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:13,413 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 84, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->84\n[build/Package and run all tests]   | 2023-05-28T18:04:13,415 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[build/Package and run all tests]   | 2023-05-28T18:04:13,416 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:04:13,417 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[build/Package and run all tests]   | 2023-05-28T18:04:13,488 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:04:13,489 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 76 bytes\n[build/Package and run all tests]   | 2023-05-28T18:04:13,503 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 84 bytes to disk to satisfy reduce memory limit\n[build/Package and run all tests]   | 2023-05-28T18:04:13,504 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 88 bytes from disk\n[build/Package and run all tests]   | 2023-05-28T18:04:13,506 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[build/Package and run all tests]   | 2023-05-28T18:04:13,506 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:04:13,506 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 76 bytes\n[build/Package and run all tests]   | 2023-05-28T18:04:13,507 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:04:13,510 INFO  ExecReducer:99 - conf classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter5117273890938880417.jar]\n[build/Package and run all tests]   | 2023-05-28T18:04:13,510 INFO  ExecReducer:101 - thread classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter5117273890938880417.jar]\n[build/Package and run all tests]   | 2023-05-28T18:04:13,520 INFO  ExecReducer:147 - \n[build/Package and run all tests]   | <SEL>Id =3\n[build/Package and run all tests]   |   <Children>\n[build/Package and run all tests]   |     <FS>Id =4\n[build/Package and run all tests]   |       <Children>\n[build/Package and run all tests]   |       <\\Children>\n[build/Package and run all tests]   |       <Parent>Id = 3 null<\\Parent>\n[build/Package and run all tests]   |     <\\FS>\n[build/Package and run all tests]   |   <\\Children>\n[build/Package and run all tests]   |   <Parent><\\Parent>\n[build/Package and run all tests]   | <\\SEL>\n[build/Package and run all tests]   | 2023-05-28T18:04:13,561 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local368395857_0005_r_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:04:13,563 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[build/Package and run all tests]   | 2023-05-28T18:04:13,563 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local368395857_0005_r_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:04:13,564 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local368395857_0005_r_000000_0: Counters: 29\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=203122306\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=208761048\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tCombine output records=0\n[build/Package and run all tests]   | \t\tReduce input groups=4\n[build/Package and run all tests]   | \t\tReduce shuffle bytes=88\n[build/Package and run all tests]   | \t\tReduce input records=4\n[build/Package and run all tests]   | \t\tReduce output records=0\n[build/Package and run all tests]   | \t\tSpilled Records=4\n[build/Package and run all tests]   | \t\tShuffled Maps =1\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=1\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2086141952\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_0=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_4=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_3=4\n[build/Package and run all tests]   | \tShuffle Errors\n[build/Package and run all tests]   | \t\tBAD_ID=0\n[build/Package and run all tests]   | \t\tCONNECTION=0\n[build/Package and run all tests]   | \t\tIO_ERROR=0\n[build/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[build/Package and run all tests]   | \t\tWRONG_MAP=0\n[build/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:04:13,564 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local368395857_0005_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:13,565 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:04:14,154 Stage-1 map = 100%,  reduce = 100%\n[build/Package and run all tests]   | Ended Job = job_local368395857_0005\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:14,168 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:04:14,182 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.HelloAnnotatedHiveRunnerTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:14,200 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 30.192 s - in com.klarna.hiverunner.examples.HelloAnnotatedHiveRunnerTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.examples.junit4.HelloHiveRunnerTest\n[build/Package and run all tests]   | 2023-05-28T18:04:16,236 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | 2023-05-28T18:04:16,256 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.HelloHiveRunnerTest in /\n[build/Package and run all tests]   | Hive Session ID = 6de39543-54f1-4a18-ac07-5fc7e55d663f\n[build/Package and run all tests]   | 2023-05-28T18:04:17,329 INFO  SessionState:1227 - Hive Session ID = 6de39543-54f1-4a18-ac07-5fc7e55d663f\n[build/Package and run all tests]   | 2023-05-28T18:04:17,844 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:04:18,764 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:04:20,296 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:04:21,407 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:04:24,651 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:04:24,652 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:04:24,819 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 6262331b-5847-4396-84aa-ed6ead7d1cd9\n[build/Package and run all tests]   | 2023-05-28T18:04:25,427 INFO  SessionState:1227 - Hive Session ID = 6262331b-5847-4396-84aa-ed6ead7d1cd9\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:27,258 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:27,578 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.my_schema, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:27,724 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:04:27,745 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:04:28,071 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:04:28,196 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[build/Package and run all tests]   | 2023-05-28T18:04:28,273 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:04:28,316 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:04:28,317 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:04:28,318 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[build/Package and run all tests]   | 2023-05-28T18:04:28,330 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:04:28,331 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:04:28,344 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[build/Package and run all tests]   | 2023-05-28T18:04:28,345 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[build/Package and run all tests]   | 2023-05-28T18:04:28,357 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:04:28,357 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:04:28,363 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:04:28,364 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:04:28,372 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:04:28,372 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:04:28,373 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[build/Package and run all tests]   | 2023-05-28T18:04:28,374 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[build/Package and run all tests]   | 2023-05-28T18:04:28,404 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:04:28,405 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:04:28,407 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:04:28,407 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:04:28,411 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_2146687829' to file:/tmp/hiverunner_tests2881247999452189371/warehouse7871604104098706513/source_db.db/test_table/_SCRATCH0.8339174604743234\n[build/Package and run all tests]   | 2023-05-28T18:04:28,420 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:04:28,420 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:04:28,424 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:04:28,424 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:04:28,489 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:04:28,539 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:04:29,048 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180428_5638426b-24e1-481d-b5b4-4fd349261331\n[build/Package and run all tests]   | Total jobs = 1\n[build/Package and run all tests]   | Launching Job 1 out of 1\n[build/Package and run all tests]   | Number of reduce tasks not specified. Estimated from input data size: 1\n[build/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[build/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[build/Package and run all tests]   | In order to limit the maximum number of reducers:\n[build/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[build/Package and run all tests]   | In order to set a constant number of reducers:\n[build/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[build/Package and run all tests]   | 2023-05-28T18:04:29,414 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[build/Package and run all tests]   | 2023-05-28T18:04:29,434 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[build/Package and run all tests]   | 2023-05-28T18:04:29,453 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[build/Package and run all tests]   | 2023-05-28T18:04:29,453 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[build/Package and run all tests]   | 2023-05-28T18:04:29,474 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:04:29,545 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:04:29,790 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:04:29,811 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:04:29,851 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:04:29,927 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local617947705_0001\n[build/Package and run all tests]   | 2023-05-28T18:04:29,928 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:04:30,346 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | 2023-05-28T18:04:30,348 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:04:30,349 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:04:30,365 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:04:30,366 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local617947705_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:30,418 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:04:30,427 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_tests2881247999452189371/warehouse7871604104098706513/source_db.db/test_table/part-m-2146687829:0+28InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:04:30,520 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[build/Package and run all tests]   | 2023-05-28T18:04:30,520 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[build/Package and run all tests]   | 2023-05-28T18:04:30,520 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[build/Package and run all tests]   | 2023-05-28T18:04:30,521 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[build/Package and run all tests]   | 2023-05-28T18:04:30,542 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[build/Package and run all tests]   | 2023-05-28T18:04:30,542 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[build/Package and run all tests]   | 2023-05-28T18:04:30,542 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[build/Package and run all tests]   | 2023-05-28T18:04:30,542 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:04:30,543 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[build/Package and run all tests]   | 2023-05-28T18:04:30,547 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[build/Package and run all tests]   | 2023-05-28T18:04:30,625 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:04:30,626 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[build/Package and run all tests]   | 2023-05-28T18:04:30,626 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[build/Package and run all tests]   | 2023-05-28T18:04:30,626 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 34; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:04:30,626 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n[build/Package and run all tests]   | 2023-05-28T18:04:30,682 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[build/Package and run all tests]   | 2023-05-28T18:04:30,701 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local617947705_0001_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:04:30,704 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_tests2881247999452189371/warehouse7871604104098706513/source_db.db/test_table/part-m-2146687829:0+28\n[build/Package and run all tests]   | 2023-05-28T18:04:30,704 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local617947705_0001_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:04:30,708 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local617947705_0001_m_000000_0: Counters: 25\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=40624327\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=41758498\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=4\n[build/Package and run all tests]   | \t\tMap output records=2\n[build/Package and run all tests]   | \t\tMap output bytes=34\n[build/Package and run all tests]   | \t\tMap output materialized bytes=44\n[build/Package and run all tests]   | \t\tInput split bytes=270\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tSpilled Records=2\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=1999110144\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_8=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_9=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_7=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | 2023-05-28T18:04:30,708 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local617947705_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:30,710 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28T18:04:30,716 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[build/Package and run all tests]   | 2023-05-28T18:04:30,717 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local617947705_0001_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:30,732 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:04:30,737 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@12fb049\n[build/Package and run all tests]   | 2023-05-28T18:04:30,739 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:04:30,761 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1399377024, maxSingleShuffleLimit=349844256, mergeThreshold=923588864, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[build/Package and run all tests]   | 2023-05-28T18:04:30,766 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local617947705_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[build/Package and run all tests]   | 2023-05-28T18:04:30,802 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#1 about to shuffle output of map attempt_local617947705_0001_m_000000_0 decomp: 40 len: 44 to MEMORY\n[build/Package and run all tests]   | 2023-05-28T18:04:30,805 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 40 bytes from map-output for attempt_local617947705_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:30,808 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 40, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->40\n[build/Package and run all tests]   | 2023-05-28T18:04:30,810 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[build/Package and run all tests]   | 2023-05-28T18:04:30,811 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:04:30,812 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[build/Package and run all tests]   | 2023-05-28T18:04:30,869 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:04:30,869 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[build/Package and run all tests]   | 2023-05-28T18:04:30,875 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 40 bytes to disk to satisfy reduce memory limit\n[build/Package and run all tests]   | 2023-05-28T18:04:30,876 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 44 bytes from disk\n[build/Package and run all tests]   | 2023-05-28T18:04:30,878 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[build/Package and run all tests]   | 2023-05-28T18:04:30,878 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:04:30,878 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[build/Package and run all tests]   | 2023-05-28T18:04:30,879 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:04:30,883 INFO  ExecReducer:99 - conf classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter618555745041378554.jar]\n[build/Package and run all tests]   | 2023-05-28T18:04:30,883 INFO  ExecReducer:101 - thread classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter618555745041378554.jar]\n[build/Package and run all tests]   | 2023-05-28T18:04:30,897 INFO  ExecReducer:147 - \n[build/Package and run all tests]   | <GBY>Id =4\n[build/Package and run all tests]   |   <Children>\n[build/Package and run all tests]   |     <FS>Id =6\n[build/Package and run all tests]   |       <Children>\n[build/Package and run all tests]   |       <\\Children>\n[build/Package and run all tests]   |       <Parent>Id = 4 null<\\Parent>\n[build/Package and run all tests]   |     <\\FS>\n[build/Package and run all tests]   |   <\\Children>\n[build/Package and run all tests]   |   <Parent><\\Parent>\n[build/Package and run all tests]   | <\\GBY>\n[build/Package and run all tests]   | 2023-05-28T18:04:30,903 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[build/Package and run all tests]   | 2023-05-28T18:04:30,944 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local617947705_0001_r_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:04:30,946 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[build/Package and run all tests]   | 2023-05-28T18:04:30,946 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local617947705_0001_r_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:04:30,947 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local617947705_0001_r_000000_0: Counters: 29\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=40624447\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=41758679\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tCombine output records=0\n[build/Package and run all tests]   | \t\tReduce input groups=2\n[build/Package and run all tests]   | \t\tReduce shuffle bytes=44\n[build/Package and run all tests]   | \t\tReduce input records=2\n[build/Package and run all tests]   | \t\tReduce output records=0\n[build/Package and run all tests]   | \t\tSpilled Records=2\n[build/Package and run all tests]   | \t\tShuffled Maps =1\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=1\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=1999110144\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_1_my_schema.result=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_4=2\n[build/Package and run all tests]   | \tShuffle Errors\n[build/Package and run all tests]   | \t\tBAD_ID=0\n[build/Package and run all tests]   | \t\tCONNECTION=0\n[build/Package and run all tests]   | \t\tIO_ERROR=0\n[build/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[build/Package and run all tests]   | \t\tWRONG_MAP=0\n[build/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:04:30,947 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local617947705_0001_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:30,948 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:04:31,376 Stage-1 map = 100%,  reduce = 100%\n[build/Package and run all tests]   | Ended Job = job_local617947705_0001\n[build/Package and run all tests]   | Loading data to table my_schema.result\n[build/Package and run all tests]   | 2023-05-28T18:04:31,403 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:31,596 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:31,658 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:04:31,670 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:04:31,693 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.HelloHiveRunnerTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:31,752 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.606 s - in com.klarna.hiverunner.examples.junit4.HelloHiveRunnerTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest\n[build/Package and run all tests]   | 2023-05-28T18:04:34,076 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | 2023-05-28T18:04:34,096 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest in /\n[build/Package and run all tests]   | Hive Session ID = 158243e1-5a93-4a72-a000-8c41b578eb61\n[build/Package and run all tests]   | 2023-05-28T18:04:35,019 INFO  SessionState:1227 - Hive Session ID = 158243e1-5a93-4a72-a000-8c41b578eb61\n[build/Package and run all tests]   | 2023-05-28T18:04:35,557 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:04:36,495 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:04:38,141 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:04:39,327 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:04:42,531 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:04:42,532 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:04:42,690 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 012d7c1c-0483-49b9-bbc0-04d2af58ed0e\n[build/Package and run all tests]   | 2023-05-28T18:04:43,240 INFO  SessionState:1227 - Hive Session ID = 012d7c1c-0483-49b9-bbc0-04d2af58ed0e\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:45,197 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.bar, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:46,064 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180445_4633bdcc-7331-40ea-a246-d3c62afb307e\n[build/Package and run all tests]   | Total jobs = 3\n[build/Package and run all tests]   | Launching Job 1 out of 3\n[build/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[build/Package and run all tests]   | 2023-05-28T18:04:46,371 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[build/Package and run all tests]   | 2023-05-28T18:04:46,391 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[build/Package and run all tests]   | 2023-05-28T18:04:46,409 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[build/Package and run all tests]   | 2023-05-28T18:04:46,410 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[build/Package and run all tests]   | 2023-05-28T18:04:46,439 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:04:46,510 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:04:46,792 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[build/Package and run all tests]   | 2023-05-28T18:04:46,813 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:04:46,867 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:04:46,960 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local2012034456_0001\n[build/Package and run all tests]   | 2023-05-28T18:04:46,961 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:04:47,184 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:04:47,187 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:04:47,188 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:04:47,200 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:04:47,206 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local2012034456_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:47,260 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:04:47,291 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_tests1772066009358669505/hadooptmp7010710289133342055/foo/data_from_string.csv:0+11,/tmp/hiverunner_tests1772066009358669505/hadooptmp7010710289133342055/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:04:47,374 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[build/Package and run all tests]   | 2023-05-28T18:04:47,375 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[build/Package and run all tests]   | 2023-05-28T18:04:47,375 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[build/Package and run all tests]   | 2023-05-28T18:04:47,375 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[build/Package and run all tests]   | 2023-05-28T18:04:47,416 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[build/Package and run all tests]   | 2023-05-28T18:04:47,418 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[build/Package and run all tests]   | 2023-05-28T18:04:47,533 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:04:47,542 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local2012034456_0001_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:04:47,544 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_tests1772066009358669505/hadooptmp7010710289133342055/foo/data_from_file.csv:0+12\n[build/Package and run all tests]   | 2023-05-28T18:04:47,544 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local2012034456_0001_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:04:47,548 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local2012034456_0001_m_000000_0: Counters: 24\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=40624403\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=41752164\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=4\n[build/Package and run all tests]   | \t\tMap output records=0\n[build/Package and run all tests]   | \t\tInput split bytes=367\n[build/Package and run all tests]   | \t\tSpilled Records=0\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=58\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2000683008\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_1_bar.foo_prim=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:04:47,548 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local2012034456_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:47,550 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:04:48,200 Stage-1 map = 100%,  reduce = 0%\n[build/Package and run all tests]   | Ended Job = job_local2012034456_0001\n[build/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[build/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[build/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_tests1772066009358669505/warehouse1489964532772434721/bar.db/.hive-staging_hive_2023-05-28_18-04-45_523_3222536820412319177-1/-ext-10001\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_tests1772066009358669505/warehouse1489964532772434721/bar.db/foo_prim\n[build/Package and run all tests]   | 2023-05-28T18:04:48,233 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:48,311 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | WARNING: Order/Sort by without limit in sub query or view [a] is removed, as it's pointless and bad for performance.\n[build/Package and run all tests]   | 2023-05-28T18:04:48,413 WARN  org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer:531 - Cannot determine basic stats for table: bar@foo_prim from metastore. Falling back.\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:48,473 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:04:48,484 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:04:48,500 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:48,556 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | 2023-05-28T18:04:48,563 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest in /\n[build/Package and run all tests]   | Hive Session ID = 351b7a41-9ac8-4a22-9efe-7868ec43a23e\n[build/Package and run all tests]   | 2023-05-28T18:04:48,623 INFO  SessionState:1227 - Hive Session ID = 351b7a41-9ac8-4a22-9efe-7868ec43a23e\n[build/Package and run all tests]   | 2023-05-28T18:04:48,661 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:04:48,663 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:04:49,046 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:04:51,095 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = cca77527-0480-4081-8616-e7643d4d02ef\n[build/Package and run all tests]   | 2023-05-28T18:04:51,183 INFO  SessionState:1227 - Hive Session ID = cca77527-0480-4081-8616-e7643d4d02ef\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:51,265 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.bar, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:51,657 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180451_17c7abd7-6c21-43a8-b97b-38f4fa276b31\n[build/Package and run all tests]   | Total jobs = 3\n[build/Package and run all tests]   | Launching Job 1 out of 3\n[build/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[build/Package and run all tests]   | 2023-05-28T18:04:51,706 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:04:51,720 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:04:51,735 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:04:51,903 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[build/Package and run all tests]   | 2023-05-28T18:04:51,905 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:04:51,929 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:04:51,967 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local1829869750_0002\n[build/Package and run all tests]   | 2023-05-28T18:04:51,967 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:04:52,125 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:04:52,126 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:04:52,127 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:04:52,131 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:04:52,131 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local1829869750_0002_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:52,137 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:04:52,138 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_tests622824758172833808/hadooptmp993743252736242670/foo/data_from_string.csv:0+11,/tmp/hiverunner_tests622824758172833808/hadooptmp993743252736242670/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:04:52,147 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[build/Package and run all tests]   | 2023-05-28T18:04:52,170 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:04:52,174 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1829869750_0002_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:04:52,176 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_tests622824758172833808/hadooptmp993743252736242670/foo/data_from_file.csv:0+12\n[build/Package and run all tests]   | 2023-05-28T18:04:52,176 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1829869750_0002_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:04:52,177 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1829869750_0002_m_000000_0: Counters: 24\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=81248845\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=83504220\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=4\n[build/Package and run all tests]   | \t\tMap output records=0\n[build/Package and run all tests]   | \t\tInput split bytes=363\n[build/Package and run all tests]   | \t\tSpilled Records=0\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2086666240\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_1_bar.foo_prim=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:04:52,177 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local1829869750_0002_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:52,178 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:04:53,131 Stage-1 map = 100%,  reduce = 0%\n[build/Package and run all tests]   | Ended Job = job_local1829869750_0002\n[build/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[build/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[build/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_tests622824758172833808/warehouse2724932767154032277/bar.db/.hive-staging_hive_2023-05-28_18-04-51_463_1086333491767131877-2/-ext-10001\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_tests622824758172833808/warehouse2724932767154032277/bar.db/foo_prim\n[build/Package and run all tests]   | 2023-05-28T18:04:53,143 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:53,194 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:53,246 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 2\n[build/Package and run all tests]   | 2023-05-28T18:04:53,250 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:53,271 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | 2023-05-28T18:04:53,277 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest in /\n[build/Package and run all tests]   | Hive Session ID = f8179c5e-218b-4c90-987e-44e938a7c569\n[build/Package and run all tests]   | 2023-05-28T18:04:53,326 INFO  SessionState:1227 - Hive Session ID = f8179c5e-218b-4c90-987e-44e938a7c569\n[build/Package and run all tests]   | 2023-05-28T18:04:53,373 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:04:53,375 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:04:53,767 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:04:55,820 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = b9076043-b9e8-461c-822b-220c6c88fce6\n[build/Package and run all tests]   | 2023-05-28T18:04:55,917 INFO  SessionState:1227 - Hive Session ID = b9076043-b9e8-461c-822b-220c6c88fce6\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:56,080 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.bar, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:56,441 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180456_68673436-d533-42d6-953c-d2342813310c\n[build/Package and run all tests]   | Total jobs = 3\n[build/Package and run all tests]   | Launching Job 1 out of 3\n[build/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[build/Package and run all tests]   | 2023-05-28T18:04:56,513 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:04:56,527 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:04:56,544 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:04:56,729 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[build/Package and run all tests]   | 2023-05-28T18:04:56,732 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:04:56,768 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:04:56,831 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local14746046_0003\n[build/Package and run all tests]   | 2023-05-28T18:04:56,832 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:04:57,049 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:04:57,049 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:04:57,050 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:04:57,053 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:04:57,054 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local14746046_0003_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:57,059 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:04:57,061 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_tests8140625997738979500/hadooptmp2239700113729387832/foo/data_from_string.csv:0+11,/tmp/hiverunner_tests8140625997738979500/hadooptmp2239700113729387832/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:04:57,068 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[build/Package and run all tests]   | 2023-05-28T18:04:57,112 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:04:57,115 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local14746046_0003_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:04:57,116 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_tests8140625997738979500/hadooptmp2239700113729387832/foo/data_from_file.csv:0+12\n[build/Package and run all tests]   | 2023-05-28T18:04:57,116 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local14746046_0003_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:04:57,117 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local14746046_0003_m_000000_0: Counters: 24\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=121873271\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=125248618\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=4\n[build/Package and run all tests]   | \t\tMap output records=0\n[build/Package and run all tests]   | \t\tInput split bytes=367\n[build/Package and run all tests]   | \t\tSpilled Records=0\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2086666240\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_1_bar.foo_prim=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:04:57,117 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local14746046_0003_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:04:57,117 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:04:58,054 Stage-1 map = 100%,  reduce = 0%\n[build/Package and run all tests]   | Ended Job = job_local14746046_0003\n[build/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[build/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[build/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_tests8140625997738979500/warehouse4703791818433114746/bar.db/.hive-staging_hive_2023-05-28_18-04-56_254_7261858196435192701-3/-ext-10001\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_tests8140625997738979500/warehouse4703791818433114746/bar.db/foo_prim\n[build/Package and run all tests]   | 2023-05-28T18:04:58,066 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:58,120 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:58,214 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:04:58,217 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:04:58,240 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | 2023-05-28T18:04:58,247 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest in /\n[build/Package and run all tests]   | Hive Session ID = a787a9a8-b905-48cc-bff7-66feb278b0b6\n[build/Package and run all tests]   | 2023-05-28T18:04:58,299 INFO  SessionState:1227 - Hive Session ID = a787a9a8-b905-48cc-bff7-66feb278b0b6\n[build/Package and run all tests]   | 2023-05-28T18:04:58,362 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:04:58,365 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:04:58,757 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:00,953 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = a205998f-2f6b-4e89-9006-1aab6a7ffe84\n[build/Package and run all tests]   | 2023-05-28T18:05:01,097 INFO  SessionState:1227 - Hive Session ID = a205998f-2f6b-4e89-9006-1aab6a7ffe84\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:01,167 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.bar, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:01,482 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180501_131465db-bb4f-45fc-bb99-7911fdc38a8b\n[build/Package and run all tests]   | Total jobs = 3\n[build/Package and run all tests]   | Launching Job 1 out of 3\n[build/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[build/Package and run all tests]   | 2023-05-28T18:05:01,509 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:05:01,517 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:05:01,533 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:05:01,685 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[build/Package and run all tests]   | 2023-05-28T18:05:01,686 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:05:01,711 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:05:01,739 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local1500343632_0004\n[build/Package and run all tests]   | 2023-05-28T18:05:01,739 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:05:01,919 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | 2023-05-28T18:05:01,920 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:05:01,921 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:05:01,923 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:05:01,923 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local1500343632_0004_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:05:01,927 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:05:01,928 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_tests3372328139520818731/hadooptmp690817511494742028/foo/data_from_string.csv:0+11,/tmp/hiverunner_tests3372328139520818731/hadooptmp690817511494742028/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:05:01,936 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[build/Package and run all tests]   | 2023-05-28T18:05:01,955 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:05:01,958 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1500343632_0004_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:05:01,959 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_tests3372328139520818731/hadooptmp690817511494742028/foo/data_from_file.csv:0+12\n[build/Package and run all tests]   | 2023-05-28T18:05:01,959 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1500343632_0004_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:05:01,960 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1500343632_0004_m_000000_0: Counters: 24\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=162497701\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=167000775\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=4\n[build/Package and run all tests]   | \t\tMap output records=0\n[build/Package and run all tests]   | \t\tInput split bytes=365\n[build/Package and run all tests]   | \t\tSpilled Records=0\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2085617664\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_1_bar.foo_prim=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:05:01,960 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local1500343632_0004_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:05:01,960 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:05:02,924 Stage-1 map = 100%,  reduce = 0%\n[build/Package and run all tests]   | Ended Job = job_local1500343632_0004\n[build/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[build/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[build/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_tests3372328139520818731/warehouse7073301538602658631/bar.db/.hive-staging_hive_2023-05-28_18-05-01_330_2711969647801244919-4/-ext-10001\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_tests3372328139520818731/warehouse7073301538602658631/bar.db/foo_prim\n[build/Package and run all tests]   | 2023-05-28T18:05:02,933 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:02,974 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:03,051 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180502_4ea4b74a-a670-40bc-afd6-cd52e45090c5\n[build/Package and run all tests]   | Total jobs = 1\n[build/Package and run all tests]   | Launching Job 1 out of 1\n[build/Package and run all tests]   | Number of reduce tasks determined at compile time: 1\n[build/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[build/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[build/Package and run all tests]   | In order to limit the maximum number of reducers:\n[build/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[build/Package and run all tests]   | In order to set a constant number of reducers:\n[build/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[build/Package and run all tests]   | 2023-05-28T18:05:03,078 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:05:03,084 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:05:03,092 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:05:03,237 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[build/Package and run all tests]   | 2023-05-28T18:05:03,238 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:05:03,262 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:05:03,303 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local354919119_0005\n[build/Package and run all tests]   | 2023-05-28T18:05:03,304 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:05:03,409 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | 2023-05-28T18:05:03,410 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:05:03,410 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:05:03,412 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:05:03,412 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local354919119_0005_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:05:03,414 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:05:03,415 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_tests3372328139520818731/hadooptmp690817511494742028/foo/data_from_string.csv:0+11,/tmp/hiverunner_tests3372328139520818731/hadooptmp690817511494742028/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:05:03,424 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[build/Package and run all tests]   | 2023-05-28T18:05:03,444 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[build/Package and run all tests]   | 2023-05-28T18:05:03,445 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[build/Package and run all tests]   | 2023-05-28T18:05:03,445 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[build/Package and run all tests]   | 2023-05-28T18:05:03,445 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:05:03,445 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[build/Package and run all tests]   | 2023-05-28T18:05:03,449 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[build/Package and run all tests]   | 2023-05-28T18:05:03,471 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:05:03,471 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[build/Package and run all tests]   | 2023-05-28T18:05:03,471 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[build/Package and run all tests]   | 2023-05-28T18:05:03,471 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 74; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:05:03,471 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214384(104857536); length = 13/6553600\n[build/Package and run all tests]   | 2023-05-28T18:05:03,522 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[build/Package and run all tests]   | 2023-05-28T18:05:03,528 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local354919119_0005_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:05:03,529 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_tests3372328139520818731/hadooptmp690817511494742028/foo/data_from_file.csv:0+12\n[build/Package and run all tests]   | 2023-05-28T18:05:03,530 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local354919119_0005_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:05:03,530 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local354919119_0005_m_000000_0: Counters: 24\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=203122102\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=208753244\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=4\n[build/Package and run all tests]   | \t\tMap output records=4\n[build/Package and run all tests]   | \t\tMap output bytes=74\n[build/Package and run all tests]   | \t\tMap output materialized bytes=88\n[build/Package and run all tests]   | \t\tInput split bytes=365\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tSpilled Records=4\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2085617664\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_6=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | 2023-05-28T18:05:03,530 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local354919119_0005_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:05:03,531 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28T18:05:03,534 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[build/Package and run all tests]   | 2023-05-28T18:05:03,534 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local354919119_0005_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:05:03,542 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:05:03,545 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@17089d22\n[build/Package and run all tests]   | 2023-05-28T18:05:03,547 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:05:03,566 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1459932288, maxSingleShuffleLimit=364983072, mergeThreshold=963555328, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[build/Package and run all tests]   | 2023-05-28T18:05:03,570 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local354919119_0005_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[build/Package and run all tests]   | 2023-05-28T18:05:03,604 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#1 about to shuffle output of map attempt_local354919119_0005_m_000000_0 decomp: 84 len: 88 to MEMORY\n[build/Package and run all tests]   | 2023-05-28T18:05:03,608 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 84 bytes from map-output for attempt_local354919119_0005_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:05:03,610 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 84, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->84\n[build/Package and run all tests]   | 2023-05-28T18:05:03,612 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[build/Package and run all tests]   | 2023-05-28T18:05:03,613 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:05:03,613 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[build/Package and run all tests]   | 2023-05-28T18:05:03,723 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:05:03,723 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 76 bytes\n[build/Package and run all tests]   | 2023-05-28T18:05:03,728 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 84 bytes to disk to satisfy reduce memory limit\n[build/Package and run all tests]   | 2023-05-28T18:05:03,728 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 88 bytes from disk\n[build/Package and run all tests]   | 2023-05-28T18:05:03,730 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[build/Package and run all tests]   | 2023-05-28T18:05:03,730 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:05:03,730 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 76 bytes\n[build/Package and run all tests]   | 2023-05-28T18:05:03,731 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:05:03,734 INFO  ExecReducer:99 - conf classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter9005167082500000487.jar]\n[build/Package and run all tests]   | 2023-05-28T18:05:03,735 INFO  ExecReducer:101 - thread classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter9005167082500000487.jar]\n[build/Package and run all tests]   | 2023-05-28T18:05:03,743 INFO  ExecReducer:147 - \n[build/Package and run all tests]   | <SEL>Id =3\n[build/Package and run all tests]   |   <Children>\n[build/Package and run all tests]   |     <FS>Id =4\n[build/Package and run all tests]   |       <Children>\n[build/Package and run all tests]   |       <\\Children>\n[build/Package and run all tests]   |       <Parent>Id = 3 null<\\Parent>\n[build/Package and run all tests]   |     <\\FS>\n[build/Package and run all tests]   |   <\\Children>\n[build/Package and run all tests]   |   <Parent><\\Parent>\n[build/Package and run all tests]   | <\\SEL>\n[build/Package and run all tests]   | 2023-05-28T18:05:03,775 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local354919119_0005_r_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:05:03,776 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[build/Package and run all tests]   | 2023-05-28T18:05:03,777 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local354919119_0005_r_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:05:03,777 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local354919119_0005_r_000000_0: Counters: 29\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=203122310\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=208753506\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tCombine output records=0\n[build/Package and run all tests]   | \t\tReduce input groups=4\n[build/Package and run all tests]   | \t\tReduce shuffle bytes=88\n[build/Package and run all tests]   | \t\tReduce input records=4\n[build/Package and run all tests]   | \t\tReduce output records=0\n[build/Package and run all tests]   | \t\tSpilled Records=4\n[build/Package and run all tests]   | \t\tShuffled Maps =1\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=1\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2085617664\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_0=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_4=4\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_3=4\n[build/Package and run all tests]   | \tShuffle Errors\n[build/Package and run all tests]   | \t\tBAD_ID=0\n[build/Package and run all tests]   | \t\tCONNECTION=0\n[build/Package and run all tests]   | \t\tIO_ERROR=0\n[build/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[build/Package and run all tests]   | \t\tWRONG_MAP=0\n[build/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:05:03,777 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local354919119_0005_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:05:03,778 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:05:04,415 Stage-1 map = 100%,  reduce = 100%\n[build/Package and run all tests]   | Ended Job = job_local354919119_0005\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:04,432 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:05:04,449 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:04,468 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 30.663 s - in com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.examples.junit4.SetHiveConfValuesTest\n[build/Package and run all tests]   | 2023-05-28T18:05:06,534 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | 2023-05-28T18:05:06,553 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.SetHiveConfValuesTest in /\n[build/Package and run all tests]   | Hive Session ID = 146eaaf2-9c07-463e-a9fa-97cd29929cdb\n[build/Package and run all tests]   | 2023-05-28T18:05:07,637 INFO  SessionState:1227 - Hive Session ID = 146eaaf2-9c07-463e-a9fa-97cd29929cdb\n[build/Package and run all tests]   | 2023-05-28T18:05:08,177 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:05:09,062 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:10,649 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:14,513 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:05:14,514 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:05:14,684 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = e3fc605f-6c6b-4797-aad7-fa2076c96c37\n[build/Package and run all tests]   | 2023-05-28T18:05:15,225 INFO  SessionState:1227 - Hive Session ID = e3fc605f-6c6b-4797-aad7-fa2076c96c37\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:17,216 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:17,613 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:17,643 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:18,047 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:18,160 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[build/Package and run all tests]   | 2023-05-28T18:05:18,260 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:18,302 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:18,303 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:18,304 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[build/Package and run all tests]   | 2023-05-28T18:05:18,318 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:18,318 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:18,337 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[build/Package and run all tests]   | 2023-05-28T18:05:18,338 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[build/Package and run all tests]   | 2023-05-28T18:05:18,349 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:18,350 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:18,357 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:18,358 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:18,368 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:18,368 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:18,369 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[build/Package and run all tests]   | 2023-05-28T18:05:18,370 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[build/Package and run all tests]   | 2023-05-28T18:05:18,409 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:18,410 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:18,412 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:18,413 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:18,416 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_393965538' to file:/tmp/hiverunner_tests3766075491101444235/warehouse3473694901266054549/source_db.db/table_a/_SCRATCH0.8886246079772666\n[build/Package and run all tests]   | 2023-05-28T18:05:18,425 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:18,426 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:18,429 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:18,430 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:18,487 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:18,545 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:18,890 WARN  org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer:531 - Cannot determine basic stats for table: source_db@table_a from metastore. Falling back.\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:18,932 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:05:18,977 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | Hello World 2023-05-28T18:05:19,009 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.SetHiveConfValuesTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:19,062 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.613 s - in com.klarna.hiverunner.examples.junit4.SetHiveConfValuesTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.examples.junit4.InsertTestDataTest\n[build/Package and run all tests]   | 2023-05-28T18:05:21,122 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | 2023-05-28T18:05:21,143 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.InsertTestDataTest in /\n[build/Package and run all tests]   | Hive Session ID = 327df98e-1ab5-406d-8d84-ba1974d31c52\n[build/Package and run all tests]   | 2023-05-28T18:05:22,244 INFO  SessionState:1227 - Hive Session ID = 327df98e-1ab5-406d-8d84-ba1974d31c52\n[build/Package and run all tests]   | 2023-05-28T18:05:22,821 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:05:23,811 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:25,674 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:29,117 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:05:29,118 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:05:29,303 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = e227f4d9-fa6f-4853-b984-d5736a88671f\n[build/Package and run all tests]   | 2023-05-28T18:05:29,864 INFO  SessionState:1227 - Hive Session ID = e227f4d9-fa6f-4853-b984-d5736a88671f\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:31,854 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:32,223 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:32,271 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:32,558 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:32,655 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[build/Package and run all tests]   | 2023-05-28T18:05:32,734 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:32,776 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:32,776 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:32,777 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[build/Package and run all tests]   | 2023-05-28T18:05:32,791 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:32,791 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:32,809 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[build/Package and run all tests]   | 2023-05-28T18:05:32,810 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[build/Package and run all tests]   | 2023-05-28T18:05:32,822 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:32,822 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:32,829 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:32,829 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:32,838 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:32,839 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:32,839 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[build/Package and run all tests]   | 2023-05-28T18:05:32,841 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[build/Package and run all tests]   | 2023-05-28T18:05:32,902 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:32,903 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:32,906 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:32,906 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:32,910 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_1996715341' to file:/tmp/hiverunner_tests6702639436759343197/warehouse1668954914083784598/source_db.db/test_table/_SCRATCH0.8003241554323486\n[build/Package and run all tests]   | 2023-05-28T18:05:32,920 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:32,920 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:32,924 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:32,924 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:32,972 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:33,024 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:33,292 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:05:33,369 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | Result from insertRowsFromCode:\n[build/Package and run all tests]   | [Value1, 1, true]\n[build/Package and run all tests]   | [Value2, 99, false]\n[build/Package and run all tests]   | 2023-05-28T18:05:33,400 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.InsertTestDataTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:33,457 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | 2023-05-28T18:05:33,466 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.InsertTestDataTest in /\n[build/Package and run all tests]   | Hive Session ID = 4279ba01-dd57-4232-9186-25fbb6da4181\n[build/Package and run all tests]   | 2023-05-28T18:05:33,515 INFO  SessionState:1227 - Hive Session ID = 4279ba01-dd57-4232-9186-25fbb6da4181\n[build/Package and run all tests]   | 2023-05-28T18:05:33,534 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:05:33,537 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:33,989 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:36,018 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 41837ce6-1086-4f48-a0bb-7ee58ef71e9c\n[build/Package and run all tests]   | 2023-05-28T18:05:36,138 INFO  SessionState:1227 - Hive Session ID = 41837ce6-1086-4f48-a0bb-7ee58ef71e9c\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:36,232 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:36,460 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:36,463 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:36,644 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:36,784 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:36,796 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:36,797 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:36,799 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:36,800 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:36,826 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:36,827 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:36,831 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:36,831 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:36,836 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:36,836 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:36,886 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:36,886 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:36,888 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:36,889 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:36,891 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_790217815' to file:/tmp/hiverunner_tests5806069853856698935/warehouse2384136441234481619/source_db.db/test_table/_SCRATCH0.020144626407172628\n[build/Package and run all tests]   | 2023-05-28T18:05:36,897 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:36,897 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:36,901 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:36,901 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:36,994 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:37,040 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:37,098 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | Result from insertRowsFromTsvFile:\n[build/Package and run all tests]   | [textA, 42, true]\n[build/Package and run all tests]   | [textB, 3, true]\n[build/Package and run all tests]   | [textC, 99, false]\n[build/Package and run all tests]   | 2023-05-28T18:05:37,102 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.InsertTestDataTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:37,122 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | 2023-05-28T18:05:37,126 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.InsertTestDataTest in /\n[build/Package and run all tests]   | Hive Session ID = cb2d09ed-4320-4066-9870-6eeac3c429d0\n[build/Package and run all tests]   | 2023-05-28T18:05:37,173 INFO  SessionState:1227 - Hive Session ID = cb2d09ed-4320-4066-9870-6eeac3c429d0\n[build/Package and run all tests]   | 2023-05-28T18:05:37,191 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:05:37,193 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:37,580 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:39,867 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 33415b2e-2fcd-46b0-bb14-d49066aa90a3\n[build/Package and run all tests]   | 2023-05-28T18:05:40,014 INFO  SessionState:1227 - Hive Session ID = 33415b2e-2fcd-46b0-bb14-d49066aa90a3\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:40,115 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:40,341 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:40,343 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:40,494 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:40,613 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:40,625 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:40,626 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:40,629 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:40,629 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:40,649 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:40,649 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:40,652 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:40,653 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:40,656 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:40,657 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:40,675 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:40,675 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:40,678 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:40,678 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:40,681 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_210212811' to file:/tmp/hiverunner_tests450637981994726684/warehouse3200528416566092612/source_db.db/test_table/_SCRATCH0.48602643795951017\n[build/Package and run all tests]   | 2023-05-28T18:05:40,687 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:40,688 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:40,692 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:40,692 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:40,738 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:40,782 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:40,842 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | Result from insertRowsFromCodeWithSelectedColumns:\n[build/Package and run all tests]   | [Value1, null, true]\n[build/Package and run all tests]   | [Value2, null, false]\n[build/Package and run all tests]   | 2023-05-28T18:05:40,844 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.InsertTestDataTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:40,864 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | 2023-05-28T18:05:40,869 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.InsertTestDataTest in /\n[build/Package and run all tests]   | Hive Session ID = a407cf52-ed38-45d1-8c5b-1ba28c838c35\n[build/Package and run all tests]   | 2023-05-28T18:05:40,917 INFO  SessionState:1227 - Hive Session ID = a407cf52-ed38-45d1-8c5b-1ba28c838c35\n[build/Package and run all tests]   | 2023-05-28T18:05:40,938 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:05:40,940 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:41,326 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:43,342 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 794c164f-71a4-47a2-98aa-ab9d831ffc96\n[build/Package and run all tests]   | 2023-05-28T18:05:43,446 INFO  SessionState:1227 - Hive Session ID = 794c164f-71a4-47a2-98aa-ab9d831ffc96\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:43,521 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:43,776 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:43,778 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:43,928 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:44,043 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:44,054 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:44,055 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:44,058 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:44,058 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:44,115 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:44,115 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:44,118 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:44,118 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:44,122 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:44,122 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:44,174 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:44,175 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:44,177 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:44,177 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:44,180 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_535801314' to file:/tmp/hiverunner_tests7343857867797487804/warehouse2618492988274728509/source_db.db/test_table2/_SCRATCH0.3120131076507291/col_c=A\n[build/Package and run all tests]   | 2023-05-28T18:05:44,184 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:44,184 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:44,188 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:44,188 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:44,232 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:44,386 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:44,436 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:44,519 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:44,532 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:44,533 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:44,536 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:44,536 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:44,557 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:44,557 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:44,560 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:44,560 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:44,564 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:44,564 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:44,584 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:44,584 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:44,587 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:44,587 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:44,590 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_34557157' to file:/tmp/hiverunner_tests7343857867797487804/warehouse2618492988274728509/source_db.db/test_table2/_SCRATCH0.7051358137380856/col_c=B\n[build/Package and run all tests]   | 2023-05-28T18:05:44,595 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:44,595 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:44,598 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:44,598 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:44,646 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:44,753 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:44,888 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:05:44,912 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | Result from insertRowsIntoPartitionedTableStoredAsSequencefileWithCustomDelimiterAndNullValue:\n[build/Package and run all tests]   | [textA, 42, A]\n[build/Package and run all tests]   | [null, 3, A]\n[build/Package and run all tests]   | [textC, 99, B]\n[build/Package and run all tests]   | 2023-05-28T18:05:44,914 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.InsertTestDataTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:44,931 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | 2023-05-28T18:05:44,938 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.InsertTestDataTest in /\n[build/Package and run all tests]   | Hive Session ID = 114f8bb6-d532-47ae-bf03-03b8e34baf59\n[build/Package and run all tests]   | 2023-05-28T18:05:44,986 INFO  SessionState:1227 - Hive Session ID = 114f8bb6-d532-47ae-bf03-03b8e34baf59\n[build/Package and run all tests]   | 2023-05-28T18:05:45,004 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:05:45,006 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:45,483 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:47,723 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = a50e38cc-c26f-474f-b064-9bd738ba965e\n[build/Package and run all tests]   | 2023-05-28T18:05:47,819 INFO  SessionState:1227 - Hive Session ID = a50e38cc-c26f-474f-b064-9bd738ba965e\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:47,881 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:48,052 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:48,054 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:48,175 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:48,286 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:48,296 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:48,296 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:48,299 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:48,299 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:48,313 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:48,313 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:48,317 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:48,317 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:48,320 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:48,320 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:48,336 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:48,336 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:48,338 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:48,339 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:48,341 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_1548140312' to file:/tmp/hiverunner_tests2873778687948050433/warehouse3281563226134638124/source_db.db/test_table/_SCRATCH0.5662426539924732\n[build/Package and run all tests]   | 2023-05-28T18:05:48,345 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:48,345 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:48,347 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:48,348 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:48,391 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:48,426 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:48,469 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | Result from insertRowsFromTsvFileWithSubsetHeader:\n[build/Package and run all tests]   | [textA, null, true]\n[build/Package and run all tests]   | [textB, null, true]\n[build/Package and run all tests]   | [textC, null, false]\n[build/Package and run all tests]   | 2023-05-28T18:05:48,471 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.InsertTestDataTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:48,485 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | 2023-05-28T18:05:48,490 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.InsertTestDataTest in /\n[build/Package and run all tests]   | Hive Session ID = 01492bf3-0a02-4af1-9e85-30e79ee1aa92\n[build/Package and run all tests]   | 2023-05-28T18:05:48,536 INFO  SessionState:1227 - Hive Session ID = 01492bf3-0a02-4af1-9e85-30e79ee1aa92\n[build/Package and run all tests]   | 2023-05-28T18:05:48,555 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:05:48,557 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:48,996 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:50,944 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = f414bb6a-0767-4ebf-978a-b3eedfb4db62\n[build/Package and run all tests]   | 2023-05-28T18:05:51,053 INFO  SessionState:1227 - Hive Session ID = f414bb6a-0767-4ebf-978a-b3eedfb4db62\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:51,122 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:51,305 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:51,308 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:51,428 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:51,534 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:51,541 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:51,541 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:51,543 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:51,544 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:51,567 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:51,567 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:51,570 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:51,570 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:51,573 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:51,573 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:51,601 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:51,601 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:51,603 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:51,604 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:51,607 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_533038508' to file:/tmp/hiverunner_tests3694695947680826671/warehouse2251771603933443609/source_db.db/test_table/_SCRATCH0.3214451561950532\n[build/Package and run all tests]   | 2023-05-28T18:05:51,611 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:51,612 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:51,615 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:05:51,615 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:05:51,658 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:05:51,694 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:51,733 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | Result from insertRowsFromTsvFileWithHeader:\n[build/Package and run all tests]   | [textA, 42, true]\n[build/Package and run all tests]   | [textB, 3, true]\n[build/Package and run all tests]   | [textC, 99, false]\n[build/Package and run all tests]   | 2023-05-28T18:05:51,735 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.InsertTestDataTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:05:51,747 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 30.724 s - in com.klarna.hiverunner.examples.junit4.InsertTestDataTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.examples.SetHiveConfValuesTest\n[build/Package and run all tests]   | 2023-05-28T18:05:53,815 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | Hive Session ID = d02fd602-1819-41a6-a4eb-993a9ce2c3e7\n[build/Package and run all tests]   | 2023-05-28T18:05:54,336 INFO  SessionState:1227 - Hive Session ID = d02fd602-1819-41a6-a4eb-993a9ce2c3e7\n[build/Package and run all tests]   | 2023-05-28T18:05:54,858 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:05:55,777 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:57,141 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:05:58,356 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:01,663 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:06:01,663 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:06:01,771 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 69552dba-de87-4a01-b10c-f24169e47914\n[build/Package and run all tests]   | 2023-05-28T18:06:02,149 INFO  SessionState:1227 - Hive Session ID = 69552dba-de87-4a01-b10c-f24169e47914\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:04,053 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:04,441 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:04,463 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:04,808 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:04,907 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[build/Package and run all tests]   | 2023-05-28T18:06:05,001 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:05,048 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:05,049 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:05,050 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[build/Package and run all tests]   | 2023-05-28T18:06:05,066 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:05,066 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:05,079 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[build/Package and run all tests]   | 2023-05-28T18:06:05,080 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[build/Package and run all tests]   | 2023-05-28T18:06:05,092 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:05,092 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:05,098 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:05,098 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:05,106 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:05,107 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:05,108 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[build/Package and run all tests]   | 2023-05-28T18:06:05,109 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[build/Package and run all tests]   | 2023-05-28T18:06:05,138 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:05,138 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:05,142 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:05,142 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:05,146 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_682830122' to file:/tmp/hiverunner_test7440233170732041334/warehouse1434589669556145600/source_db.db/table_a/_SCRATCH0.13136906788223335\n[build/Package and run all tests]   | 2023-05-28T18:06:05,154 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:05,154 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:05,158 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:05,158 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:05,204 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:05,250 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:05,555 WARN  org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer:531 - Cannot determine basic stats for table: source_db@table_a from metastore. Falling back.\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:05,602 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:06:05,673 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | Hello World 2023-05-28T18:06:05,709 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.SetHiveConfValuesTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:05,763 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.364 s - in com.klarna.hiverunner.examples.SetHiveConfValuesTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.examples.InsertTestDataTest\n[build/Package and run all tests]   | 2023-05-28T18:06:08,275 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | Hive Session ID = 81503046-9fef-4ef8-a0c6-898308d1267a\n[build/Package and run all tests]   | 2023-05-28T18:06:08,889 INFO  SessionState:1227 - Hive Session ID = 81503046-9fef-4ef8-a0c6-898308d1267a\n[build/Package and run all tests]   | 2023-05-28T18:06:09,443 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:06:10,543 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:12,074 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:13,265 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:16,233 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:06:16,234 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:06:16,397 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 5cca0e73-98d2-4ac5-bf79-56259a08ea80\n[build/Package and run all tests]   | 2023-05-28T18:06:16,966 INFO  SessionState:1227 - Hive Session ID = 5cca0e73-98d2-4ac5-bf79-56259a08ea80\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:19,002 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:19,401 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:19,423 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:19,772 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:19,892 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[build/Package and run all tests]   | 2023-05-28T18:06:19,969 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:20,009 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:20,010 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:20,011 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[build/Package and run all tests]   | 2023-05-28T18:06:20,024 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:20,024 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:20,035 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[build/Package and run all tests]   | 2023-05-28T18:06:20,036 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[build/Package and run all tests]   | 2023-05-28T18:06:20,047 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:20,047 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:20,053 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:20,054 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:20,062 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:20,063 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:20,063 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[build/Package and run all tests]   | 2023-05-28T18:06:20,065 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[build/Package and run all tests]   | 2023-05-28T18:06:20,106 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:20,107 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:20,109 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:20,110 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:20,113 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_1915647937' to file:/tmp/hiverunner_test2622454328424856287/warehouse8824905599031584553/source_db.db/test_table/_SCRATCH0.6239194060622453\n[build/Package and run all tests]   | 2023-05-28T18:06:20,123 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:20,123 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:20,127 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:20,127 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:20,175 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:20,225 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:20,469 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:06:20,534 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | Result from from code:\n[build/Package and run all tests]   | [Value1, 1, true]\n[build/Package and run all tests]   | [Value2, 99, false]\n[build/Package and run all tests]   | 2023-05-28T18:06:20,568 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.InsertTestDataTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:20,626 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 31a82faf-9750-485a-9e0a-0a6da13a6a5b\n[build/Package and run all tests]   | 2023-05-28T18:06:20,685 INFO  SessionState:1227 - Hive Session ID = 31a82faf-9750-485a-9e0a-0a6da13a6a5b\n[build/Package and run all tests]   | 2023-05-28T18:06:20,706 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:06:20,709 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:21,092 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:23,226 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = d746c9cb-95bf-4c60-a858-b6b7b878703f\n[build/Package and run all tests]   | 2023-05-28T18:06:23,387 INFO  SessionState:1227 - Hive Session ID = d746c9cb-95bf-4c60-a858-b6b7b878703f\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:23,469 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:23,695 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:23,841 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:23,958 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:23,971 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:23,971 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:23,974 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:23,974 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:23,991 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:23,991 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:23,995 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:23,995 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:24,000 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:24,000 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:24,017 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:24,017 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:24,020 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:24,020 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:24,023 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_171219650' to file:/tmp/hiverunner_test8124056704142529910/warehouse7086513443520696947/source_db.db/test_table/_SCRATCH0.40993793707728243\n[build/Package and run all tests]   | 2023-05-28T18:06:24,029 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:24,029 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:24,032 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:24,032 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:24,076 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:24,115 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:24,164 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | Result from TSV file:\n[build/Package and run all tests]   | [textA, 42, true]\n[build/Package and run all tests]   | [textB, 3, true]\n[build/Package and run all tests]   | [textC, 99, false]\n[build/Package and run all tests]   | 2023-05-28T18:06:24,167 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.InsertTestDataTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:24,186 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 40211ece-ee2a-493a-adba-1e566263c1a5\n[build/Package and run all tests]   | 2023-05-28T18:06:24,238 INFO  SessionState:1227 - Hive Session ID = 40211ece-ee2a-493a-adba-1e566263c1a5\n[build/Package and run all tests]   | 2023-05-28T18:06:24,258 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:06:24,260 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:24,607 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:26,528 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 127fda17-9177-49cd-8e75-43a6db71e623\n[build/Package and run all tests]   | 2023-05-28T18:06:26,612 INFO  SessionState:1227 - Hive Session ID = 127fda17-9177-49cd-8e75-43a6db71e623\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:26,682 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:26,875 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:27,009 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:27,122 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:27,134 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:27,134 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:27,137 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:27,137 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:27,165 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:27,165 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:27,168 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:27,168 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:27,172 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:27,172 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:27,189 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:27,189 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:27,192 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:27,192 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:27,195 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_316082930' to file:/tmp/hiverunner_test3831639691536571438/warehouse8079864719541104295/source_db.db/test_table/_SCRATCH0.4945012170061127\n[build/Package and run all tests]   | 2023-05-28T18:06:27,200 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:27,201 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:27,204 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:27,204 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:27,248 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:27,287 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:27,342 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | Result from from code selected columns:\n[build/Package and run all tests]   | [Value1, null, true]\n[build/Package and run all tests]   | [Value2, null, false]\n[build/Package and run all tests]   | 2023-05-28T18:06:27,345 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.InsertTestDataTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:27,363 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 7a7e40df-483d-499d-83f2-d7ff887e8543\n[build/Package and run all tests]   | 2023-05-28T18:06:27,412 INFO  SessionState:1227 - Hive Session ID = 7a7e40df-483d-499d-83f2-d7ff887e8543\n[build/Package and run all tests]   | 2023-05-28T18:06:27,431 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:06:27,434 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:27,798 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:29,706 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 8916b973-e656-4c02-9c9c-78004e4e9d4d\n[build/Package and run all tests]   | 2023-05-28T18:06:29,788 INFO  SessionState:1227 - Hive Session ID = 8916b973-e656-4c02-9c9c-78004e4e9d4d\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:29,896 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:30,168 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:30,298 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:30,414 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:30,426 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:30,427 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:30,430 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:30,430 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:30,448 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:30,449 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:30,452 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:30,452 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:30,457 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:30,457 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:30,520 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:30,521 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:30,523 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:30,524 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:30,526 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_1324571630' to file:/tmp/hiverunner_test3441906403579361297/warehouse2743534780461480677/source_db.db/test_table2/_SCRATCH0.9035794969779289/col_c=A\n[build/Package and run all tests]   | 2023-05-28T18:06:30,531 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:30,531 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:30,535 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:30,535 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:30,584 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:30,769 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:30,832 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:30,901 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:30,915 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:30,915 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:30,918 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:30,919 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:30,957 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:30,957 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:30,960 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:30,961 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:30,964 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:30,965 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:30,983 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:30,984 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:30,986 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:30,987 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:30,990 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_1648566160' to file:/tmp/hiverunner_test3441906403579361297/warehouse2743534780461480677/source_db.db/test_table2/_SCRATCH0.36269251765149746/col_c=B\n[build/Package and run all tests]   | 2023-05-28T18:06:30,996 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:30,996 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:31,000 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:31,000 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:31,051 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:31,147 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:31,255 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:06:31,279 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | Result from long method name:\n[build/Package and run all tests]   | [textA, 42, A]\n[build/Package and run all tests]   | [null, 3, A]\n[build/Package and run all tests]   | [textC, 99, B]\n[build/Package and run all tests]   | 2023-05-28T18:06:31,281 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.InsertTestDataTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:31,299 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = d77d2e96-feb4-4d1f-b44b-298fb2b7b714\n[build/Package and run all tests]   | 2023-05-28T18:06:31,359 INFO  SessionState:1227 - Hive Session ID = d77d2e96-feb4-4d1f-b44b-298fb2b7b714\n[build/Package and run all tests]   | 2023-05-28T18:06:31,379 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:06:31,382 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:31,904 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:34,220 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = aace3b08-91a4-47d1-ad11-5b29d00b6f8d\n[build/Package and run all tests]   | 2023-05-28T18:06:34,357 INFO  SessionState:1227 - Hive Session ID = aace3b08-91a4-47d1-ad11-5b29d00b6f8d\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:34,423 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:34,611 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:34,746 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:34,868 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:34,878 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:34,878 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:34,881 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:34,881 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:34,894 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:34,894 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:34,897 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:34,898 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:34,900 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:34,901 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:34,915 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:34,915 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:34,918 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:34,918 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:34,920 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_611156352' to file:/tmp/hiverunner_test1906687762090442810/warehouse1208474855875792979/source_db.db/test_table/_SCRATCH0.03939028553568391\n[build/Package and run all tests]   | 2023-05-28T18:06:34,924 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:34,925 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:34,927 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:34,927 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:34,972 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:35,009 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:35,077 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | Result from TSV file subset header:\n[build/Package and run all tests]   | [textA, null, true]\n[build/Package and run all tests]   | [textB, null, true]\n[build/Package and run all tests]   | [textC, null, false]\n[build/Package and run all tests]   | 2023-05-28T18:06:35,079 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.InsertTestDataTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:35,093 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = d92adb6c-3ebe-4434-ba35-397dc4193213\n[build/Package and run all tests]   | 2023-05-28T18:06:35,138 INFO  SessionState:1227 - Hive Session ID = d92adb6c-3ebe-4434-ba35-397dc4193213\n[build/Package and run all tests]   | 2023-05-28T18:06:35,155 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:06:35,157 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:35,547 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:37,745 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = fb8768da-2000-4903-ac14-87bab4653b10\n[build/Package and run all tests]   | 2023-05-28T18:06:37,869 INFO  SessionState:1227 - Hive Session ID = fb8768da-2000-4903-ac14-87bab4653b10\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:37,928 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:38,108 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:38,232 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:38,331 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:38,339 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:38,340 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:38,342 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:38,342 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:38,358 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:38,359 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:38,362 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:38,362 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:38,365 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:38,366 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:38,381 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:38,381 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:38,383 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:38,383 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:38,386 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_948879914' to file:/tmp/hiverunner_test3157349473414115651/warehouse2356699491088616709/source_db.db/test_table/_SCRATCH0.7943874901799699\n[build/Package and run all tests]   | 2023-05-28T18:06:38,389 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:38,390 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:38,393 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:06:38,393 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:06:38,433 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:06:38,470 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:38,509 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | Result from TSV file header:\n[build/Package and run all tests]   | [textA, 42, true]\n[build/Package and run all tests]   | [textB, 3, true]\n[build/Package and run all tests]   | [textC, 99, false]\n[build/Package and run all tests]   | 2023-05-28T18:06:38,512 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.InsertTestDataTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:38,525 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 30.814 s - in com.klarna.hiverunner.examples.InsertTestDataTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[build/Package and run all tests]   | 2023-05-28T18:06:41,016 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | Hive Session ID = 8d3b1b87-9670-41ef-95d9-ff7395121551\n[build/Package and run all tests]   | 2023-05-28T18:06:41,494 INFO  SessionState:1227 - Hive Session ID = 8d3b1b87-9670-41ef-95d9-ff7395121551\n[build/Package and run all tests]   | 2023-05-28T18:06:41,994 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:06:42,864 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:44,244 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:48,254 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:06:48,255 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:06:48,410 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = c54cbd92-0a7e-4f25-9d04-20adff27785c\n[build/Package and run all tests]   | 2023-05-28T18:06:48,950 INFO  SessionState:1227 - Hive Session ID = c54cbd92-0a7e-4f25-9d04-20adff27785c\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:51,595 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:06:51,660 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:06:51,698 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:51,752 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 9bb4c651-5894-4227-8fe1-1d0817daed50\n[build/Package and run all tests]   | 2023-05-28T18:06:51,821 INFO  SessionState:1227 - Hive Session ID = 9bb4c651-5894-4227-8fe1-1d0817daed50\n[build/Package and run all tests]   | 2023-05-28T18:06:51,843 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:06:51,847 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:52,199 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:54,690 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 4362ef48-088a-4662-82a5-a9c923976c90\n[build/Package and run all tests]   | 2023-05-28T18:06:54,857 INFO  SessionState:1227 - Hive Session ID = 4362ef48-088a-4662-82a5-a9c923976c90\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:55,539 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 3\n[build/Package and run all tests]   | 2023-05-28T18:06:55,545 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:55,563 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 035171da-a31a-4b54-9422-29188f2be518\n[build/Package and run all tests]   | 2023-05-28T18:06:55,619 INFO  SessionState:1227 - Hive Session ID = 035171da-a31a-4b54-9422-29188f2be518\n[build/Package and run all tests]   | 2023-05-28T18:06:55,647 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:06:55,649 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:56,036 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:58,441 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = db127b3e-b5aa-4654-b2b8-1ce5aa221277\n[build/Package and run all tests]   | 2023-05-28T18:06:58,591 INFO  SessionState:1227 - Hive Session ID = db127b3e-b5aa-4654-b2b8-1ce5aa221277\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:58,899 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:06:58,916 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 4423699e-1c03-4a3d-b219-31c7b28ff89c\n[build/Package and run all tests]   | 2023-05-28T18:06:58,971 INFO  SessionState:1227 - Hive Session ID = 4423699e-1c03-4a3d-b219-31c7b28ff89c\n[build/Package and run all tests]   | 2023-05-28T18:06:58,989 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:06:58,992 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:06:59,301 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:01,364 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 935816b4-4f5f-4dc5-b8c8-760e43a5596c\n[build/Package and run all tests]   | 2023-05-28T18:07:01,458 INFO  SessionState:1227 - Hive Session ID = 935816b4-4f5f-4dc5-b8c8-760e43a5596c\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:01,839 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 3\n[build/Package and run all tests]   | 2023-05-28T18:07:01,844 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:01,862 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 37ee5d46-0fb5-4f2f-8d76-fdbd6726b6b1\n[build/Package and run all tests]   | 2023-05-28T18:07:01,917 INFO  SessionState:1227 - Hive Session ID = 37ee5d46-0fb5-4f2f-8d76-fdbd6726b6b1\n[build/Package and run all tests]   | 2023-05-28T18:07:01,938 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:07:01,941 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:02,229 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:04,546 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 38c3678d-c4a5-4f18-a7b6-32242625e279\n[build/Package and run all tests]   | 2023-05-28T18:07:04,685 INFO  SessionState:1227 - Hive Session ID = 38c3678d-c4a5-4f18-a7b6-32242625e279\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:05,009 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:07:05,012 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:05,029 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 1599fd4a-5415-4f7b-b823-dccbced00631\n[build/Package and run all tests]   | 2023-05-28T18:07:05,083 INFO  SessionState:1227 - Hive Session ID = 1599fd4a-5415-4f7b-b823-dccbced00631\n[build/Package and run all tests]   | 2023-05-28T18:07:05,103 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:07:05,106 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:05,438 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:07,658 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 7dd61228-a837-4476-932b-49cf04dccd23\n[build/Package and run all tests]   | 2023-05-28T18:07:07,803 INFO  SessionState:1227 - Hive Session ID = 7dd61228-a837-4476-932b-49cf04dccd23\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:08,151 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:07:08,153 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:08,169 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = e66ba6e9-0ed0-40b8-a16f-74a8c882a92a\n[build/Package and run all tests]   | 2023-05-28T18:07:08,223 INFO  SessionState:1227 - Hive Session ID = e66ba6e9-0ed0-40b8-a16f-74a8c882a92a\n[build/Package and run all tests]   | 2023-05-28T18:07:08,244 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:07:08,246 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:08,580 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:10,537 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 5ce1a81c-f52d-4668-be1c-0688509ca563\n[build/Package and run all tests]   | 2023-05-28T18:07:10,656 INFO  SessionState:1227 - Hive Session ID = 5ce1a81c-f52d-4668-be1c-0688509ca563\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:11,091 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 3\n[build/Package and run all tests]   | 2023-05-28T18:07:11,095 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:11,108 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = d95c8d97-bee9-4dd9-891c-b4f078be5d8d\n[build/Package and run all tests]   | 2023-05-28T18:07:11,159 INFO  SessionState:1227 - Hive Session ID = d95c8d97-bee9-4dd9-891c-b4f078be5d8d\n[build/Package and run all tests]   | 2023-05-28T18:07:11,189 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:07:11,192 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:11,506 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:13,451 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 3a3fabd2-a7a9-42bc-a437-7c003ef6fa23\n[build/Package and run all tests]   | 2023-05-28T18:07:13,579 INFO  SessionState:1227 - Hive Session ID = 3a3fabd2-a7a9-42bc-a437-7c003ef6fa23\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:13,858 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:07:13,860 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:13,872 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.481 s - in com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.HiveShellHiveCliEmulationTest\n[build/Package and run all tests]   | 2023-05-28T18:07:16,432 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | Hive Session ID = 327da5cc-9929-4095-840b-40dbc09ab350\n[build/Package and run all tests]   | 2023-05-28T18:07:16,922 INFO  SessionState:1227 - Hive Session ID = 327da5cc-9929-4095-840b-40dbc09ab350\n[build/Package and run all tests]   | 2023-05-28T18:07:17,463 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:07:18,379 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:19,963 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:21,154 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:24,593 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:07:24,593 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:07:24,754 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = cc324859-b881-4b05-8af8-0d32e6e14fd9\n[build/Package and run all tests]   | 2023-05-28T18:07:25,331 INFO  SessionState:1227 - Hive Session ID = cc324859-b881-4b05-8af8-0d32e6e14fd9\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:27,412 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellHiveCliEmulationTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:27,467 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 0028bd06-e27c-4081-8489-fa0e6691a278\n[build/Package and run all tests]   | 2023-05-28T18:07:27,535 INFO  SessionState:1227 - Hive Session ID = 0028bd06-e27c-4081-8489-fa0e6691a278\n[build/Package and run all tests]   | 2023-05-28T18:07:27,564 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:07:27,568 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:27,925 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:30,474 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = be4e1dbd-23f1-4579-bb85-14b3a4d93516\n[build/Package and run all tests]   | 2023-05-28T18:07:30,645 INFO  SessionState:1227 - Hive Session ID = be4e1dbd-23f1-4579-bb85-14b3a4d93516\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:30,749 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellHiveCliEmulationTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:30,771 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 5bb9592a-408c-4160-997b-7ef681d52d19\n[build/Package and run all tests]   | 2023-05-28T18:07:30,825 INFO  SessionState:1227 - Hive Session ID = 5bb9592a-408c-4160-997b-7ef681d52d19\n[build/Package and run all tests]   | 2023-05-28T18:07:30,849 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:07:30,852 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:31,192 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:33,769 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 68c85381-f607-493e-9be0-cc4544180424\n[build/Package and run all tests]   | 2023-05-28T18:07:33,924 INFO  SessionState:1227 - Hive Session ID = 68c85381-f607-493e-9be0-cc4544180424\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:33,998 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellHiveCliEmulationTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:34,015 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 097cdb17-42bc-44a5-9bbf-8b0f63deeab5\n[build/Package and run all tests]   | 2023-05-28T18:07:34,070 INFO  SessionState:1227 - Hive Session ID = 097cdb17-42bc-44a5-9bbf-8b0f63deeab5\n[build/Package and run all tests]   | 2023-05-28T18:07:34,094 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:07:34,098 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:34,445 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:36,835 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 4d60fb64-1ad3-4c92-b875-e78c53cd35d4\n[build/Package and run all tests]   | 2023-05-28T18:07:36,982 INFO  SessionState:1227 - Hive Session ID = 4d60fb64-1ad3-4c92-b875-e78c53cd35d4\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:37,051 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellHiveCliEmulationTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:37,066 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 983018b5-ae44-4983-9f2b-15597f6ed0c6\n[build/Package and run all tests]   | 2023-05-28T18:07:37,116 INFO  SessionState:1227 - Hive Session ID = 983018b5-ae44-4983-9f2b-15597f6ed0c6\n[build/Package and run all tests]   | 2023-05-28T18:07:37,143 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:07:37,146 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:37,416 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:39,883 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 193f1662-f00d-4486-9d39-92c5fffeb5a7\n[build/Package and run all tests]   | 2023-05-28T18:07:40,025 INFO  SessionState:1227 - Hive Session ID = 193f1662-f00d-4486-9d39-92c5fffeb5a7\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:40,088 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellHiveCliEmulationTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:40,102 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 67caa6e8-9757-420c-88fb-f20f2cf444ce\n[build/Package and run all tests]   | 2023-05-28T18:07:40,155 INFO  SessionState:1227 - Hive Session ID = 67caa6e8-9757-420c-88fb-f20f2cf444ce\n[build/Package and run all tests]   | 2023-05-28T18:07:40,175 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:07:40,178 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:40,457 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:42,302 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 29ca8945-6bce-48ce-854e-4769485ee9ff\n[build/Package and run all tests]   | 2023-05-28T18:07:42,386 INFO  SessionState:1227 - Hive Session ID = 29ca8945-6bce-48ce-854e-4769485ee9ff\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:42,451 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellHiveCliEmulationTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:42,467 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 70361e71-ae0f-4aa4-8bb8-4481a5283fea\n[build/Package and run all tests]   | 2023-05-28T18:07:42,516 INFO  SessionState:1227 - Hive Session ID = 70361e71-ae0f-4aa4-8bb8-4481a5283fea\n[build/Package and run all tests]   | 2023-05-28T18:07:42,550 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:07:42,552 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:42,834 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:45,116 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 3385d7fe-e34c-4e10-bf3e-ae9c77438d58\n[build/Package and run all tests]   | 2023-05-28T18:07:45,217 INFO  SessionState:1227 - Hive Session ID = 3385d7fe-e34c-4e10-bf3e-ae9c77438d58\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | NoViableAltException(-1@[])\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1387)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:220)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:74)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:616)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:197)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:260)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:247)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:541)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:510)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:267)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.HiveServerContainer.executeStatement(HiveServerContainer.java:127)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatementsWithCommandShellEmulation(HiveShellBase.java:116)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatementWithCommandShellEmulation(HiveShellBase.java:110)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatement(HiveShellBase.java:100)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:89)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:82)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.HiveShellHiveCliEmulationTest.lambda$testQueryStripFullLineComment$0(HiveShellHiveCliEmulationTest.java:61)\n[build/Package and run all tests]   | \tat org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:55)\n[build/Package and run all tests]   | \tat org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:37)\n[build/Package and run all tests]   | \tat org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3007)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.HiveShellHiveCliEmulationTest.testQueryStripFullLineComment(HiveShellHiveCliEmulationTest.java:61)\n[build/Package and run all tests]   | \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n[build/Package and run all tests]   | \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n[build/Package and run all tests]   | \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n[build/Package and run all tests]   | \tat java.lang.reflect.Method.invoke(Method.java:498)\n[build/Package and run all tests]   | \tat org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[build/Package and run all tests]   | \tat java.util.ArrayList.forEach(ArrayList.java:1259)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[build/Package and run all tests]   | \tat java.util.ArrayList.forEach(ArrayList.java:1259)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\n[build/Package and run all tests]   | FAILED: ParseException line 1:0 cannot recognize input near '<EOF>' '<EOF>' '<EOF>'\n[build/Package and run all tests]   | 2023-05-28T18:07:45,346 ERROR org.apache.hadoop.hive.ql.Driver:1250 - FAILED: ParseException line 1:0 cannot recognize input near '<EOF>' '<EOF>' '<EOF>'\n[build/Package and run all tests]   | org.apache.hadoop.hive.ql.parse.ParseException: line 1:0 cannot recognize input near '<EOF>' '<EOF>' '<EOF>'\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:223)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:74)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:616)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)\n[build/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:197)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:260)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:247)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:541)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:510)\n[build/Package and run all tests]   | \tat org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:267)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.HiveServerContainer.executeStatement(HiveServerContainer.java:127)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatementsWithCommandShellEmulation(HiveShellBase.java:116)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatementWithCommandShellEmulation(HiveShellBase.java:110)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatement(HiveShellBase.java:100)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:89)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:82)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.HiveShellHiveCliEmulationTest.lambda$testQueryStripFullLineComment$0(HiveShellHiveCliEmulationTest.java:61)\n[build/Package and run all tests]   | \tat org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:55)\n[build/Package and run all tests]   | \tat org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:37)\n[build/Package and run all tests]   | \tat org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3007)\n[build/Package and run all tests]   | \tat com.klarna.hiverunner.HiveShellHiveCliEmulationTest.testQueryStripFullLineComment(HiveShellHiveCliEmulationTest.java:61)\n[build/Package and run all tests]   | \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n[build/Package and run all tests]   | \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n[build/Package and run all tests]   | \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n[build/Package and run all tests]   | \tat java.lang.reflect.Method.invoke(Method.java:498)\n[build/Package and run all tests]   | \tat org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)\n[build/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[build/Package and run all tests]   | \tat java.util.ArrayList.forEach(ArrayList.java:1259)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[build/Package and run all tests]   | \tat java.util.ArrayList.forEach(ArrayList.java:1259)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)\n[build/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)\n[build/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\n[build/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:07:45,357 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellHiveCliEmulationTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:45,370 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 29.589 s - in com.klarna.hiverunner.HiveShellHiveCliEmulationTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.CtasTest\n[build/Package and run all tests]   | 2023-05-28T18:07:48,084 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | Hive Session ID = f42ac3ae-28ef-40f8-9fcd-7a5f932aff47\n[build/Package and run all tests]   | 2023-05-28T18:07:48,616 INFO  SessionState:1227 - Hive Session ID = f42ac3ae-28ef-40f8-9fcd-7a5f932aff47\n[build/Package and run all tests]   | 2023-05-28T18:07:49,159 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:07:50,091 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:51,626 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:52,735 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:07:55,095 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:07:55,095 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:07:55,256 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 1bf5028c-467d-4f5f-93bf-5484dee4546c\n[build/Package and run all tests]   | 2023-05-28T18:07:55,827 INFO  SessionState:1227 - Hive Session ID = 1bf5028c-467d-4f5f-93bf-5484dee4546c\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:07:58,575 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180758_e6db1741-c85b-4e6e-afaf-bcfe4705909f\n[build/Package and run all tests]   | Total jobs = 3\n[build/Package and run all tests]   | Launching Job 1 out of 3\n[build/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[build/Package and run all tests]   | 2023-05-28T18:07:58,883 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[build/Package and run all tests]   | 2023-05-28T18:07:58,904 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[build/Package and run all tests]   | 2023-05-28T18:07:58,922 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[build/Package and run all tests]   | 2023-05-28T18:07:58,923 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[build/Package and run all tests]   | 2023-05-28T18:07:58,958 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:07:59,028 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:07:59,289 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:07:59,308 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:07:59,348 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:07:59,408 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local472584974_0001\n[build/Package and run all tests]   | 2023-05-28T18:07:59,409 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:07:59,772 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:07:59,775 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:07:59,777 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:07:59,789 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:07:59,794 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local472584974_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:07:59,839 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:07:59,850 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test164589696114940250/hadooptmp3024910483773309090/foo/data.csv:0+11InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:07:59,933 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[build/Package and run all tests]   | 2023-05-28T18:07:59,934 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[build/Package and run all tests]   | 2023-05-28T18:07:59,934 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[build/Package and run all tests]   | 2023-05-28T18:07:59,934 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[build/Package and run all tests]   | 2023-05-28T18:07:59,979 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[build/Package and run all tests]   | 2023-05-28T18:07:59,981 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[build/Package and run all tests]   | 2023-05-28T18:08:00,048 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:08:00,062 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local472584974_0001_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:08:00,064 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test164589696114940250/hadooptmp3024910483773309090/foo/data.csv:0+11\n[build/Package and run all tests]   | 2023-05-28T18:08:00,064 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local472584974_0001_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:08:00,068 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local472584974_0001_m_000000_0: Counters: 24\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=40624263\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=41747127\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=3\n[build/Package and run all tests]   | \t\tMap output records=0\n[build/Package and run all tests]   | \t\tInput split bytes=239\n[build/Package and run all tests]   | \t\tSpilled Records=0\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=33\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2000683008\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_1_default.foo_prim=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=3\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:08:00,068 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local472584974_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:08:00,071 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:08:00,793 Stage-1 map = 100%,  reduce = 0%\n[build/Package and run all tests]   | Ended Job = job_local472584974_0001\n[build/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[build/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[build/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test164589696114940250/warehouse343117059798358480/.hive-staging_hive_2023-05-28_18-07-58_057_550944659365560958-1/-ext-10001\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test164589696114940250/warehouse343117059798358480/foo_prim\n[build/Package and run all tests]   | 2023-05-28T18:08:00,827 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:00,916 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:00,983 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:08:00,991 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:08:01,016 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.CtasTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:01,070 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 7a741940-0887-4d18-913b-47fadabf22b6\n[build/Package and run all tests]   | 2023-05-28T18:08:01,131 INFO  SessionState:1227 - Hive Session ID = 7a741940-0887-4d18-913b-47fadabf22b6\n[build/Package and run all tests]   | 2023-05-28T18:08:01,152 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:08:01,156 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:08:01,514 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:08:03,700 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 8d96185f-d384-45fb-a52a-7949268d0e5f\n[build/Package and run all tests]   | 2023-05-28T18:08:03,806 INFO  SessionState:1227 - Hive Session ID = 8d96185f-d384-45fb-a52a-7949268d0e5f\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:04,164 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180804_97c9f322-ab11-4d8f-95f2-64fa5885b9d0\n[build/Package and run all tests]   | Total jobs = 3\n[build/Package and run all tests]   | Launching Job 1 out of 3\n[build/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[build/Package and run all tests]   | 2023-05-28T18:08:04,188 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:08:04,195 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:08:04,203 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:08:04,352 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:08:04,354 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:08:04,377 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:08:04,411 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local339277539_0002\n[build/Package and run all tests]   | 2023-05-28T18:08:04,412 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:08:04,621 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | 2023-05-28T18:08:04,622 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:08:04,622 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:08:04,625 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:08:04,626 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local339277539_0002_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:08:04,630 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:08:04,632 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test1830433306940377504/hadooptmp2559797464005173949/foo/data.csv:0+11InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:08:04,640 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[build/Package and run all tests]   | 2023-05-28T18:08:04,663 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:08:04,665 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local339277539_0002_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:08:04,667 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test1830433306940377504/hadooptmp2559797464005173949/foo/data.csv:0+11\n[build/Package and run all tests]   | 2023-05-28T18:08:04,667 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local339277539_0002_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:08:04,667 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local339277539_0002_m_000000_0: Counters: 24\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=81248555\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=83494321\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=3\n[build/Package and run all tests]   | \t\tMap output records=0\n[build/Package and run all tests]   | \t\tInput split bytes=240\n[build/Package and run all tests]   | \t\tSpilled Records=0\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2086666240\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_1_default.foo_prim=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=3\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:08:04,668 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local339277539_0002_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:08:04,668 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:08:05,629 Stage-1 map = 100%,  reduce = 0%\n[build/Package and run all tests]   | Ended Job = job_local339277539_0002\n[build/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[build/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[build/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test1830433306940377504/warehouse845906462792138/.hive-staging_hive_2023-05-28_18-08-04_016_5022672030538604728-1/-ext-10001\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test1830433306940377504/warehouse845906462792138/foo_prim\n[build/Package and run all tests]   | 2023-05-28T18:08:05,640 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:05,690 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:05,726 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:08:05,729 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.CtasTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:05,749 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = cab5085f-84a5-4a22-a734-96a2dadeb7c1\n[build/Package and run all tests]   | 2023-05-28T18:08:05,800 INFO  SessionState:1227 - Hive Session ID = cab5085f-84a5-4a22-a734-96a2dadeb7c1\n[build/Package and run all tests]   | 2023-05-28T18:08:05,819 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:08:05,822 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:08:06,231 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:08:08,568 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 892899e5-1a2e-4d31-bb4d-7f23bd9c9a79\n[build/Package and run all tests]   | 2023-05-28T18:08:08,681 INFO  SessionState:1227 - Hive Session ID = 892899e5-1a2e-4d31-bb4d-7f23bd9c9a79\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:09,029 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180808_0b26d360-077c-4fea-bbb5-f2238cad453e\n[build/Package and run all tests]   | Total jobs = 3\n[build/Package and run all tests]   | Launching Job 1 out of 3\n[build/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[build/Package and run all tests]   | 2023-05-28T18:08:09,054 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:08:09,061 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:08:09,069 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:08:09,217 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:08:09,219 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:08:09,247 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:08:09,276 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local1881077233_0003\n[build/Package and run all tests]   | 2023-05-28T18:08:09,277 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:08:09,490 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | 2023-05-28T18:08:09,490 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:08:09,491 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:08:09,495 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:08:09,495 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local1881077233_0003_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:08:09,499 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:08:09,501 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test4215716259968544712/hadooptmp6931637335595229610/foo/data.csv:0+11InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:08:09,507 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[build/Package and run all tests]   | 2023-05-28T18:08:09,525 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:08:09,527 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1881077233_0003_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:08:09,529 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test4215716259968544712/hadooptmp6931637335595229610/foo/data.csv:0+11\n[build/Package and run all tests]   | 2023-05-28T18:08:09,529 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1881077233_0003_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:08:09,530 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1881077233_0003_m_000000_0: Counters: 24\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=121872848\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=125245466\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=3\n[build/Package and run all tests]   | \t\tMap output records=0\n[build/Package and run all tests]   | \t\tInput split bytes=240\n[build/Package and run all tests]   | \t\tSpilled Records=0\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2087190528\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_1_default.foo_prim=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=3\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:08:09,530 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local1881077233_0003_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:08:09,530 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:08:10,495 Stage-1 map = 100%,  reduce = 0%\n[build/Package and run all tests]   | Ended Job = job_local1881077233_0003\n[build/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[build/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[build/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test4215716259968544712/warehouse4239821535090480999/.hive-staging_hive_2023-05-28_18-08-08_882_5679350402880969355-1/-ext-10001\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test4215716259968544712/warehouse4239821535090480999/foo_prim\n[build/Package and run all tests]   | 2023-05-28T18:08:10,504 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:10,552 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:08:10,727 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180810_7537a38c-272e-4e6f-95c0-a43c17fe3e0d\n[build/Package and run all tests]   | Total jobs = 1\n[build/Package and run all tests]   | Launching Job 1 out of 1\n[build/Package and run all tests]   | Number of reduce tasks determined at compile time: 1\n[build/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[build/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[build/Package and run all tests]   | In order to limit the maximum number of reducers:\n[build/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[build/Package and run all tests]   | In order to set a constant number of reducers:\n[build/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[build/Package and run all tests]   | 2023-05-28T18:08:10,766 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:08:10,772 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:08:10,781 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:08:10,922 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:08:10,923 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:08:10,946 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:08:10,977 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local1199428534_0004\n[build/Package and run all tests]   | 2023-05-28T18:08:10,977 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:08:11,111 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | 2023-05-28T18:08:11,112 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:08:11,112 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:08:11,115 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:08:11,115 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local1199428534_0004_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:08:11,116 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:08:11,117 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test4215716259968544712/warehouse4239821535090480999/foo_prim/000000_0:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:08:11,133 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[build/Package and run all tests]   | 2023-05-28T18:08:11,152 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[build/Package and run all tests]   | 2023-05-28T18:08:11,152 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[build/Package and run all tests]   | 2023-05-28T18:08:11,152 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[build/Package and run all tests]   | 2023-05-28T18:08:11,152 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:08:11,152 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[build/Package and run all tests]   | 2023-05-28T18:08:11,156 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[build/Package and run all tests]   | 2023-05-28T18:08:11,183 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:08:11,183 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[build/Package and run all tests]   | 2023-05-28T18:08:11,183 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[build/Package and run all tests]   | 2023-05-28T18:08:11,184 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 11; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:08:11,184 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n[build/Package and run all tests]   | 2023-05-28T18:08:11,229 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[build/Package and run all tests]   | 2023-05-28T18:08:11,236 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1199428534_0004_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:08:11,238 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test4215716259968544712/warehouse4239821535090480999/foo_prim/000000_0:0+12\n[build/Package and run all tests]   | 2023-05-28T18:08:11,238 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1199428534_0004_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:08:11,239 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1199428534_0004_m_000000_0: Counters: 25\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=162497134\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=167002354\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=3\n[build/Package and run all tests]   | \t\tMap output records=1\n[build/Package and run all tests]   | \t\tMap output bytes=11\n[build/Package and run all tests]   | \t\tMap output materialized bytes=19\n[build/Package and run all tests]   | \t\tInput split bytes=245\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tSpilled Records=1\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2087190528\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_8=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_9=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_7=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=3\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | 2023-05-28T18:08:11,239 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local1199428534_0004_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:08:11,239 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28T18:08:11,244 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[build/Package and run all tests]   | 2023-05-28T18:08:11,244 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local1199428534_0004_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:08:11,253 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:08:11,257 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4bce66db\n[build/Package and run all tests]   | 2023-05-28T18:08:11,259 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:08:11,278 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1461033344, maxSingleShuffleLimit=365258336, mergeThreshold=964282048, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[build/Package and run all tests]   | 2023-05-28T18:08:11,283 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local1199428534_0004_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[build/Package and run all tests]   | 2023-05-28T18:08:11,317 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#1 about to shuffle output of map attempt_local1199428534_0004_m_000000_0 decomp: 15 len: 19 to MEMORY\n[build/Package and run all tests]   | 2023-05-28T18:08:11,320 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 15 bytes from map-output for attempt_local1199428534_0004_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:08:11,323 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 15, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->15\n[build/Package and run all tests]   | 2023-05-28T18:08:11,325 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[build/Package and run all tests]   | 2023-05-28T18:08:11,326 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:08:11,326 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[build/Package and run all tests]   | 2023-05-28T18:08:11,366 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:08:11,366 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 8 bytes\n[build/Package and run all tests]   | 2023-05-28T18:08:11,370 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 15 bytes to disk to satisfy reduce memory limit\n[build/Package and run all tests]   | 2023-05-28T18:08:11,371 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 19 bytes from disk\n[build/Package and run all tests]   | 2023-05-28T18:08:11,372 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[build/Package and run all tests]   | 2023-05-28T18:08:11,372 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:08:11,373 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 8 bytes\n[build/Package and run all tests]   | 2023-05-28T18:08:11,373 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:08:11,376 INFO  ExecReducer:99 - conf classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter8714579476214194586.jar]\n[build/Package and run all tests]   | 2023-05-28T18:08:11,377 INFO  ExecReducer:101 - thread classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter8714579476214194586.jar]\n[build/Package and run all tests]   | 2023-05-28T18:08:11,386 INFO  ExecReducer:147 - \n[build/Package and run all tests]   | <GBY>Id =4\n[build/Package and run all tests]   |   <Children>\n[build/Package and run all tests]   |     <FS>Id =6\n[build/Package and run all tests]   |       <Children>\n[build/Package and run all tests]   |       <\\Children>\n[build/Package and run all tests]   |       <Parent>Id = 4 null<\\Parent>\n[build/Package and run all tests]   |     <\\FS>\n[build/Package and run all tests]   |   <\\Children>\n[build/Package and run all tests]   |   <Parent><\\Parent>\n[build/Package and run all tests]   | <\\GBY>\n[build/Package and run all tests]   | 2023-05-28T18:08:11,423 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1199428534_0004_r_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:08:11,424 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[build/Package and run all tests]   | 2023-05-28T18:08:11,424 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1199428534_0004_r_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:08:11,425 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1199428534_0004_r_000000_0: Counters: 29\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=162497204\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=167002486\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tCombine output records=0\n[build/Package and run all tests]   | \t\tReduce input groups=1\n[build/Package and run all tests]   | \t\tReduce shuffle bytes=19\n[build/Package and run all tests]   | \t\tReduce input records=1\n[build/Package and run all tests]   | \t\tReduce output records=0\n[build/Package and run all tests]   | \t\tSpilled Records=1\n[build/Package and run all tests]   | \t\tShuffled Maps =1\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=1\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2087190528\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_0=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_4=1\n[build/Package and run all tests]   | \tShuffle Errors\n[build/Package and run all tests]   | \t\tBAD_ID=0\n[build/Package and run all tests]   | \t\tCONNECTION=0\n[build/Package and run all tests]   | \t\tIO_ERROR=0\n[build/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[build/Package and run all tests]   | \t\tWRONG_MAP=0\n[build/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:08:11,425 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local1199428534_0004_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:08:11,425 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:08:12,119 Stage-1 map = 100%,  reduce = 100%\n[build/Package and run all tests]   | Ended Job = job_local1199428534_0004\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:12,132 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:08:12,144 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.CtasTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:12,162 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 24.794 s - in com.klarna.hiverunner.CtasTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.HiveCliSourceTest\n[build/Package and run all tests]   | 2023-05-28T18:08:14,163 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | 2023-05-28T18:08:14,184 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.HiveCliSourceTest in /\n[build/Package and run all tests]   | Hive Session ID = f11de187-10ff-4b17-8d5e-c776fa3aa1d4\n[build/Package and run all tests]   | 2023-05-28T18:08:15,207 INFO  SessionState:1227 - Hive Session ID = f11de187-10ff-4b17-8d5e-c776fa3aa1d4\n[build/Package and run all tests]   | 2023-05-28T18:08:15,687 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:08:16,547 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:08:18,009 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:08:19,188 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:08:22,316 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:08:22,316 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:08:22,472 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = efcd24e5-0806-40f6-b79f-978440b5b57a\n[build/Package and run all tests]   | 2023-05-28T18:08:23,023 INFO  SessionState:1227 - Hive Session ID = efcd24e5-0806-40f6-b79f-978440b5b57a\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:25,047 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.test_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:25,401 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:08:25,421 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:08:25,772 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:08:25,884 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[build/Package and run all tests]   | 2023-05-28T18:08:25,972 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:08:26,013 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:08:26,014 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:08:26,014 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[build/Package and run all tests]   | 2023-05-28T18:08:26,028 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:08:26,028 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:08:26,041 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[build/Package and run all tests]   | 2023-05-28T18:08:26,042 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[build/Package and run all tests]   | 2023-05-28T18:08:26,053 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:08:26,054 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:08:26,060 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:08:26,060 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:08:26,068 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:08:26,069 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:08:26,069 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[build/Package and run all tests]   | 2023-05-28T18:08:26,071 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[build/Package and run all tests]   | 2023-05-28T18:08:26,099 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:08:26,099 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:08:26,102 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:08:26,102 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:08:26,106 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_749940757' to file:/tmp/hiverunner_tests8093386044134805908/warehouse2253554353429900180/test_db.db/src/_SCRATCH0.38678643936722745\n[build/Package and run all tests]   | 2023-05-28T18:08:26,114 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:08:26,114 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:08:26,117 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:08:26,118 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:08:26,171 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:08:26,220 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:26,442 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.db_b, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:26,569 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.db_c, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:26,639 WARN  org.apache.hadoop.hive.ql.parse.RowResolver:132 - Duplicate column info for a.c0 was overwritten in RowResolver map: _col0: string by _col0: string\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:26,771 WARN  org.apache.hadoop.hive.ql.parse.RowResolver:132 - Duplicate column info for a.c0 was overwritten in RowResolver map: _col0: string by _col0: string\n[build/Package and run all tests]   | 2023-05-28T18:08:27,145 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180826_a5d6b8fb-1372-48e7-87a8-b3e8db1b1fdd\n[build/Package and run all tests]   | Total jobs = 1\n[build/Package and run all tests]   | Launching Job 1 out of 1\n[build/Package and run all tests]   | Number of reduce tasks not specified. Estimated from input data size: 1\n[build/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[build/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[build/Package and run all tests]   | In order to limit the maximum number of reducers:\n[build/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[build/Package and run all tests]   | In order to set a constant number of reducers:\n[build/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[build/Package and run all tests]   | 2023-05-28T18:08:27,492 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[build/Package and run all tests]   | 2023-05-28T18:08:27,512 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[build/Package and run all tests]   | 2023-05-28T18:08:27,530 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[build/Package and run all tests]   | 2023-05-28T18:08:27,530 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[build/Package and run all tests]   | 2023-05-28T18:08:27,562 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:08:27,638 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:08:28,013 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:08:28,034 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:08:28,072 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:08:28,150 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local538797230_0001\n[build/Package and run all tests]   | 2023-05-28T18:08:28,151 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:08:28,551 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | 2023-05-28T18:08:28,552 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:08:28,553 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:08:28,569 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:08:28,570 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local538797230_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:08:28,621 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:08:28,630 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_tests8093386044134805908/warehouse2253554353429900180/test_db.db/src/part-m-749940757:0+20InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:08:28,731 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[build/Package and run all tests]   | 2023-05-28T18:08:28,732 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[build/Package and run all tests]   | 2023-05-28T18:08:28,732 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[build/Package and run all tests]   | 2023-05-28T18:08:28,732 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[build/Package and run all tests]   | 2023-05-28T18:08:28,756 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[build/Package and run all tests]   | 2023-05-28T18:08:28,756 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[build/Package and run all tests]   | 2023-05-28T18:08:28,756 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[build/Package and run all tests]   | 2023-05-28T18:08:28,756 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:08:28,756 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[build/Package and run all tests]   | 2023-05-28T18:08:28,761 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[build/Package and run all tests]   | 2023-05-28T18:08:28,842 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:08:28,842 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[build/Package and run all tests]   | 2023-05-28T18:08:28,843 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[build/Package and run all tests]   | 2023-05-28T18:08:28,843 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 28; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:08:28,843 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n[build/Package and run all tests]   | 2023-05-28T18:08:28,913 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[build/Package and run all tests]   | 2023-05-28T18:08:28,930 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local538797230_0001_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:08:28,932 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_tests8093386044134805908/warehouse2253554353429900180/test_db.db/src/part-m-749940757:0+20\n[build/Package and run all tests]   | 2023-05-28T18:08:28,932 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local538797230_0001_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:08:28,936 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local538797230_0001_m_000000_0: Counters: 26\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=40624309\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=41755136\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=5\n[build/Package and run all tests]   | \t\tMap output records=2\n[build/Package and run all tests]   | \t\tMap output bytes=28\n[build/Package and run all tests]   | \t\tMap output materialized bytes=38\n[build/Package and run all tests]   | \t\tInput split bytes=260\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tSpilled Records=2\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=1997537280\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=5\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FIL_14=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_16=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_17=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_15=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=5\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | 2023-05-28T18:08:28,936 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local538797230_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:08:28,938 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28T18:08:28,942 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[build/Package and run all tests]   | 2023-05-28T18:08:28,942 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local538797230_0001_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:08:28,953 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:08:28,957 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@78772b41\n[build/Package and run all tests]   | 2023-05-28T18:08:28,959 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:08:28,979 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1398276096, maxSingleShuffleLimit=349569024, mergeThreshold=922862272, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[build/Package and run all tests]   | 2023-05-28T18:08:28,984 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local538797230_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[build/Package and run all tests]   | 2023-05-28T18:08:29,019 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#1 about to shuffle output of map attempt_local538797230_0001_m_000000_0 decomp: 34 len: 38 to MEMORY\n[build/Package and run all tests]   | 2023-05-28T18:08:29,023 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 34 bytes from map-output for attempt_local538797230_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:08:29,025 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 34, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->34\n[build/Package and run all tests]   | 2023-05-28T18:08:29,027 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[build/Package and run all tests]   | 2023-05-28T18:08:29,028 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:08:29,029 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[build/Package and run all tests]   | 2023-05-28T18:08:29,078 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:08:29,078 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 24 bytes\n[build/Package and run all tests]   | 2023-05-28T18:08:29,083 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 34 bytes to disk to satisfy reduce memory limit\n[build/Package and run all tests]   | 2023-05-28T18:08:29,084 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 38 bytes from disk\n[build/Package and run all tests]   | 2023-05-28T18:08:29,085 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[build/Package and run all tests]   | 2023-05-28T18:08:29,085 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:08:29,086 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 24 bytes\n[build/Package and run all tests]   | 2023-05-28T18:08:29,087 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:08:29,090 INFO  ExecReducer:99 - conf classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter2428797752214176194.jar]\n[build/Package and run all tests]   | 2023-05-28T18:08:29,090 INFO  ExecReducer:101 - thread classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter2428797752214176194.jar]\n[build/Package and run all tests]   | 2023-05-28T18:08:29,104 INFO  ExecReducer:147 - \n[build/Package and run all tests]   | <GBY>Id =6\n[build/Package and run all tests]   |   <Children>\n[build/Package and run all tests]   |     <FIL>Id =12\n[build/Package and run all tests]   |       <Children>\n[build/Package and run all tests]   |         <FS>Id =11\n[build/Package and run all tests]   |           <Children>\n[build/Package and run all tests]   |           <\\Children>\n[build/Package and run all tests]   |           <Parent>Id = 12 null<\\Parent>\n[build/Package and run all tests]   |         <\\FS>\n[build/Package and run all tests]   |       <\\Children>\n[build/Package and run all tests]   |       <Parent>Id = 6 null<\\Parent>\n[build/Package and run all tests]   |     <\\FIL>\n[build/Package and run all tests]   |   <\\Children>\n[build/Package and run all tests]   |   <Parent><\\Parent>\n[build/Package and run all tests]   | <\\GBY>\n[build/Package and run all tests]   | 2023-05-28T18:08:29,111 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[build/Package and run all tests]   | 2023-05-28T18:08:29,153 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local538797230_0001_r_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:08:29,154 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[build/Package and run all tests]   | 2023-05-28T18:08:29,155 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local538797230_0001_r_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:08:29,155 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local538797230_0001_r_000000_0: Counters: 30\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=40624417\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=41755289\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tCombine output records=0\n[build/Package and run all tests]   | \t\tReduce input groups=2\n[build/Package and run all tests]   | \t\tReduce shuffle bytes=38\n[build/Package and run all tests]   | \t\tReduce input records=2\n[build/Package and run all tests]   | \t\tReduce output records=0\n[build/Package and run all tests]   | \t\tSpilled Records=2\n[build/Package and run all tests]   | \t\tShuffled Maps =1\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=1\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=1997537280\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_0=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FIL_12=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_11=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_6=2\n[build/Package and run all tests]   | \tShuffle Errors\n[build/Package and run all tests]   | \t\tBAD_ID=0\n[build/Package and run all tests]   | \t\tCONNECTION=0\n[build/Package and run all tests]   | \t\tIO_ERROR=0\n[build/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[build/Package and run all tests]   | \t\tWRONG_MAP=0\n[build/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:08:29,156 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local538797230_0001_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:08:29,156 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:08:29,578 Stage-1 map = 100%,  reduce = 100%\n[build/Package and run all tests]   | Ended Job = job_local538797230_0001\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:29,604 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:08:29,614 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:08:29,639 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.HiveCliSourceTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:29,697 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.624 s - in com.klarna.hiverunner.HiveCliSourceTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.NoTimeoutTest\n[build/Package and run all tests]   | 2023-05-28T18:08:32,124 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | Hive Session ID = c42234b3-059c-4559-adde-902f408b45b9\n[build/Package and run all tests]   | 2023-05-28T18:08:32,602 INFO  SessionState:1227 - Hive Session ID = c42234b3-059c-4559-adde-902f408b45b9\n[build/Package and run all tests]   | 2023-05-28T18:08:33,140 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:08:34,055 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:08:35,608 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:08:36,663 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:08:40,011 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:08:40,011 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:08:40,175 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 2f735dad-7cbb-4607-9d94-c04c176ed4ca\n[build/Package and run all tests]   | 2023-05-28T18:08:40,610 INFO  SessionState:1227 - Hive Session ID = 2f735dad-7cbb-4607-9d94-c04c176ed4ca\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:42,697 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.baz, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:43,742 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180843_f4ae3429-9942-4002-9e21-5806fa92bcfa\n[build/Package and run all tests]   | Total jobs = 3\n[build/Package and run all tests]   | Launching Job 1 out of 3\n[build/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[build/Package and run all tests]   | 2023-05-28T18:08:44,071 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[build/Package and run all tests]   | 2023-05-28T18:08:44,091 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[build/Package and run all tests]   | 2023-05-28T18:08:44,109 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[build/Package and run all tests]   | 2023-05-28T18:08:44,109 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[build/Package and run all tests]   | 2023-05-28T18:08:44,144 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:08:44,223 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:08:44,478 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:08:44,508 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:08:44,546 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:08:44,636 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local18730491_0001\n[build/Package and run all tests]   | 2023-05-28T18:08:44,637 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:08:44,885 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:08:44,887 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:08:44,889 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:08:44,902 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:08:44,907 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local18730491_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:08:44,956 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:08:45,000 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test3419950736743630930/localscratchdir8806521149629376425/4469a818-5ccb-4153-9775-c7876214273e/hive_2023-05-28_18-08-43_061_235477167141929927-1/dummy_path/dummy_file:0+1InputFormatClass: org.apache.hadoop.hive.ql.io.NullRowsInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:08:45,070 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[build/Package and run all tests]   | 2023-05-28T18:08:45,070 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[build/Package and run all tests]   | 2023-05-28T18:08:45,070 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[build/Package and run all tests]   | 2023-05-28T18:08:45,071 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[build/Package and run all tests]   | 2023-05-28T18:08:45,092 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[build/Package and run all tests]   | 2023-05-28T18:08:45,094 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[build/Package and run all tests]   | 2023-05-28T18:08:45,123 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:08:45,166 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local18730491_0001_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:08:45,170 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - map\n[build/Package and run all tests]   | 2023-05-28T18:08:45,171 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local18730491_0001_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:08:45,175 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local18730491_0001_m_000000_0: Counters: 25\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=40624364\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=41746452\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=4\n[build/Package and run all tests]   | \t\tMap output records=0\n[build/Package and run all tests]   | \t\tInput split bytes=351\n[build/Package and run all tests]   | \t\tSpilled Records=0\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=31\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2000683008\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_1_baz.foo=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_3=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_1=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_UDTF_2=1\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:08:45,175 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local18730491_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:08:45,177 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:08:45,908 Stage-1 map = 100%,  reduce = 0%\n[build/Package and run all tests]   | Ended Job = job_local18730491_0001\n[build/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[build/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[build/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[build/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test3419950736743630930/warehouse6614269557269153644/baz.db/foo/.hive-staging_hive_2023-05-28_18-08-43_061_235477167141929927-1/-ext-10000\n[build/Package and run all tests]   | Loading data to table baz.foo\n[build/Package and run all tests]   | 2023-05-28T18:08:45,938 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:46,159 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:46,242 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:08:46,255 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:08:47,307 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.NoTimeoutTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:47,364 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.812 s - in com.klarna.hiverunner.NoTimeoutTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.BeelineRunTest\n[build/Package and run all tests]   | 2023-05-28T18:08:49,410 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | 2023-05-28T18:08:49,430 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.BeelineRunTest in /\n[build/Package and run all tests]   | Hive Session ID = 702cbb9f-2f07-4eea-9717-3cfa56fa1913\n[build/Package and run all tests]   | 2023-05-28T18:08:50,422 INFO  SessionState:1227 - Hive Session ID = 702cbb9f-2f07-4eea-9717-3cfa56fa1913\n[build/Package and run all tests]   | 2023-05-28T18:08:50,923 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:08:51,878 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:08:53,436 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:08:56,715 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:08:56,716 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:08:56,873 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = ca9759af-a1df-4485-bc63-703515d45542\n[build/Package and run all tests]   | 2023-05-28T18:08:57,441 INFO  SessionState:1227 - Hive Session ID = ca9759af-a1df-4485-bc63-703515d45542\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:59,394 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.test_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:08:59,763 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:08:59,784 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:09:00,101 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:09:00,213 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[build/Package and run all tests]   | 2023-05-28T18:09:00,292 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:09:00,335 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:09:00,335 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:09:00,336 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[build/Package and run all tests]   | 2023-05-28T18:09:00,350 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:09:00,350 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:09:00,363 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[build/Package and run all tests]   | 2023-05-28T18:09:00,363 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[build/Package and run all tests]   | 2023-05-28T18:09:00,376 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:09:00,376 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:09:00,382 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:09:00,382 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:09:00,390 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:09:00,391 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:09:00,392 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[build/Package and run all tests]   | 2023-05-28T18:09:00,393 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[build/Package and run all tests]   | 2023-05-28T18:09:00,419 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:09:00,419 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:09:00,422 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:09:00,422 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:09:00,425 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_1325465465' to file:/tmp/hiverunner_tests6366052435291492468/warehouse3775860474064483293/test_db.db/src/_SCRATCH0.19112100892488748\n[build/Package and run all tests]   | 2023-05-28T18:09:00,433 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:09:00,433 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:09:00,437 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:09:00,437 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:09:00,482 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:09:00,529 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:00,718 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.db_b, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:00,839 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.db_c, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:00,911 WARN  org.apache.hadoop.hive.ql.parse.RowResolver:132 - Duplicate column info for a.c0 was overwritten in RowResolver map: _col0: string by _col0: string\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:01,041 WARN  org.apache.hadoop.hive.ql.parse.RowResolver:132 - Duplicate column info for a.c0 was overwritten in RowResolver map: _col0: string by _col0: string\n[build/Package and run all tests]   | 2023-05-28T18:09:01,372 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528180900_df490108-2fd5-4dbd-9e68-8b007fb71225\n[build/Package and run all tests]   | Total jobs = 1\n[build/Package and run all tests]   | Launching Job 1 out of 1\n[build/Package and run all tests]   | Number of reduce tasks not specified. Estimated from input data size: 1\n[build/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[build/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[build/Package and run all tests]   | In order to limit the maximum number of reducers:\n[build/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[build/Package and run all tests]   | In order to set a constant number of reducers:\n[build/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[build/Package and run all tests]   | 2023-05-28T18:09:01,692 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[build/Package and run all tests]   | 2023-05-28T18:09:01,712 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[build/Package and run all tests]   | 2023-05-28T18:09:01,730 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[build/Package and run all tests]   | 2023-05-28T18:09:01,731 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[build/Package and run all tests]   | 2023-05-28T18:09:01,749 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:09:01,811 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:09:02,057 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:09:02,075 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:09:02,168 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:09:02,233 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local310213606_0001\n[build/Package and run all tests]   | 2023-05-28T18:09:02,234 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:09:02,480 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | 2023-05-28T18:09:02,481 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:09:02,482 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:09:02,495 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:09:02,496 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local310213606_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:09:02,540 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:09:02,549 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_tests6366052435291492468/warehouse3775860474064483293/test_db.db/src/part-m-1325465465:0+20InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:09:02,632 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[build/Package and run all tests]   | 2023-05-28T18:09:02,632 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[build/Package and run all tests]   | 2023-05-28T18:09:02,632 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[build/Package and run all tests]   | 2023-05-28T18:09:02,632 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[build/Package and run all tests]   | 2023-05-28T18:09:02,651 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[build/Package and run all tests]   | 2023-05-28T18:09:02,651 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[build/Package and run all tests]   | 2023-05-28T18:09:02,651 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[build/Package and run all tests]   | 2023-05-28T18:09:02,652 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:09:02,652 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[build/Package and run all tests]   | 2023-05-28T18:09:02,656 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[build/Package and run all tests]   | 2023-05-28T18:09:02,735 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:09:02,736 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[build/Package and run all tests]   | 2023-05-28T18:09:02,736 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[build/Package and run all tests]   | 2023-05-28T18:09:02,736 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 28; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:09:02,736 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n[build/Package and run all tests]   | 2023-05-28T18:09:02,779 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[build/Package and run all tests]   | 2023-05-28T18:09:02,797 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local310213606_0001_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:09:02,799 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_tests6366052435291492468/warehouse3775860474064483293/test_db.db/src/part-m-1325465465:0+20\n[build/Package and run all tests]   | 2023-05-28T18:09:02,799 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local310213606_0001_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:09:02,802 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local310213606_0001_m_000000_0: Counters: 26\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=40624310\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=41755123\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=5\n[build/Package and run all tests]   | \t\tMap output records=2\n[build/Package and run all tests]   | \t\tMap output bytes=28\n[build/Package and run all tests]   | \t\tMap output materialized bytes=38\n[build/Package and run all tests]   | \t\tInput split bytes=261\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tSpilled Records=2\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=1996488704\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=5\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FIL_14=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_16=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_17=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_15=3\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=5\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | 2023-05-28T18:09:02,803 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local310213606_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:09:02,804 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28T18:09:02,810 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[build/Package and run all tests]   | 2023-05-28T18:09:02,810 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local310213606_0001_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:09:02,822 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:09:02,825 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@59560d17\n[build/Package and run all tests]   | 2023-05-28T18:09:02,827 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:09:02,846 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1397542016, maxSingleShuffleLimit=349385504, mergeThreshold=922377792, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[build/Package and run all tests]   | 2023-05-28T18:09:02,850 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local310213606_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[build/Package and run all tests]   | 2023-05-28T18:09:02,884 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#1 about to shuffle output of map attempt_local310213606_0001_m_000000_0 decomp: 34 len: 38 to MEMORY\n[build/Package and run all tests]   | 2023-05-28T18:09:02,887 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 34 bytes from map-output for attempt_local310213606_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:09:02,890 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 34, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->34\n[build/Package and run all tests]   | 2023-05-28T18:09:02,892 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[build/Package and run all tests]   | 2023-05-28T18:09:02,893 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:09:02,894 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[build/Package and run all tests]   | 2023-05-28T18:09:02,915 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:09:02,916 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 24 bytes\n[build/Package and run all tests]   | 2023-05-28T18:09:02,920 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 34 bytes to disk to satisfy reduce memory limit\n[build/Package and run all tests]   | 2023-05-28T18:09:02,921 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 38 bytes from disk\n[build/Package and run all tests]   | 2023-05-28T18:09:02,922 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[build/Package and run all tests]   | 2023-05-28T18:09:02,922 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:09:02,923 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 24 bytes\n[build/Package and run all tests]   | 2023-05-28T18:09:02,924 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[build/Package and run all tests]   | 2023-05-28T18:09:02,927 INFO  ExecReducer:99 - conf classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter4995508428773969835.jar]\n[build/Package and run all tests]   | 2023-05-28T18:09:02,927 INFO  ExecReducer:101 - thread classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter4995508428773969835.jar]\n[build/Package and run all tests]   | 2023-05-28T18:09:02,940 INFO  ExecReducer:147 - \n[build/Package and run all tests]   | <GBY>Id =6\n[build/Package and run all tests]   |   <Children>\n[build/Package and run all tests]   |     <FIL>Id =12\n[build/Package and run all tests]   |       <Children>\n[build/Package and run all tests]   |         <FS>Id =11\n[build/Package and run all tests]   |           <Children>\n[build/Package and run all tests]   |           <\\Children>\n[build/Package and run all tests]   |           <Parent>Id = 12 null<\\Parent>\n[build/Package and run all tests]   |         <\\FS>\n[build/Package and run all tests]   |       <\\Children>\n[build/Package and run all tests]   |       <Parent>Id = 6 null<\\Parent>\n[build/Package and run all tests]   |     <\\FIL>\n[build/Package and run all tests]   |   <\\Children>\n[build/Package and run all tests]   |   <Parent><\\Parent>\n[build/Package and run all tests]   | <\\GBY>\n[build/Package and run all tests]   | 2023-05-28T18:09:02,946 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[build/Package and run all tests]   | 2023-05-28T18:09:02,984 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local310213606_0001_r_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:09:02,985 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[build/Package and run all tests]   | 2023-05-28T18:09:02,985 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local310213606_0001_r_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:09:02,986 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local310213606_0001_r_000000_0: Counters: 30\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=40624418\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=41755276\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tCombine output records=0\n[build/Package and run all tests]   | \t\tReduce input groups=2\n[build/Package and run all tests]   | \t\tReduce shuffle bytes=38\n[build/Package and run all tests]   | \t\tReduce input records=2\n[build/Package and run all tests]   | \t\tReduce output records=0\n[build/Package and run all tests]   | \t\tSpilled Records=2\n[build/Package and run all tests]   | \t\tShuffled Maps =1\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=1\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=1996488704\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_0=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FIL_12=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_11=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_6=2\n[build/Package and run all tests]   | \tShuffle Errors\n[build/Package and run all tests]   | \t\tBAD_ID=0\n[build/Package and run all tests]   | \t\tCONNECTION=0\n[build/Package and run all tests]   | \t\tIO_ERROR=0\n[build/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[build/Package and run all tests]   | \t\tWRONG_MAP=0\n[build/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:09:02,986 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local310213606_0001_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:09:02,987 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:09:03,506 Stage-1 map = 100%,  reduce = 100%\n[build/Package and run all tests]   | Ended Job = job_local310213606_0001\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:03,531 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:09:03,538 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:09:03,565 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.BeelineRunTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:03,621 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.317 s - in com.klarna.hiverunner.BeelineRunTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.AnnotatedFieldsInSuperClassTest\n[build/Package and run all tests]   | 2023-05-28T18:09:06,234 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | Hive Session ID = 60d87e22-cd82-497b-801e-cbe3adfb893c\n[build/Package and run all tests]   | 2023-05-28T18:09:06,699 INFO  SessionState:1227 - Hive Session ID = 60d87e22-cd82-497b-801e-cbe3adfb893c\n[build/Package and run all tests]   | 2023-05-28T18:09:07,201 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:09:08,092 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:09:09,633 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:09:10,760 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:09:14,190 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:09:14,191 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:09:14,374 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 3741def2-62e4-4ac3-8234-3fc44658f219\n[build/Package and run all tests]   | 2023-05-28T18:09:15,003 INFO  SessionState:1227 - Hive Session ID = 3741def2-62e4-4ac3-8234-3fc44658f219\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:17,191 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.test_db, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:17,574 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:09:17,596 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:09:18,009 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:09:18,151 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[build/Package and run all tests]   | 2023-05-28T18:09:18,243 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:09:18,288 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:09:18,288 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:09:18,289 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[build/Package and run all tests]   | 2023-05-28T18:09:18,305 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:09:18,305 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:09:18,321 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[build/Package and run all tests]   | 2023-05-28T18:09:18,322 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[build/Package and run all tests]   | 2023-05-28T18:09:18,335 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:09:18,336 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:09:18,343 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:09:18,343 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:09:18,353 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:09:18,353 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:09:18,354 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[build/Package and run all tests]   | 2023-05-28T18:09:18,355 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[build/Package and run all tests]   | 2023-05-28T18:09:18,395 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:09:18,396 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:09:18,399 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:09:18,399 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:09:18,403 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_1024254752' to file:/tmp/hiverunner_test6320303995371421254/warehouse1034492206591600707/test_db.db/test_table/_SCRATCH0.7557079611367071\n[build/Package and run all tests]   | 2023-05-28T18:09:18,413 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:09:18,413 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:09:18,417 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:09:18,418 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:09:18,472 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:09:18,529 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:18,796 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:09:18,864 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:09:18,895 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.AnnotatedFieldsInSuperClassTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:18,959 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.4 s - in com.klarna.hiverunner.AnnotatedFieldsInSuperClassTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.BigResultSetTest\n[build/Package and run all tests]   | 2023-05-28T18:09:21,551 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | Hive Session ID = ef36e3c7-4f97-4bf1-ad8d-32c949ab6d59\n[build/Package and run all tests]   | 2023-05-28T18:09:22,161 INFO  SessionState:1227 - Hive Session ID = ef36e3c7-4f97-4bf1-ad8d-32c949ab6d59\n[build/Package and run all tests]   | 2023-05-28T18:09:22,663 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:09:23,556 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:09:25,002 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:09:29,139 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:09:29,139 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:09:29,295 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = d261dd14-147a-4585-948c-df3bafa8d36c\n[build/Package and run all tests]   | 2023-05-28T18:09:29,761 INFO  SessionState:1227 - Hive Session ID = d261dd14-147a-4585-948c-df3bafa8d36c\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:31,921 WARN  org.apache.hadoop.hive.metastore.HiveMetaStore:1849 - Location: file:/tmp/hiverunner_test5263043930902622622/hadooptmp2428585833775795243/foo specified for non-external table:FOO\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:32,535 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:09:32,600 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:09:32,671 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.BigResultSetTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:32,731 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.838 s - in com.klarna.hiverunner.BigResultSetTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.InteractiveHiveShellTest\n[build/Package and run all tests]   | 2023-05-28T18:09:34,822 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | 2023-05-28T18:09:34,843 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.InteractiveHiveShellTest in /\n[build/Package and run all tests]   | Hive Session ID = 88a2025f-978a-41d1-8d96-aec347095cfa\n[build/Package and run all tests]   | 2023-05-28T18:09:35,907 INFO  SessionState:1227 - Hive Session ID = 88a2025f-978a-41d1-8d96-aec347095cfa\n[build/Package and run all tests]   | 2023-05-28T18:09:36,433 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:09:37,334 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:09:38,457 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:09:41,786 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:09:41,786 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:09:41,944 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = f3982ca0-56e6-4c58-8020-7a1206641856\n[build/Package and run all tests]   | 2023-05-28T18:09:42,439 INFO  SessionState:1227 - Hive Session ID = f3982ca0-56e6-4c58-8020-7a1206641856\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:44,440 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.foo, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:44,510 INFO  hive.ql.exec.DDLTask:2790 - results : 2\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:44,527 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:09:44,597 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:09:44,630 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.InteractiveHiveShellTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:44,690 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | 2023-05-28T18:09:44,697 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.InteractiveHiveShellTest in /\n[build/Package and run all tests]   | Hive Session ID = 48da121a-b4ea-455d-864f-0b5430502d80\n[build/Package and run all tests]   | 2023-05-28T18:09:44,768 INFO  SessionState:1227 - Hive Session ID = 48da121a-b4ea-455d-864f-0b5430502d80\n[build/Package and run all tests]   | 2023-05-28T18:09:44,790 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:09:44,795 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:09:45,205 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:09:47,763 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = e3175d21-b2fe-49bc-bf97-069b5b7d75af\n[build/Package and run all tests]   | 2023-05-28T18:09:47,901 INFO  SessionState:1227 - Hive Session ID = e3175d21-b2fe-49bc-bf97-069b5b7d75af\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:47,982 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.foo, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:48,312 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:09:48,315 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.InteractiveHiveShellTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:48,405 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | 2023-05-28T18:09:48,410 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.InteractiveHiveShellTest in /\n[build/Package and run all tests]   | Hive Session ID = abaa0ad2-91bc-43e2-9e83-03ab9c1fe1d6\n[build/Package and run all tests]   | 2023-05-28T18:09:48,462 INFO  SessionState:1227 - Hive Session ID = abaa0ad2-91bc-43e2-9e83-03ab9c1fe1d6\n[build/Package and run all tests]   | 2023-05-28T18:09:48,492 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:09:48,495 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:09:48,906 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:09:51,768 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 2826ad52-7ff1-4c4b-a493-951d4cd5471a\n[build/Package and run all tests]   | 2023-05-28T18:09:51,933 INFO  SessionState:1227 - Hive Session ID = 2826ad52-7ff1-4c4b-a493-951d4cd5471a\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:52,056 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.foo, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:52,241 INFO  hive.ql.exec.DDLTask:2790 - results : 2\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:52,283 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:52,320 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:09:52,323 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.InteractiveHiveShellTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:09:52,343 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 17.614 s - in com.klarna.hiverunner.InteractiveHiveShellTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.SerdeTest\n[build/Package and run all tests]   | 2023-05-28T18:09:55,083 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | Hive Session ID = b10a432f-bee7-46ba-95c7-e5eb9105538d\n[build/Package and run all tests]   | 2023-05-28T18:09:55,482 INFO  SessionState:1227 - Hive Session ID = b10a432f-bee7-46ba-95c7-e5eb9105538d\n[build/Package and run all tests]   | 2023-05-28T18:09:55,837 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:09:56,709 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:09:57,920 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:09:59,039 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:10:01,641 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:10:01,642 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:10:01,811 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = b180f2d0-1d18-4ffe-a1c9-c9de2c1c39f0\n[build/Package and run all tests]   | 2023-05-28T18:10:02,394 INFO  SessionState:1227 - Hive Session ID = b180f2d0-1d18-4ffe-a1c9-c9de2c1c39f0\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:10:04,515 WARN  org.apache.hadoop.hive.metastore.HiveMetaStore:1849 - Location: file:/tmp/hiverunner_test2986916854393801339/hadooptmp649544923183654535/serde specified for non-external table:serde_test\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:10:05,279 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:10:05,353 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:10:05,390 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.SerdeTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:10:05,448 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 137f471b-b18c-47e1-b268-8c67ff025668\n[build/Package and run all tests]   | 2023-05-28T18:10:05,521 INFO  SessionState:1227 - Hive Session ID = 137f471b-b18c-47e1-b268-8c67ff025668\n[build/Package and run all tests]   | 2023-05-28T18:10:05,545 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:10:05,549 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:10:05,934 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:10:08,612 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 2bfba700-301f-4927-9f7a-4b503f146510\n[build/Package and run all tests]   | 2023-05-28T18:10:08,764 INFO  SessionState:1227 - Hive Session ID = 2bfba700-301f-4927-9f7a-4b503f146510\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:10:08,896 WARN  org.apache.hadoop.hive.metastore.HiveMetaStore:1849 - Location: file:/tmp/hiverunner_test7091197463604939954/hadooptmp4397588394329023978/serde specified for non-external table:serde_test\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:10:09,227 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:10:09,231 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.SerdeTest\n[build/Package and run all tests]   | 2023-05-28T18:10:09,248 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.697 s - in com.klarna.hiverunner.SerdeTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.CommentTest\n[build/Package and run all tests]   | 2023-05-28T18:10:11,392 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | Hive Session ID = f8bb6365-553d-42d1-a56b-93bf8255ce58\n[build/Package and run all tests]   | 2023-05-28T18:10:11,880 INFO  SessionState:1227 - Hive Session ID = f8bb6365-553d-42d1-a56b-93bf8255ce58\n[build/Package and run all tests]   | 2023-05-28T18:10:12,421 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:10:13,306 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:10:14,841 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:10:15,971 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:10:18,792 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:10:18,793 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:10:18,957 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 0e964acb-8bac-4c10-a752-ab96e232241e\n[build/Package and run all tests]   | 2023-05-28T18:10:19,549 INFO  SessionState:1227 - Hive Session ID = 0e964acb-8bac-4c10-a752-ab96e232241e\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:10:21,596 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.CommentTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:10:21,654 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | Hive Session ID = 8959f268-343e-4d25-bcd4-7d150d08a55d\n[build/Package and run all tests]   | 2023-05-28T18:10:21,721 INFO  SessionState:1227 - Hive Session ID = 8959f268-343e-4d25-bcd4-7d150d08a55d\n[build/Package and run all tests]   | 2023-05-28T18:10:21,751 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:10:21,755 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:10:22,123 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:10:24,714 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = af20e65e-6601-4156-8feb-2fec2d55f296\n[build/Package and run all tests]   | 2023-05-28T18:10:24,882 INFO  SessionState:1227 - Hive Session ID = af20e65e-6601-4156-8feb-2fec2d55f296\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:10:24,976 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.CommentTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:10:24,998 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.049 s - in com.klarna.hiverunner.CommentTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.LeftOuterJoinTest\n[build/Package and run all tests]   | 2023-05-28T18:10:27,620 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | Hive Session ID = c0def29f-f978-4ae2-9df3-66133e17a312\n[build/Package and run all tests]   | 2023-05-28T18:10:28,147 INFO  SessionState:1227 - Hive Session ID = c0def29f-f978-4ae2-9df3-66133e17a312\n[build/Package and run all tests]   | 2023-05-28T18:10:28,703 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:10:29,620 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:10:31,164 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:10:32,378 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:10:35,257 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:10:35,258 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:10:35,414 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = 9872d1c8-54fd-48a3-958e-c2c58939610b\n[build/Package and run all tests]   | 2023-05-28T18:10:36,006 INFO  SessionState:1227 - Hive Session ID = 9872d1c8-54fd-48a3-958e-c2c58939610b\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:10:37,998 WARN  org.apache.hadoop.hive.metastore.HiveMetaStore:1849 - Location: file:/tmp/hiverunner_test6095952865636127302/hadooptmp6961114261077673879/foo specified for non-external table:foo\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:10:38,224 WARN  org.apache.hadoop.hive.metastore.HiveMetaStore:1849 - Location: file:/tmp/hiverunner_test6095952865636127302/hadooptmp6961114261077673879/bar specified for non-external table:bar\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:10:38,851 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528181038_f4d4a027-a08a-4f86-ba34-b1e054ca31a8\n[build/Package and run all tests]   | Total jobs = 1\n[build/Package and run all tests]   | Launching Job 1 out of 1\n[build/Package and run all tests]   | Number of reduce tasks not specified. Estimated from input data size: 1\n[build/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[build/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[build/Package and run all tests]   | In order to limit the maximum number of reducers:\n[build/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[build/Package and run all tests]   | In order to set a constant number of reducers:\n[build/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[build/Package and run all tests]   | 2023-05-28T18:10:39,214 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[build/Package and run all tests]   | 2023-05-28T18:10:39,238 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[build/Package and run all tests]   | 2023-05-28T18:10:39,261 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[build/Package and run all tests]   | 2023-05-28T18:10:39,262 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[build/Package and run all tests]   | 2023-05-28T18:10:39,292 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:10:39,358 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:10:39,610 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[build/Package and run all tests]   | 2023-05-28T18:10:39,630 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:10:39,631 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:10:39,673 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:2\n[build/Package and run all tests]   | 2023-05-28T18:10:39,741 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local969965960_0001\n[build/Package and run all tests]   | 2023-05-28T18:10:39,742 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   | 2023-05-28T18:10:40,040 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[build/Package and run all tests]   | Job running in-process (local Hadoop)\n[build/Package and run all tests]   | 2023-05-28T18:10:40,044 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:10:40,046 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[build/Package and run all tests]   | 2023-05-28T18:10:40,060 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[build/Package and run all tests]   | 2023-05-28T18:10:40,066 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local969965960_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:10:40,122 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:10:40,131 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test6095952865636127302/hadooptmp6961114261077673879/foo/data.csv:0+29InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:10:40,217 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[build/Package and run all tests]   | 2023-05-28T18:10:40,218 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[build/Package and run all tests]   | 2023-05-28T18:10:40,219 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[build/Package and run all tests]   | 2023-05-28T18:10:40,219 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[build/Package and run all tests]   | 2023-05-28T18:10:40,294 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[build/Package and run all tests]   | 2023-05-28T18:10:40,295 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[build/Package and run all tests]   | 2023-05-28T18:10:40,295 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[build/Package and run all tests]   | 2023-05-28T18:10:40,295 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:10:40,295 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[build/Package and run all tests]   | 2023-05-28T18:10:40,307 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[build/Package and run all tests]   | 2023-05-28T18:10:40,368 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:10:40,369 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[build/Package and run all tests]   | 2023-05-28T18:10:40,369 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[build/Package and run all tests]   | 2023-05-28T18:10:40,369 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 28; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:10:40,369 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n[build/Package and run all tests]   | 2023-05-28T18:10:40,434 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[build/Package and run all tests]   | 2023-05-28T18:10:40,456 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local969965960_0001_m_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:10:40,459 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test6095952865636127302/hadooptmp6961114261077673879/foo/data.csv:0+29\n[build/Package and run all tests]   | 2023-05-28T18:10:40,460 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local969965960_0001_m_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:10:40,465 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local969965960_0001_m_000000_0: Counters: 25\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=40624536\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=41751096\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=2\n[build/Package and run all tests]   | \t\tMap output records=2\n[build/Package and run all tests]   | \t\tMap output bytes=28\n[build/Package and run all tests]   | \t\tMap output materialized bytes=38\n[build/Package and run all tests]   | \t\tInput split bytes=240\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tSpilled Records=2\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=48\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2001207296\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_2=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_3=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_1=0\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | 2023-05-28T18:10:40,466 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local969965960_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:10:40,468 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local969965960_0001_m_000001_0\n[build/Package and run all tests]   | 2023-05-28T18:10:40,470 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:10:40,471 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test6095952865636127302/hadooptmp6961114261077673879/bar/data.csv:0+29InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[build/Package and run all tests]   | \n[build/Package and run all tests]   | 2023-05-28T18:10:40,478 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[build/Package and run all tests]   | 2023-05-28T18:10:40,489 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[build/Package and run all tests]   | 2023-05-28T18:10:40,489 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[build/Package and run all tests]   | 2023-05-28T18:10:40,489 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[build/Package and run all tests]   | 2023-05-28T18:10:40,490 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:10:40,490 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[build/Package and run all tests]   | 2023-05-28T18:10:40,491 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[build/Package and run all tests]   | 2023-05-28T18:10:40,497 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[build/Package and run all tests]   | 2023-05-28T18:10:40,498 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[build/Package and run all tests]   | 2023-05-28T18:10:40,498 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[build/Package and run all tests]   | 2023-05-28T18:10:40,498 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 52; bufvoid = 104857600\n[build/Package and run all tests]   | 2023-05-28T18:10:40,498 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n[build/Package and run all tests]   | 2023-05-28T18:10:40,649 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[build/Package and run all tests]   | 2023-05-28T18:10:40,654 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local969965960_0001_m_000001_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:10:40,656 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test6095952865636127302/hadooptmp6961114261077673879/bar/data.csv:0+29\n[build/Package and run all tests]   | 2023-05-28T18:10:40,661 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local969965960_0001_m_000001_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:10:40,661 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local969965960_0001_m_000001_0: Counters: 25\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=40625064\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=41751190\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tMap input records=2\n[build/Package and run all tests]   | \t\tMap output records=2\n[build/Package and run all tests]   | \t\tMap output bytes=52\n[build/Package and run all tests]   | \t\tMap output materialized bytes=62\n[build/Package and run all tests]   | \t\tInput split bytes=240\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tSpilled Records=2\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=0\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2001207296\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[build/Package and run all tests]   | \t\tRECORDS_IN=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_2=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_3=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_1=2\n[build/Package and run all tests]   | \tFile Input Format Counters \n[build/Package and run all tests]   | \t\tBytes Read=0\n[build/Package and run all tests]   | 2023-05-28T18:10:40,661 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local969965960_0001_m_000001_0\n[build/Package and run all tests]   | 2023-05-28T18:10:40,662 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[build/Package and run all tests]   | 2023-05-28T18:10:40,667 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[build/Package and run all tests]   | 2023-05-28T18:10:40,667 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local969965960_0001_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:10:40,680 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[build/Package and run all tests]   | 2023-05-28T18:10:40,684 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@a3234b7\n[build/Package and run all tests]   | 2023-05-28T18:10:40,686 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:10:40,707 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1400845056, maxSingleShuffleLimit=350211264, mergeThreshold=924557760, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[build/Package and run all tests]   | 2023-05-28T18:10:40,713 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local969965960_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[build/Package and run all tests]   | 2023-05-28T18:10:40,754 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#1 about to shuffle output of map attempt_local969965960_0001_m_000001_0 decomp: 58 len: 62 to MEMORY\n[build/Package and run all tests]   | 2023-05-28T18:10:40,759 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 58 bytes from map-output for attempt_local969965960_0001_m_000001_0\n[build/Package and run all tests]   | 2023-05-28T18:10:40,762 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 58, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->58\n[build/Package and run all tests]   | 2023-05-28T18:10:40,790 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#1 about to shuffle output of map attempt_local969965960_0001_m_000000_0 decomp: 34 len: 38 to MEMORY\n[build/Package and run all tests]   | 2023-05-28T18:10:40,790 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 34 bytes from map-output for attempt_local969965960_0001_m_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:10:40,791 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 34, inMemoryMapOutputs.size() -> 2, commitMemory -> 58, usedMemory ->92\n[build/Package and run all tests]   | 2023-05-28T18:10:40,792 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[build/Package and run all tests]   | 2023-05-28T18:10:40,793 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 2 / 2 copied.\n[build/Package and run all tests]   | 2023-05-28T18:10:40,794 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 2 in-memory map-outputs and 0 on-disk map-outputs\n[build/Package and run all tests]   | 2023-05-28T18:10:40,818 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 2 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:10:40,819 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 2 segments left of total size: 68 bytes\n[build/Package and run all tests]   | 2023-05-28T18:10:40,824 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 2 segments, 92 bytes to disk to satisfy reduce memory limit\n[build/Package and run all tests]   | 2023-05-28T18:10:40,825 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 94 bytes from disk\n[build/Package and run all tests]   | 2023-05-28T18:10:40,826 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[build/Package and run all tests]   | 2023-05-28T18:10:40,826 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[build/Package and run all tests]   | 2023-05-28T18:10:40,827 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 78 bytes\n[build/Package and run all tests]   | 2023-05-28T18:10:40,828 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 2 / 2 copied.\n[build/Package and run all tests]   | 2023-05-28T18:10:40,831 INFO  ExecReducer:99 - conf classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter6200154440538354518.jar]\n[build/Package and run all tests]   | 2023-05-28T18:10:40,831 INFO  ExecReducer:101 - thread classpath = [file:/tmp/bc275e72-fd77-11ed-a890-af2cc187fc11/HiveRunner-HiveRunner/target/surefire/surefirebooter6200154440538354518.jar]\n[build/Package and run all tests]   | 2023-05-28T18:10:40,845 INFO  ExecReducer:147 - \n[build/Package and run all tests]   | <JOIN>Id =4\n[build/Package and run all tests]   |   <Children>\n[build/Package and run all tests]   |     <SEL>Id =5\n[build/Package and run all tests]   |       <Children>\n[build/Package and run all tests]   |         <FS>Id =6\n[build/Package and run all tests]   |           <Children>\n[build/Package and run all tests]   |           <\\Children>\n[build/Package and run all tests]   |           <Parent>Id = 5 null<\\Parent>\n[build/Package and run all tests]   |         <\\FS>\n[build/Package and run all tests]   |       <\\Children>\n[build/Package and run all tests]   |       <Parent>Id = 4 null<\\Parent>\n[build/Package and run all tests]   |     <\\SEL>\n[build/Package and run all tests]   |   <\\Children>\n[build/Package and run all tests]   |   <Parent><\\Parent>\n[build/Package and run all tests]   | <\\JOIN>\n[build/Package and run all tests]   | 2023-05-28T18:10:40,853 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[build/Package and run all tests]   | 2023-05-28T18:10:40,855 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[build/Package and run all tests]   | 2023-05-28T18:10:40,896 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local969965960_0001_r_000000_0 is done. And is in the process of committing\n[build/Package and run all tests]   | 2023-05-28T18:10:40,898 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[build/Package and run all tests]   | 2023-05-28T18:10:40,898 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local969965960_0001_r_000000_0' done.\n[build/Package and run all tests]   | 2023-05-28T18:10:40,899 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local969965960_0001_r_000000_0: Counters: 31\n[build/Package and run all tests]   | \tFile System Counters\n[build/Package and run all tests]   | \t\tFILE: Number of bytes read=40625322\n[build/Package and run all tests]   | \t\tFILE: Number of bytes written=41751429\n[build/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[build/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[build/Package and run all tests]   | \tMap-Reduce Framework\n[build/Package and run all tests]   | \t\tCombine input records=0\n[build/Package and run all tests]   | \t\tCombine output records=0\n[build/Package and run all tests]   | \t\tReduce input groups=4\n[build/Package and run all tests]   | \t\tReduce shuffle bytes=100\n[build/Package and run all tests]   | \t\tReduce input records=4\n[build/Package and run all tests]   | \t\tReduce output records=0\n[build/Package and run all tests]   | \t\tSpilled Records=4\n[build/Package and run all tests]   | \t\tShuffled Maps =2\n[build/Package and run all tests]   | \t\tFailed Shuffles=0\n[build/Package and run all tests]   | \t\tMerged Map outputs=2\n[build/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[build/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2001207296\n[build/Package and run all tests]   | \tHIVE\n[build/Package and run all tests]   | \t\tCREATED_FILES=1\n[build/Package and run all tests]   | \t\tRECORDS_OUT_0=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_JOIN_4=2\n[build/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=2\n[build/Package and run all tests]   | \t\tSKEWJOINFOLLOWUPJOBS=0\n[build/Package and run all tests]   | \tShuffle Errors\n[build/Package and run all tests]   | \t\tBAD_ID=0\n[build/Package and run all tests]   | \t\tCONNECTION=0\n[build/Package and run all tests]   | \t\tIO_ERROR=0\n[build/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[build/Package and run all tests]   | \t\tWRONG_MAP=0\n[build/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[build/Package and run all tests]   | \tFile Output Format Counters \n[build/Package and run all tests]   | \t\tBytes Written=0\n[build/Package and run all tests]   | 2023-05-28T18:10:40,899 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local969965960_0001_r_000000_0\n[build/Package and run all tests]   | 2023-05-28T18:10:40,899 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[build/Package and run all tests]   | 2023-05-28 18:10:41,077 Stage-1 map = 100%,  reduce = 100%\n[build/Package and run all tests]   | Ended Job = job_local969965960_0001\n[build/Package and run all tests]   | MapReduce Jobs Launched: \n[build/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[build/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:10:41,101 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:10:41,109 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:10:41,139 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.LeftOuterJoinTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:10:41,201 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.295 s - in com.klarna.hiverunner.LeftOuterJoinTest\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.data.InsertIntoTableTest\n[build/Package and run all tests]   | [INFO] Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.703 s - in com.klarna.hiverunner.data.InsertIntoTableTest\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.data.TableDataBuilderTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.216 s - in com.klarna.hiverunner.data.TableDataBuilderTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.data.TableDataInserterTest\n[build/Package and run all tests]   | 2023-05-28T18:10:49,892 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | Hive Session ID = b3429819-4ecb-44b6-bb5e-d67d838b0df6\n[build/Package and run all tests]   | 2023-05-28T18:10:50,629 INFO  SessionState:1227 - Hive Session ID = b3429819-4ecb-44b6-bb5e-d67d838b0df6\n[build/Package and run all tests]   | 2023-05-28T18:10:51,070 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:10:51,691 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:10:52,890 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:10:54,118 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:10:57,429 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:10:57,430 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:10:57,571 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = e1090b74-3364-4377-82a5-a45a02a2801c\n[build/Package and run all tests]   | 2023-05-28T18:10:58,017 INFO  SessionState:1227 - Hive Session ID = e1090b74-3364-4377-82a5-a45a02a2801c\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:10:59,743 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.testdb, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:10:59,913 WARN  org.apache.hadoop.hive.metastore.HiveMetaStore:1849 - Location: file:/tmp/hiverunner_test8271963713467824079/hiverunner_data1258509537039135913 specified for non-external table:test_table\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:11:00,197 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:11:00,219 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:11:00,537 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[build/Package and run all tests]   | 2023-05-28T18:11:00,590 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:11:00,616 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:11:00,616 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:11:00,617 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[build/Package and run all tests]   | 2023-05-28T18:11:00,624 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:11:00,625 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:11:00,642 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[build/Package and run all tests]   | 2023-05-28T18:11:00,642 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[build/Package and run all tests]   | 2023-05-28T18:11:00,654 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:11:00,655 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:11:00,661 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:11:00,661 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:11:00,667 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:11:00,667 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:11:00,668 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[build/Package and run all tests]   | 2023-05-28T18:11:00,669 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[build/Package and run all tests]   | 2023-05-28T18:11:00,678 INFO  org.apache.orc.impl.MemoryManagerImpl:85 - orc.rows.between.memory.checks=5000\n[build/Package and run all tests]   | 2023-05-28T18:11:00,707 INFO  org.apache.orc.impl.PhysicalFsWriter:92 - ORC writer created for path: file:/tmp/hiverunner_test8271963713467824079/hiverunner_data1258509537039135913/_SCRATCH0.8015402545308639/local_date=2015-10-14/_temporary/0/_temporary/attempt__0000_m_000000_305694300/part-m-305694300 with stripeSize: 67108864 blockSize: 268435456 compression: ZLIB bufferSize: 262144\n[build/Package and run all tests]   | 2023-05-28T18:11:00,730 INFO  org.apache.orc.impl.OrcCodecPool:56 - Got brand-new codec ZLIB\n[build/Package and run all tests]   | 2023-05-28T18:11:00,772 INFO  org.apache.orc.impl.WriterImpl:188 - ORC writer created for path: file:/tmp/hiverunner_test8271963713467824079/hiverunner_data1258509537039135913/_SCRATCH0.8015402545308639/local_date=2015-10-14/_temporary/0/_temporary/attempt__0000_m_000000_305694300/part-m-305694300 with stripeSize: 67108864 blockSize: 268435456 compression: ZLIB bufferSize: 262144\n[build/Package and run all tests]   | 2023-05-28T18:11:00,828 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:11:00,829 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:11:00,832 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:11:00,833 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:11:00,837 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_305694300' to file:/tmp/hiverunner_test8271963713467824079/hiverunner_data1258509537039135913/_SCRATCH0.8015402545308639/local_date=2015-10-14\n[build/Package and run all tests]   | 2023-05-28T18:11:00,846 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:11:00,847 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:11:00,850 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:11:00,851 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:11:00,900 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:11:01,082 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:11:01,144 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:11:01,231 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:11:01,250 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:11:01,251 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:11:01,255 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:11:01,255 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:11:01,277 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:11:01,278 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:11:01,281 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:11:01,282 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:11:01,287 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:11:01,287 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:11:01,293 INFO  org.apache.orc.impl.PhysicalFsWriter:92 - ORC writer created for path: file:/tmp/hiverunner_test8271963713467824079/hiverunner_data1258509537039135913/_SCRATCH0.4633392428811711/local_date=2015-10-15/_temporary/0/_temporary/attempt__0000_m_000000_1345795485/part-m-1345795485 with stripeSize: 67108864 blockSize: 268435456 compression: ZLIB bufferSize: 262144\n[build/Package and run all tests]   | 2023-05-28T18:11:01,308 INFO  org.apache.orc.impl.WriterImpl:188 - ORC writer created for path: file:/tmp/hiverunner_test8271963713467824079/hiverunner_data1258509537039135913/_SCRATCH0.4633392428811711/local_date=2015-10-15/_temporary/0/_temporary/attempt__0000_m_000000_1345795485/part-m-1345795485 with stripeSize: 67108864 blockSize: 268435456 compression: ZLIB bufferSize: 262144\n[build/Package and run all tests]   | 2023-05-28T18:11:01,313 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:11:01,313 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:11:01,315 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:11:01,316 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:11:01,318 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_1345795485' to file:/tmp/hiverunner_test8271963713467824079/hiverunner_data1258509537039135913/_SCRATCH0.4633392428811711/local_date=2015-10-15\n[build/Package and run all tests]   | 2023-05-28T18:11:01,324 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:11:01,325 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:11:01,328 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[build/Package and run all tests]   | 2023-05-28T18:11:01,328 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[build/Package and run all tests]   | 2023-05-28T18:11:01,377 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | 2023-05-28T18:11:01,505 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:11:01,853 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:11:01,867 ERROR org.apache.hadoop.hive.ql.io.AcidUtils:1003 - Failed to get files with ID; using regular API: Only supported for DFS; got class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem\n[build/Package and run all tests]   | 2023-05-28T18:11:01,985 ERROR org.apache.hadoop.hive.ql.io.AcidUtils:1003 - Failed to get files with ID; using regular API: Only supported for DFS; got class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem\n[build/Package and run all tests]   | 2023-05-28T18:11:02,011 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.data.TableDataInserterTest\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:11:02,074 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[build/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.933 s - in com.klarna.hiverunner.data.TableDataInserterTest\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.data.ConvertersTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.652 s - in com.klarna.hiverunner.data.ConvertersTest\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.data.TsvFileParserTest\n[build/Package and run all tests]   | [INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.098 s - in com.klarna.hiverunner.data.TsvFileParserTest\n[build/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[build/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[build/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[build/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.TimeoutAndRetryTest\n[build/Package and run all tests]   | 2023-05-28T18:11:07,514 INFO  com.klarna.hiverunner.ThrowOnTimeout:53 - Starting timeout monitoring (30s) of test case com.klarna.hiverunner.TimeoutAndRetryTest.\n[build/Package and run all tests]   | 2023-05-28T18:11:07,546 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[build/Package and run all tests]   | 2023-05-28T18:11:07,563 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.TimeoutAndRetryTest in /\n[build/Package and run all tests]   | Hive Session ID = a34fa893-e421-4ead-87de-87b6eaefb9de\n[build/Package and run all tests]   | 2023-05-28T18:11:08,743 INFO  SessionState:1227 - Hive Session ID = a34fa893-e421-4ead-87de-87b6eaefb9de\n[build/Package and run all tests]   | 2023-05-28T18:11:09,317 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[build/Package and run all tests]   | 2023-05-28T18:11:10,242 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[build/Package and run all tests]   | 2023-05-28T18:11:11,861 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:11:13,188 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[build/Package and run all tests]   | 2023-05-28T18:11:16,422 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[build/Package and run all tests]   | 2023-05-28T18:11:16,423 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@172.17.0.12\n[build/Package and run all tests]   | 2023-05-28T18:11:16,568 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[build/Package and run all tests]   | Hive Session ID = f7c6d135-1168-455a-8f02-46e2bf1e7fec\n[build/Package and run all tests]   | 2023-05-28T18:11:16,998 INFO  SessionState:1227 - Hive Session ID = f7c6d135-1168-455a-8f02-46e2bf1e7fec\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | /\n[build/Package and run all tests]   | 2023-05-28T18:11:18,831 INFO  hive.ql.exec.DDLTask:2790 - results : 1\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:11:18,851 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[build/Package and run all tests]   | 2023-05-28T18:11:18,930 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | [default]\n[build/Package and run all tests]   | 2023-05-28T18:11:19,006 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.baz, returning NoSuchObjectException\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:11:19,082 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[build/Package and run all tests]   | [baz\t\tlocation/in/test\trunneradmin\tUSER\t]\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | OK\n[build/Package and run all tests]   | 2023-05-28T18:11:19,996 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[build/Package and run all tests]   | Query ID = runneradmin_20230528181119_aa1d1478-fe49-4660-af2e-2a9b3c7de5c2\n[build/Package and run all tests]   | Total jobs = 3\n[build/Package and run all tests]   | Launching Job 1 out of 3\n[build/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[build/Package and run all tests]   | 2023-05-28T18:11:20,310 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[build/Package and run all tests]   | 2023-05-28T18:11:20,330 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[build/Package and run all tests]   | 2023-05-28T18:11:20,348 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[build/Package and run all tests]   | 2023-05-28T18:11:20,348 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[build/Package and run all tests]   | 2023-05-28T18:11:20,382 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[build/Package and run all tests]   | 2023-05-28T18:11:20,448 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[build/Package and run all tests]   | 2023-05-28T18:11:20,631 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[build/Package and run all tests]   | 2023-05-28T18:11:20,650 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[build/Package and run all tests]   | 2023-05-28T18:11:20,697 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[build/Package and run all tests]   | 2023-05-28T18:11:20,769 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local585619948_0001\n[build/Package and run all tests]   | 2023-05-28T18:11:20,769 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[build/Package and run all tests]   \u274c  Failure - Main Run Maven Targets\n[build/Package and run all tests] Get \"http://%2Fvar%2Frun%2Fdocker.sock/v1.41/containers/77bc78e59236ff069e81d465ca721404c812a6ff55fbd9a6ff2dae6c8587654a/archive?path=%2Fvar%2Frun%2Fact%2Fworkflow%2Fpathcmd.txt\": context canceled\n[build/Package and run all tests] \u2b50 Run Post Set up JDK\n[build/Package and run all tests]   \ud83d\udc33  docker exec cmd=[node /var/run/act/actions/actions-setup-java@v2/dist/cleanup/index.js] user= workdir=\n[build/Package and run all tests]   \u2705  Success - Post Set up JDK\n[build/Package and run all tests] \ud83c\udfc1  Job succeeded\n",
    "actions_stderr": "Error: context canceled\n"
}