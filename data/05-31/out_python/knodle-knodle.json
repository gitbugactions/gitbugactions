{
    "repository": "knodle/knodle",
    "clone_url": "https://github.com/knodle/knodle.git",
    "timestamp": "2023-05-29T15:56:31.269153Z",
    "clone_success": true,
    "number of actions": 4,
    "number_of_test_actions": 1,
    "actions_successful": false,
    "actions_stdout": "[test/build] \ud83d\ude80  Start image=crawlergpt:latest\n[test/build]   \ud83d\udc33  docker pull image=crawlergpt:latest platform= username= forcePull=false\n[test/build]   \ud83d\udc33  docker create image=crawlergpt:latest platform= entrypoint=[\"tail\" \"-f\" \"/dev/null\"] cmd=[]\n[test/build]   \ud83d\udc33  docker run image=crawlergpt:latest platform= entrypoint=[\"tail\" \"-f\" \"/dev/null\"] cmd=[]\n[test/build]   \ud83d\udc33  docker exec cmd=[chown -R 1012:1000 /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/knodle-knodle] user=0 workdir=\n[test/build]   \u2601  git clone 'https://github.com/actions/setup-python' # ref=v2\n[test/build]   \u2601  git clone 'https://github.com/codecov/codecov-action' # ref=v1\n[test/build] \u2b50 Run Main actions/checkout@v2\n[test/build]   \u2705  Success - Main actions/checkout@v2\n[test/build] \u2b50 Run Main Set up Python 3.7\n[test/build]   \ud83d\udc33  docker cp src=/home/andre-silva/.cache/act/actions-setup-python@v2/ dst=/var/run/act/actions/actions-setup-python@v2/\n[test/build]   \ud83d\udc33  docker exec cmd=[chown -R 1012:1000 /var/run/act/actions/actions-setup-python@v2/] user=0 workdir=\n[test/build]   \ud83d\udc33  docker exec cmd=[node /var/run/act/actions/actions-setup-python@v2/dist/setup/index.js] user= workdir=\n[test/build]   \ud83d\udcac  ::debug::Semantic version spec of 3.7 is 3.7\n[test/build]   \ud83d\udcac  ::debug::isExplicit: \n[test/build]   \ud83d\udcac  ::debug::explicit? false\n[test/build]   \ud83d\udcac  ::debug::isExplicit: 2.7.18\n[test/build]   \ud83d\udcac  ::debug::explicit? true\n[test/build]   \ud83d\udcac  ::debug::isExplicit: 3.5.10\n[test/build]   \ud83d\udcac  ::debug::explicit? true\n[test/build]   \ud83d\udcac  ::debug::isExplicit: 3.6.14\n[test/build]   \ud83d\udcac  ::debug::explicit? true\n[test/build]   \ud83d\udcac  ::debug::isExplicit: 3.7.11\n[test/build]   \ud83d\udcac  ::debug::explicit? true\n[test/build]   \ud83d\udcac  ::debug::isExplicit: 3.8.11\n[test/build]   \ud83d\udcac  ::debug::explicit? true\n[test/build]   \ud83d\udcac  ::debug::isExplicit: 3.9.6\n[test/build]   \ud83d\udcac  ::debug::explicit? true\n[test/build]   \ud83d\udcac  ::debug::evaluating 6 versions\n[test/build]   \ud83d\udcac  ::debug::matched: 3.7.11\n[test/build]   \ud83d\udcac  ::debug::checking cache: /opt/hostedtoolcache/Python/3.7.11/x64\n[test/build]   \ud83d\udcac  ::debug::Found tool in cache Python 3.7.11 x64\n[test/build]   | Successfully setup CPython (3.7.11)\n[test/build]   \u2753 add-matcher /run/act/actions/actions-setup-python@v2/.github/python.json\n[test/build]   \u2705  Success - Main Set up Python 3.7\n[test/build]   \u2699  ::set-env:: pythonLocation=/opt/hostedtoolcache/Python/3.7.11/x64\n[test/build]   \u2699  ::set-env:: LD_LIBRARY_PATH=/opt/hostedtoolcache/Python/3.7.11/x64/lib\n[test/build]   \u2699  ::set-output:: python-version=3.7.11\n[test/build]   \u2699  ::add-path:: /opt/hostedtoolcache/Python/3.7.11/x64\n[test/build]   \u2699  ::add-path:: /opt/hostedtoolcache/Python/3.7.11/x64/bin\n[test/build] \u2b50 Run Main Install dependencies\n[test/build]   \ud83d\udc33  docker exec cmd=[bash --noprofile --norc -e -o pipefail /var/run/act/workflow/2] user= workdir=\n[test/build]   | Requirement already satisfied: pip in /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages (21.2.4)\n[test/build]   | Collecting pip\n[test/build]   |   Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)\n[test/build]   | Installing collected packages: pip\n[test/build]   |   Attempting uninstall: pip\n[test/build]   |     Found existing installation: pip 21.2.4\n[test/build]   |     Uninstalling pip-21.2.4:\n[test/build]   |       Successfully uninstalled pip-21.2.4\n[test/build]   | Successfully installed pip-23.1.2\n[test/build]   | Collecting joblib (from -r requirements.txt (line 2))\n[test/build]   |   Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 298.0/298.0 kB 3.1 MB/s eta 0:00:00\n[test/build]   | Collecting tqdm (from -r requirements.txt (line 3))\n[test/build]   |   Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 77.1/77.1 kB 6.0 MB/s eta 0:00:00\n[test/build]   | Collecting numpy<1.20.0,>=1.16.5 (from -r requirements.txt (line 6))\n[test/build]   |   Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.8/14.8 MB 23.5 MB/s eta 0:00:00\n[test/build]   | Collecting scipy (from -r requirements.txt (line 7))\n[test/build]   |   Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 38.1/38.1 MB 25.9 MB/s eta 0:00:00\n[test/build]   | Collecting pandas (from -r requirements.txt (line 8))\n[test/build]   |   Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 11.3/11.3 MB 39.9 MB/s eta 0:00:00\n[test/build]   | Collecting scikit-learn (from -r requirements.txt (line 11))\n[test/build]   |   Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 24.8/24.8 MB 23.9 MB/s eta 0:00:00\n[test/build]   | Collecting torch (from -r requirements.txt (line 12))\n[test/build]   |   Downloading torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 887.5/887.5 MB 1.8 MB/s eta 0:00:00\n[test/build]   | Collecting snorkel<=0.9.7 (from -r requirements.txt (line 13))\n[test/build]   |   Downloading snorkel-0.9.7-py3-none-any.whl (145 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 145.5/145.5 kB 5.7 MB/s eta 0:00:00\n[test/build]   | Collecting annoy (from -r requirements.txt (line 16))\n[test/build]   |   Downloading annoy-1.17.2.tar.gz (647 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 647.4/647.4 kB 6.4 MB/s eta 0:00:00\n[test/build]   |   Installing build dependencies: started\n[test/build]   |   Installing build dependencies: finished with status 'done'\n[test/build]   |   Getting requirements to build wheel: started\n[test/build]   |   Getting requirements to build wheel: finished with status 'done'\n[test/build]   |   Installing backend dependencies: started\n[test/build]   |   Installing backend dependencies: finished with status 'done'\n[test/build]   |   Preparing metadata (pyproject.toml): started\n[test/build]   |   Preparing metadata (pyproject.toml): finished with status 'done'\n[test/build]   | Collecting matplotlib (from -r requirements.txt (line 19))\n[test/build]   |   Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 11.2/11.2 MB 32.5 MB/s eta 0:00:00\n[test/build]   | Collecting skorch (from -r requirements.txt (line 22))\n[test/build]   |   Downloading skorch-0.13.0-py3-none-any.whl (209 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 210.0/210.0 kB 4.5 MB/s eta 0:00:00\n[test/build]   | Collecting cleanlab (from -r requirements.txt (line 23))\n[test/build]   |   Downloading cleanlab-2.4.0-py3-none-any.whl (225 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 225.1/225.1 kB 3.0 MB/s eta 0:00:00\n[test/build]   | Collecting python-dateutil>=2.7.3 (from pandas->-r requirements.txt (line 8))\n[test/build]   |   Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 247.7/247.7 kB 10.6 MB/s eta 0:00:00\n[test/build]   | Collecting pytz>=2017.3 (from pandas->-r requirements.txt (line 8))\n[test/build]   |   Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 502.3/502.3 kB 8.6 MB/s eta 0:00:00\n[test/build]   | Collecting threadpoolctl>=2.0.0 (from scikit-learn->-r requirements.txt (line 11))\n[test/build]   |   Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n[test/build]   | Collecting typing-extensions (from torch->-r requirements.txt (line 12))\n[test/build]   |   Downloading typing_extensions-4.6.2-py3-none-any.whl (31 kB)\n[test/build]   | Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch->-r requirements.txt (line 12))\n[test/build]   |   Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 849.3/849.3 kB 13.6 MB/s eta 0:00:00\n[test/build]   | Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch->-r requirements.txt (line 12))\n[test/build]   |   Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 557.1/557.1 MB 2.3 MB/s eta 0:00:00\n[test/build]   | Collecting nvidia-cublas-cu11==11.10.3.66 (from torch->-r requirements.txt (line 12))\n[test/build]   |   Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 317.1/317.1 MB 5.8 MB/s eta 0:00:00\n[test/build]   | Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch->-r requirements.txt (line 12))\n[test/build]   |   Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 21.0/21.0 MB 33.3 MB/s eta 0:00:00\n[test/build]   | Requirement already satisfied: setuptools in /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 12)) (47.1.0)\n[test/build]   | Collecting wheel (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 12))\n[test/build]   |   Using cached wheel-0.40.0-py3-none-any.whl (64 kB)\n[test/build]   | Collecting munkres>=1.0.6 (from snorkel<=0.9.7->-r requirements.txt (line 13))\n[test/build]   |   Downloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n[test/build]   | Collecting scikit-learn (from -r requirements.txt (line 11))\n[test/build]   |   Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 22.3/22.3 MB 34.6 MB/s eta 0:00:00\n[test/build]   | Collecting tensorboard<2.0.0,>=1.14.0 (from snorkel<=0.9.7->-r requirements.txt (line 13))\n[test/build]   |   Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.8/3.8 MB 41.4 MB/s eta 0:00:00\n[test/build]   | Collecting networkx<2.4,>=2.2 (from snorkel<=0.9.7->-r requirements.txt (line 13))\n[test/build]   |   Downloading networkx-2.3.zip (1.7 MB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.7/1.7 MB 39.2 MB/s eta 0:00:00\n[test/build]   |   Installing build dependencies: started\n[test/build]   |   Installing build dependencies: finished with status 'done'\n[test/build]   |   Getting requirements to build wheel: started\n[test/build]   |   Getting requirements to build wheel: finished with status 'done'\n[test/build]   |   Preparing metadata (pyproject.toml): started\n[test/build]   |   Preparing metadata (pyproject.toml): finished with status 'done'\n[test/build]   | Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 19))\n[test/build]   |   Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n[test/build]   | Collecting fonttools>=4.22.0 (from matplotlib->-r requirements.txt (line 19))\n[test/build]   |   Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 965.4/965.4 kB 11.1 MB/s eta 0:00:00\n[test/build]   | Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 19))\n[test/build]   |   Downloading kiwisolver-1.4.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.1/1.1 MB 27.1 MB/s eta 0:00:00\n[test/build]   | Collecting packaging>=20.0 (from matplotlib->-r requirements.txt (line 19))\n[test/build]   |   Downloading packaging-23.1-py3-none-any.whl (48 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 48.9/48.9 kB 529.2 kB/s eta 0:00:00\n[test/build]   | Collecting pillow>=6.2.0 (from matplotlib->-r requirements.txt (line 19))\n[test/build]   |   Downloading Pillow-9.5.0-cp37-cp37m-manylinux_2_28_x86_64.whl (3.4 MB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 24.4 MB/s eta 0:00:00\n[test/build]   | Collecting pyparsing>=2.2.1 (from matplotlib->-r requirements.txt (line 19))\n[test/build]   |   Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 98.3/98.3 kB 3.7 MB/s eta 0:00:00\n[test/build]   | Collecting tabulate>=0.7.7 (from skorch->-r requirements.txt (line 22))\n[test/build]   |   Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n[test/build]   | INFO: pip is looking at multiple versions of cleanlab to determine which version is compatible with other requirements. This could take a while.\n[test/build]   | Collecting cleanlab (from -r requirements.txt (line 23))\n[test/build]   |   Downloading cleanlab-2.3.1-py3-none-any.whl (175 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 175.8/175.8 kB 3.8 MB/s eta 0:00:00\n[test/build]   |   Downloading cleanlab-2.3.0-py3-none-any.whl (174 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 174.3/174.3 kB 11.9 MB/s eta 0:00:00\n[test/build]   |   Downloading cleanlab-2.2.0-py3-none-any.whl (157 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 157.5/157.5 kB 15.9 MB/s eta 0:00:00\n[test/build]   | Collecting termcolor>=1.1.0 (from cleanlab->-r requirements.txt (line 23))\n[test/build]   |   Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n[test/build]   | Collecting decorator>=4.3.0 (from networkx<2.4,>=2.2->snorkel<=0.9.7->-r requirements.txt (line 13))\n[test/build]   |   Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n[test/build]   | Collecting six>=1.5 (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 8))\n[test/build]   |   Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n[test/build]   | Collecting absl-py>=0.4 (from tensorboard<2.0.0,>=1.14.0->snorkel<=0.9.7->-r requirements.txt (line 13))\n[test/build]   |   Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 126.5/126.5 kB 15.0 MB/s eta 0:00:00\n[test/build]   | Collecting grpcio>=1.6.3 (from tensorboard<2.0.0,>=1.14.0->snorkel<=0.9.7->-r requirements.txt (line 13))\n[test/build]   |   Downloading grpcio-1.54.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5.1/5.1 MB 28.0 MB/s eta 0:00:00\n[test/build]   | Collecting markdown>=2.6.8 (from tensorboard<2.0.0,>=1.14.0->snorkel<=0.9.7->-r requirements.txt (line 13))\n[test/build]   |   Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 93.9/93.9 kB 1.1 MB/s eta 0:00:00\n[test/build]   | Collecting protobuf>=3.6.0 (from tensorboard<2.0.0,>=1.14.0->snorkel<=0.9.7->-r requirements.txt (line 13))\n[test/build]   |   Downloading protobuf-4.23.2-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 304.5/304.5 kB 6.9 MB/s eta 0:00:00\n[test/build]   | Collecting werkzeug>=0.11.15 (from tensorboard<2.0.0,>=1.14.0->snorkel<=0.9.7->-r requirements.txt (line 13))\n[test/build]   |   Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 233.6/233.6 kB 2.4 MB/s eta 0:00:00\n[test/build]   | Collecting importlib-metadata>=4.4 (from markdown>=2.6.8->tensorboard<2.0.0,>=1.14.0->snorkel<=0.9.7->-r requirements.txt (line 13))\n[test/build]   |   Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n[test/build]   | Collecting MarkupSafe>=2.1.1 (from werkzeug>=0.11.15->tensorboard<2.0.0,>=1.14.0->snorkel<=0.9.7->-r requirements.txt (line 13))\n[test/build]   |   Downloading MarkupSafe-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n[test/build]   | Collecting zipp>=0.5 (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.0.0,>=1.14.0->snorkel<=0.9.7->-r requirements.txt (line 13))\n[test/build]   |   Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n[test/build]   | Building wheels for collected packages: annoy, networkx\n[test/build]   |   Building wheel for annoy (pyproject.toml): started\n[test/build]   |   Building wheel for annoy (pyproject.toml): finished with status 'done'\n[test/build]   |   Created wheel for annoy: filename=annoy-1.17.2-cp37-cp37m-linux_x86_64.whl size=585734 sha256=840ff0bbfd5b5165666a712013d68ef4bbfdee71ba32139bdd7cb452f698e8d5\n[test/build]   |   Stored in directory: /home/runneradmin/.cache/pip/wheels/e1/a2/9e/2b4a8ef2f6dc9459cbe6b8e60cdf54d8c8a9d5ece70728b73e\n[test/build]   |   Building wheel for networkx (pyproject.toml): started\n[test/build]   |   Building wheel for networkx (pyproject.toml): finished with status 'done'\n[test/build]   |   Created wheel for networkx: filename=networkx-2.3-py2.py3-none-any.whl size=1556006 sha256=da8ef472f9c84f3648ecb4abd4278ce654c92615d4024424ae9f9e51a650d506\n[test/build]   |   Stored in directory: /home/runneradmin/.cache/pip/wheels/44/e6/b8/4efaab31158e9e9ca9ed80b11f6b11130bac9a9672b3cbbeaf\n[test/build]   | Successfully built annoy networkx\n[test/build]   | Installing collected packages: pytz, munkres, annoy, zipp, wheel, typing-extensions, tqdm, threadpoolctl, termcolor, tabulate, six, pyparsing, protobuf, pillow, packaging, nvidia-cuda-nvrtc-cu11, numpy, MarkupSafe, joblib, grpcio, fonttools, decorator, cycler, absl-py, werkzeug, scipy, python-dateutil, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, networkx, kiwisolver, importlib-metadata, scikit-learn, pandas, nvidia-cudnn-cu11, matplotlib, markdown, torch, tensorboard, skorch, cleanlab, snorkel\n[test/build]   | Successfully installed MarkupSafe-2.1.2 absl-py-1.4.0 annoy-1.17.2 cleanlab-2.2.0 cycler-0.11.0 decorator-5.1.1 fonttools-4.38.0 grpcio-1.54.2 importlib-metadata-6.6.0 joblib-1.2.0 kiwisolver-1.4.4 markdown-3.4.3 matplotlib-3.5.3 munkres-1.1.4 networkx-2.3 numpy-1.19.5 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 packaging-23.1 pandas-1.3.5 pillow-9.5.0 protobuf-4.23.2 pyparsing-3.0.9 python-dateutil-2.8.2 pytz-2023.3 scikit-learn-0.24.2 scipy-1.7.3 six-1.16.0 skorch-0.13.0 snorkel-0.9.7 tabulate-0.9.0 tensorboard-1.15.0 termcolor-2.3.0 threadpoolctl-3.1.0 torch-1.13.1 tqdm-4.65.0 typing-extensions-4.6.2 werkzeug-2.2.3 wheel-0.40.0 zipp-3.15.0\n[test/build]   | \n[test/build]   | [notice] A new release of pip is available: 20.1.1 -> 23.1.2\n[test/build]   | [notice] To update, run: pip install --upgrade pip\n[test/build]   \u2705  Success - Main Install dependencies\n[test/build] \u2b50 Run Main Lint with flake8\n[test/build]   \ud83d\udc33  docker exec cmd=[bash --noprofile --norc -e -o pipefail /var/run/act/workflow/3] user= workdir=\n[test/build]   | Collecting flake8\n[test/build]   |   Downloading flake8-5.0.4-py2.py3-none-any.whl (61 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 61.9/61.9 kB 923.1 kB/s eta 0:00:00\n[test/build]   | Collecting mccabe<0.8.0,>=0.7.0 (from flake8)\n[test/build]   |   Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n[test/build]   | Collecting pycodestyle<2.10.0,>=2.9.0 (from flake8)\n[test/build]   |   Downloading pycodestyle-2.9.1-py2.py3-none-any.whl (41 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 41.5/41.5 kB 5.2 MB/s eta 0:00:00\n[test/build]   | Collecting pyflakes<2.6.0,>=2.5.0 (from flake8)\n[test/build]   |   Downloading pyflakes-2.5.0-py2.py3-none-any.whl (66 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.1/66.1 kB 1.4 MB/s eta 0:00:00\n[test/build]   | Collecting importlib-metadata<4.3,>=1.1.0 (from flake8)\n[test/build]   |   Downloading importlib_metadata-4.2.0-py3-none-any.whl (16 kB)\n[test/build]   | Requirement already satisfied: zipp>=0.5 in /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages (from importlib-metadata<4.3,>=1.1.0->flake8) (3.15.0)\n[test/build]   | Requirement already satisfied: typing-extensions>=3.6.4 in /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages (from importlib-metadata<4.3,>=1.1.0->flake8) (4.6.2)\n[test/build]   | Installing collected packages: pyflakes, pycodestyle, mccabe, importlib-metadata, flake8\n[test/build]   |   Attempting uninstall: importlib-metadata\n[test/build]   |     Found existing installation: importlib-metadata 6.6.0\n[test/build]   |     Uninstalling importlib-metadata-6.6.0:\n[test/build]   |       Successfully uninstalled importlib-metadata-6.6.0\n[test/build]   | ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n[test/build]   | markdown 3.4.3 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 4.2.0 which is incompatible.\n[test/build]   | Successfully installed flake8-5.0.4 importlib-metadata-4.2.0 mccabe-0.7.0 pycodestyle-2.9.1 pyflakes-2.5.0\n[test/build]   | \n[test/build]   | [notice] A new release of pip is available: 20.1.1 -> 23.1.2\n[test/build]   | [notice] To update, run: pip install --upgrade pip\n[test/build]   | 0\n[test/build]   \u2705  Success - Main Lint with flake8\n[test/build] \u2b50 Run Main Test with pytest\n[test/build]   \ud83d\udc33  docker exec cmd=[bash --noprofile --norc -e -o pipefail /var/run/act/workflow/4] user= workdir=\n[test/build]   | Collecting pytest\n[test/build]   |   Downloading pytest-7.3.1-py3-none-any.whl (320 kB)\n[test/build]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 320.5/320.5 kB 5.4 MB/s eta 0:00:00\n[test/build]   | Collecting iniconfig (from pytest)\n[test/build]   |   Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n[test/build]   | Requirement already satisfied: packaging in /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages (from pytest) (23.1)\n[test/build]   | Collecting pluggy<2.0,>=0.12 (from pytest)\n[test/build]   |   Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n[test/build]   | Collecting exceptiongroup>=1.0.0rc8 (from pytest)\n[test/build]   |   Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n[test/build]   | Collecting tomli>=1.0.0 (from pytest)\n[test/build]   |   Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n[test/build]   | Requirement already satisfied: importlib-metadata>=0.12 in /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages (from pytest) (4.2.0)\n[test/build]   | Requirement already satisfied: zipp>=0.5 in /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest) (3.15.0)\n[test/build]   | Requirement already satisfied: typing-extensions>=3.6.4 in /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest) (4.6.2)\n[test/build]   | Installing collected packages: tomli, iniconfig, exceptiongroup, pluggy, pytest\n[test/build]   | Successfully installed exceptiongroup-1.1.1 iniconfig-2.0.0 pluggy-1.0.0 pytest-7.3.1 tomli-2.0.1\n[test/build]   | \n[test/build]   | [notice] A new release of pip is available: 20.1.1 -> 23.1.2\n[test/build]   | [notice] To update, run: pip install --upgrade pip\n[test/build]   | ============================= test session starts ==============================\n[test/build]   | platform linux -- Python 3.7.11, pytest-7.3.1, pluggy-1.0.0\n[test/build]   | rootdir: /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/knodle-knodle\n[test/build]   | collected 16 items / 8 errors\n[test/build]   | \n[test/build]   | ==================================== ERRORS ====================================\n[test/build]   | _____________ ERROR collecting tests/trainer/test_auto_trainer.py ______________\n[test/build]   | tests/trainer/test_auto_trainer.py:3: in <module>\n[test/build]   |     from knodle.trainer.auto_trainer import AutoTrainer\n[test/build]   | knodle/trainer/__init__.py:1: in <module>\n[test/build]   |     from knodle.trainer.config import TrainerConfig\n[test/build]   | knodle/trainer/config.py:6: in <module>\n[test/build]   |     from snorkel.classification import cross_entropy_with_probs\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/__init__.py:7: in <module>\n[test/build]   |     from .training.loggers import (  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/training/loggers/__init__.py:4: in <module>\n[test/build]   |     from .tensorboard_writer import TensorBoardWriter  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/training/loggers/tensorboard_writer.py:3: in <module>\n[test/build]   |     from torch.utils.tensorboard import SummaryWriter\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:12: in <module>\n[test/build]   |     from .writer import FileWriter, SummaryWriter  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/utils/tensorboard/writer.py:9: in <module>\n[test/build]   |     from tensorboard.compat.proto.event_pb2 import SessionLog\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/event_pb2.py:17: in <module>\n[test/build]   |     from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/summary_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/tensor_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/resource_handle_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py:42: in <module>\n[test/build]   |     serialized_options=None, file=DESCRIPTOR),\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/google/protobuf/descriptor.py:561: in __new__\n[test/build]   |     _message.Message._CheckCalledFromGeneratedFile()\n[test/build]   | E   TypeError: Descriptors cannot not be created directly.\n[test/build]   | E   If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n[test/build]   | E   If you cannot immediately regenerate your protos, some other possible workarounds are:\n[test/build]   | E    1. Downgrade the protobuf package to 3.20.x or lower.\n[test/build]   | E    2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n[test/build]   | E   \n[test/build]   | E   More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n[test/build]   | ------------------------------- Captured stderr --------------------------------\n[test/build]   | 2023-05-29 16:04:19,288 root         INFO     Initalized logger\n[test/build]   | ______________ ERROR collecting tests/trainer/test_multi_label.py ______________\n[test/build]   | tests/trainer/test_multi_label.py:3: in <module>\n[test/build]   |     from knodle.trainer import MajorityVoteTrainer, MajorityConfig\n[test/build]   | knodle/trainer/__init__.py:1: in <module>\n[test/build]   |     from knodle.trainer.config import TrainerConfig\n[test/build]   | knodle/trainer/config.py:6: in <module>\n[test/build]   |     from snorkel.classification import cross_entropy_with_probs\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/__init__.py:7: in <module>\n[test/build]   |     from .training.loggers import (  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/training/loggers/__init__.py:4: in <module>\n[test/build]   |     from .tensorboard_writer import TensorBoardWriter  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/training/loggers/tensorboard_writer.py:3: in <module>\n[test/build]   |     from torch.utils.tensorboard import SummaryWriter\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:12: in <module>\n[test/build]   |     from .writer import FileWriter, SummaryWriter  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/utils/tensorboard/writer.py:9: in <module>\n[test/build]   |     from tensorboard.compat.proto.event_pb2 import SessionLog\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/event_pb2.py:17: in <module>\n[test/build]   |     from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/summary_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/tensor_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/resource_handle_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py:42: in <module>\n[test/build]   |     serialized_options=None, file=DESCRIPTOR),\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/google/protobuf/descriptor.py:561: in __new__\n[test/build]   |     _message.Message._CheckCalledFromGeneratedFile()\n[test/build]   | E   TypeError: Descriptors cannot not be created directly.\n[test/build]   | E   If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n[test/build]   | E   If you cannot immediately regenerate your protos, some other possible workarounds are:\n[test/build]   | E    1. Downgrade the protobuf package to 3.20.x or lower.\n[test/build]   | E    2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n[test/build]   | E   \n[test/build]   | E   More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n[test/build]   | _____________ ERROR collecting tests/trainer/test_multi_trainer.py _____________\n[test/build]   | tests/trainer/test_multi_trainer.py:3: in <module>\n[test/build]   |     from knodle.trainer.multi_trainer import MultiTrainer\n[test/build]   | knodle/trainer/__init__.py:1: in <module>\n[test/build]   |     from knodle.trainer.config import TrainerConfig\n[test/build]   | knodle/trainer/config.py:6: in <module>\n[test/build]   |     from snorkel.classification import cross_entropy_with_probs\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/__init__.py:7: in <module>\n[test/build]   |     from .training.loggers import (  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/training/loggers/__init__.py:4: in <module>\n[test/build]   |     from .tensorboard_writer import TensorBoardWriter  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/training/loggers/tensorboard_writer.py:3: in <module>\n[test/build]   |     from torch.utils.tensorboard import SummaryWriter\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:12: in <module>\n[test/build]   |     from .writer import FileWriter, SummaryWriter  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/utils/tensorboard/writer.py:9: in <module>\n[test/build]   |     from tensorboard.compat.proto.event_pb2 import SessionLog\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/event_pb2.py:17: in <module>\n[test/build]   |     from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/summary_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/tensor_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/resource_handle_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py:42: in <module>\n[test/build]   |     serialized_options=None, file=DESCRIPTOR),\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/google/protobuf/descriptor.py:561: in __new__\n[test/build]   |     _message.Message._CheckCalledFromGeneratedFile()\n[test/build]   | E   TypeError: Descriptors cannot not be created directly.\n[test/build]   | E   If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n[test/build]   | E   If you cannot immediately regenerate your protos, some other possible workarounds are:\n[test/build]   | E    1. Downgrade the protobuf package to 3.20.x or lower.\n[test/build]   | E    2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n[test/build]   | E   \n[test/build]   | E   More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n[test/build]   | ______________ ERROR collecting tests/trainer/cleanlab/test_cl.py ______________\n[test/build]   | tests/trainer/cleanlab/test_cl.py:5: in <module>\n[test/build]   |     from knodle.trainer.cleanlab.cleanlab import CleanLabTrainer\n[test/build]   | knodle/trainer/__init__.py:1: in <module>\n[test/build]   |     from knodle.trainer.config import TrainerConfig\n[test/build]   | knodle/trainer/config.py:6: in <module>\n[test/build]   |     from snorkel.classification import cross_entropy_with_probs\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/__init__.py:7: in <module>\n[test/build]   |     from .training.loggers import (  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/training/loggers/__init__.py:4: in <module>\n[test/build]   |     from .tensorboard_writer import TensorBoardWriter  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/training/loggers/tensorboard_writer.py:3: in <module>\n[test/build]   |     from torch.utils.tensorboard import SummaryWriter\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:12: in <module>\n[test/build]   |     from .writer import FileWriter, SummaryWriter  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/utils/tensorboard/writer.py:9: in <module>\n[test/build]   |     from tensorboard.compat.proto.event_pb2 import SessionLog\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/event_pb2.py:17: in <module>\n[test/build]   |     from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/summary_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/tensor_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/resource_handle_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py:42: in <module>\n[test/build]   |     serialized_options=None, file=DESCRIPTOR),\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/google/protobuf/descriptor.py:561: in __new__\n[test/build]   |     _message.Message._CheckCalledFromGeneratedFile()\n[test/build]   | E   TypeError: Descriptors cannot not be created directly.\n[test/build]   | E   If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n[test/build]   | E   If you cannot immediately regenerate your protos, some other possible workarounds are:\n[test/build]   | E    1. Downgrade the protobuf package to 3.20.x or lower.\n[test/build]   | E    2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n[test/build]   | E   \n[test/build]   | E   More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n[test/build]   | _____________ ERROR collecting tests/trainer/snorkel/test_utils.py _____________\n[test/build]   | tests/trainer/snorkel/test_utils.py:6: in <module>\n[test/build]   |     from knodle.trainer.snorkel.utils import (\n[test/build]   | knodle/trainer/__init__.py:1: in <module>\n[test/build]   |     from knodle.trainer.config import TrainerConfig\n[test/build]   | knodle/trainer/config.py:6: in <module>\n[test/build]   |     from snorkel.classification import cross_entropy_with_probs\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/__init__.py:7: in <module>\n[test/build]   |     from .training.loggers import (  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/training/loggers/__init__.py:4: in <module>\n[test/build]   |     from .tensorboard_writer import TensorBoardWriter  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/training/loggers/tensorboard_writer.py:3: in <module>\n[test/build]   |     from torch.utils.tensorboard import SummaryWriter\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:12: in <module>\n[test/build]   |     from .writer import FileWriter, SummaryWriter  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/utils/tensorboard/writer.py:9: in <module>\n[test/build]   |     from tensorboard.compat.proto.event_pb2 import SessionLog\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/event_pb2.py:17: in <module>\n[test/build]   |     from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/summary_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/tensor_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/resource_handle_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py:42: in <module>\n[test/build]   |     serialized_options=None, file=DESCRIPTOR),\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/google/protobuf/descriptor.py:561: in __new__\n[test/build]   |     _message.Message._CheckCalledFromGeneratedFile()\n[test/build]   | E   TypeError: Descriptors cannot not be created directly.\n[test/build]   | E   If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n[test/build]   | E   If you cannot immediately regenerate your protos, some other possible workarounds are:\n[test/build]   | E    1. Downgrade the protobuf package to 3.20.x or lower.\n[test/build]   | E    2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n[test/build]   | E   \n[test/build]   | E   More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n[test/build]   | _____________ ERROR collecting tests/trainer/utils/test_denoise.py _____________\n[test/build]   | tests/trainer/utils/test_denoise.py:5: in <module>\n[test/build]   |     from knodle.trainer.utils.denoise import activate_neighbors\n[test/build]   | knodle/trainer/__init__.py:1: in <module>\n[test/build]   |     from knodle.trainer.config import TrainerConfig\n[test/build]   | knodle/trainer/config.py:6: in <module>\n[test/build]   |     from snorkel.classification import cross_entropy_with_probs\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/__init__.py:7: in <module>\n[test/build]   |     from .training.loggers import (  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/training/loggers/__init__.py:4: in <module>\n[test/build]   |     from .tensorboard_writer import TensorBoardWriter  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/training/loggers/tensorboard_writer.py:3: in <module>\n[test/build]   |     from torch.utils.tensorboard import SummaryWriter\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:12: in <module>\n[test/build]   |     from .writer import FileWriter, SummaryWriter  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/utils/tensorboard/writer.py:9: in <module>\n[test/build]   |     from tensorboard.compat.proto.event_pb2 import SessionLog\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/event_pb2.py:17: in <module>\n[test/build]   |     from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/summary_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/tensor_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/resource_handle_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py:42: in <module>\n[test/build]   |     serialized_options=None, file=DESCRIPTOR),\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/google/protobuf/descriptor.py:561: in __new__\n[test/build]   |     _message.Message._CheckCalledFromGeneratedFile()\n[test/build]   | E   TypeError: Descriptors cannot not be created directly.\n[test/build]   | E   If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n[test/build]   | E   If you cannot immediately regenerate your protos, some other possible workarounds are:\n[test/build]   | E    1. Downgrade the protobuf package to 3.20.x or lower.\n[test/build]   | E    2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n[test/build]   | E   \n[test/build]   | E   More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n[test/build]   | ___________ ERROR collecting tests/trainer/wscrossweigh/test_wscw.py ___________\n[test/build]   | tests/trainer/wscrossweigh/test_wscw.py:3: in <module>\n[test/build]   |     from knodle.trainer.wscrossweigh.wscrossweigh_weights_calculator import WSCrossWeighWeightsCalculator\n[test/build]   | knodle/trainer/__init__.py:1: in <module>\n[test/build]   |     from knodle.trainer.config import TrainerConfig\n[test/build]   | knodle/trainer/config.py:6: in <module>\n[test/build]   |     from snorkel.classification import cross_entropy_with_probs\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/__init__.py:7: in <module>\n[test/build]   |     from .training.loggers import (  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/training/loggers/__init__.py:4: in <module>\n[test/build]   |     from .tensorboard_writer import TensorBoardWriter  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/training/loggers/tensorboard_writer.py:3: in <module>\n[test/build]   |     from torch.utils.tensorboard import SummaryWriter\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:12: in <module>\n[test/build]   |     from .writer import FileWriter, SummaryWriter  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/utils/tensorboard/writer.py:9: in <module>\n[test/build]   |     from tensorboard.compat.proto.event_pb2 import SessionLog\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/event_pb2.py:17: in <module>\n[test/build]   |     from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/summary_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/tensor_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/resource_handle_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py:42: in <module>\n[test/build]   |     serialized_options=None, file=DESCRIPTOR),\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/google/protobuf/descriptor.py:561: in __new__\n[test/build]   |     _message.Message._CheckCalledFromGeneratedFile()\n[test/build]   | E   TypeError: Descriptors cannot not be created directly.\n[test/build]   | E   If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n[test/build]   | E   If you cannot immediately regenerate your protos, some other possible workarounds are:\n[test/build]   | E    1. Downgrade the protobuf package to 3.20.x or lower.\n[test/build]   | E    2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n[test/build]   | E   \n[test/build]   | E   More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n[test/build]   | __ ERROR collecting tests/trainer/wscrossweigh/test_wscw_data_preparation.py ___\n[test/build]   | tests/trainer/wscrossweigh/test_wscw_data_preparation.py:6: in <module>\n[test/build]   |     from knodle.trainer.wscrossweigh.data_splitting_by_rules import get_rules_sample_ids, get_samples_labels_idx_by_rule_id\n[test/build]   | knodle/trainer/__init__.py:1: in <module>\n[test/build]   |     from knodle.trainer.config import TrainerConfig\n[test/build]   | knodle/trainer/config.py:6: in <module>\n[test/build]   |     from snorkel.classification import cross_entropy_with_probs\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/__init__.py:7: in <module>\n[test/build]   |     from .training.loggers import (  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/training/loggers/__init__.py:4: in <module>\n[test/build]   |     from .tensorboard_writer import TensorBoardWriter  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/classification/training/loggers/tensorboard_writer.py:3: in <module>\n[test/build]   |     from torch.utils.tensorboard import SummaryWriter\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:12: in <module>\n[test/build]   |     from .writer import FileWriter, SummaryWriter  # noqa: F401\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/utils/tensorboard/writer.py:9: in <module>\n[test/build]   |     from tensorboard.compat.proto.event_pb2 import SessionLog\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/event_pb2.py:17: in <module>\n[test/build]   |     from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/summary_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import tensor_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/tensor_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import resource_handle_pb2 as tensorboard_dot_compat_dot_proto_dot_resource__handle__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/resource_handle_pb2.py:16: in <module>\n[test/build]   |     from tensorboard.compat.proto import tensor_shape_pb2 as tensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py:42: in <module>\n[test/build]   |     serialized_options=None, file=DESCRIPTOR),\n[test/build]   | /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/google/protobuf/descriptor.py:561: in __new__\n[test/build]   |     _message.Message._CheckCalledFromGeneratedFile()\n[test/build]   | E   TypeError: Descriptors cannot not be created directly.\n[test/build]   | E   If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n[test/build]   | E   If you cannot immediately regenerate your protos, some other possible workarounds are:\n[test/build]   | E    1. Downgrade the protobuf package to 3.20.x or lower.\n[test/build]   | E    2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n[test/build]   | E   \n[test/build]   | E   More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n[test/build]   | =============================== warnings summary ===============================\n[test/build]   | ../../../opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/types/hashing.py:1\n[test/build]   |   /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/snorkel/types/hashing.py:1: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n[test/build]   |     from collections import Hashable\n[test/build]   | \n[test/build]   | -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n[test/build]   | =========================== short test summary info ============================\n[test/build]   | ERROR tests/trainer/test_auto_trainer.py - TypeError: Descriptors cannot not be created directly.\n[test/build]   | If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n[test/build]   | If you cannot immediately regenerate your protos, some other possible workarounds are:\n[test/build]   |  1. Downgrade the protobuf package to 3.20.x or lower.\n[test/build]   |  2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n[test/build]   | \n[test/build]   | More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n[test/build]   | ERROR tests/trainer/test_multi_label.py - TypeError: Descriptors cannot not be created directly.\n[test/build]   | If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n[test/build]   | If you cannot immediately regenerate your protos, some other possible workarounds are:\n[test/build]   |  1. Downgrade the protobuf package to 3.20.x or lower.\n[test/build]   |  2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n[test/build]   | \n[test/build]   | More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n[test/build]   | ERROR tests/trainer/test_multi_trainer.py - TypeError: Descriptors cannot not be created directly.\n[test/build]   | If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n[test/build]   | If you cannot immediately regenerate your protos, some other possible workarounds are:\n[test/build]   |  1. Downgrade the protobuf package to 3.20.x or lower.\n[test/build]   |  2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n[test/build]   | \n[test/build]   | More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n[test/build]   | ERROR tests/trainer/cleanlab/test_cl.py - TypeError: Descriptors cannot not be created directly.\n[test/build]   | If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n[test/build]   | If you cannot immediately regenerate your protos, some other possible workarounds are:\n[test/build]   |  1. Downgrade the protobuf package to 3.20.x or lower.\n[test/build]   |  2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n[test/build]   | \n[test/build]   | More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n[test/build]   | ERROR tests/trainer/snorkel/test_utils.py - TypeError: Descriptors cannot not be created directly.\n[test/build]   | If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n[test/build]   | If you cannot immediately regenerate your protos, some other possible workarounds are:\n[test/build]   |  1. Downgrade the protobuf package to 3.20.x or lower.\n[test/build]   |  2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n[test/build]   | \n[test/build]   | More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n[test/build]   | ERROR tests/trainer/utils/test_denoise.py - TypeError: Descriptors cannot not be created directly.\n[test/build]   | If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n[test/build]   | If you cannot immediately regenerate your protos, some other possible workarounds are:\n[test/build]   |  1. Downgrade the protobuf package to 3.20.x or lower.\n[test/build]   |  2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n[test/build]   | \n[test/build]   | More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n[test/build]   | ERROR tests/trainer/wscrossweigh/test_wscw.py - TypeError: Descriptors cannot not be created directly.\n[test/build]   | If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n[test/build]   | If you cannot immediately regenerate your protos, some other possible workarounds are:\n[test/build]   |  1. Downgrade the protobuf package to 3.20.x or lower.\n[test/build]   |  2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n[test/build]   | \n[test/build]   | More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n[test/build]   | ERROR tests/trainer/wscrossweigh/test_wscw_data_preparation.py - TypeError: Descriptors cannot not be created directly.\n[test/build]   | If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n[test/build]   | If you cannot immediately regenerate your protos, some other possible workarounds are:\n[test/build]   |  1. Downgrade the protobuf package to 3.20.x or lower.\n[test/build]   |  2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n[test/build]   | \n[test/build]   | More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n[test/build]   | !!!!!!!!!!!!!!!!!!! Interrupted: 8 errors during collection !!!!!!!!!!!!!!!!!!!!\n[test/build]   | ========================= 1 warning, 8 errors in 3.84s =========================\n[test/build]   \u274c  Failure - Main Test with pytest\n[test/build] exitcode '2': failure\n[test/build] failed to remove container: Delete \"http://%2Fvar%2Frun%2Fdocker.sock/v1.41/containers/c2fc30dadd10f556be76b3c30169c4a11965909e0cce7db07e298406f65799eb?force=1&v=1\": context deadline exceeded\n[test/build] \ud83c\udfc1  Job failed\n",
    "actions_stderr": "Error: Error occurred running finally: Error occurred running finally: context deadline exceeded (original error: <nil>) (original error: <nil>)\n"
}