{
    "repository": "HiveRunner/HiveRunner",
    "stars": 247,
    "language": "java",
    "size": 1466,
    "clone_url": "https://github.com/HiveRunner/HiveRunner.git",
    "timestamp": "2023-06-29T11:01:17.368332Z",
    "clone_success": true,
    "number_of_actions": 3,
    "number_of_test_actions": 1,
    "actions_successful": false,
    "actions_build_tools": [
        "maven",
        "maven",
        "maven"
    ],
    "actions_test_build_tools": [
        "maven"
    ],
    "actions_run": {
        "failed": true,
        "tests": [
            {
                "classname": "com.klarna.hiverunner.HiveShellBeeLineEmulationTest",
                "name": "testQueryStripFullLineCommentNested",
                "time": 0.077,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveShellBeeLineEmulationTest",
                "name": "testScriptStripFullLineCommentNested",
                "time": 0.035,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveShellBeeLineEmulationTest",
                "name": "testScriptStripFullLineComment",
                "time": 0.017,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveShellBeeLineEmulationTest",
                "name": "testScriptStripFullLineCommentLastLine",
                "time": 0.021,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveShellBeeLineEmulationTest",
                "name": "testQueryStripFullLineCommentFirstLine",
                "time": 0.023,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveShellBeeLineEmulationTest",
                "name": "testScriptStripFullLineCommentFirstLine",
                "time": 0.022,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveShellBeeLineEmulationTest",
                "name": "testQueryStripFullLineComment",
                "time": 0.052,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest",
                "name": "testSelectFromCtas",
                "time": 13.551,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest",
                "name": "testSelectFromFooWithCustomDelimiter",
                "time": 6.417,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest",
                "name": "testTablesCreated",
                "time": 7.852,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest",
                "name": "testSelectFromFooWithTypeCheck",
                "time": 9.995,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.NoTimeoutTest",
                "name": "test",
                "time": 4.397,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.HelloHiveRunnerTest",
                "name": "testMaxValueByYear",
                "time": 5.222,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.CtasTest",
                "name": "verifyThatDataIsAvailableInCtas",
                "time": 0.177,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.CtasTest",
                "name": "tablesShouldBeCreated",
                "time": 0.064,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.CtasTest",
                "name": "testCountCtas",
                "time": 1.702,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.ExecuteScriptIntegrationTest",
                "name": "testInsertRowWithExecuteScript",
                "time": 13.424,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.InsertTestDataTest",
                "name": "insertRowsFromCode",
                "time": 1.483,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.InsertTestDataTest",
                "name": "insertRowsFromTsvFile",
                "time": 0.618,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.InsertTestDataTest",
                "name": "insertRowsFromCodeWithSelectedColumns",
                "time": 0.805,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.InsertTestDataTest",
                "name": "insertRowsIntoPartitionedTableStoredAsSequencefileWithCustomDelimiterAndNullValue",
                "time": 1.277,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.InsertTestDataTest",
                "name": "insertRowsFromTsvFileWithSubsetHeader",
                "time": 0.707,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.InsertTestDataTest",
                "name": "insertRowsFromTsvFileWithHeader",
                "time": 0.71,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveShellHiveCliEmulationTest",
                "name": "testQueryStripFullLineCommentNested",
                "time": 0.083,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveShellHiveCliEmulationTest",
                "name": "testScriptStripFullLineCommentNested",
                "time": 0.038,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveShellHiveCliEmulationTest",
                "name": "testScriptStripFullLineComment",
                "time": 0.022,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveShellHiveCliEmulationTest",
                "name": "testScriptStripFullLineCommentLastLine",
                "time": 0.044,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveShellHiveCliEmulationTest",
                "name": "testQueryStripFullLineCommentFirstLine",
                "time": 0.016,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveShellHiveCliEmulationTest",
                "name": "testScriptStripFullLineCommentFirstLine",
                "time": 0.02,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveShellHiveCliEmulationTest",
                "name": "testQueryStripFullLineComment",
                "time": 0.096,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveCliSourceTest",
                "name": "testNestedImport",
                "time": 14.015,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.junit4.HelloHiveRunnerTest",
                "name": "testMaxValueByYear",
                "time": 14.157,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveRunnerAnnotationsTest",
                "name": "testHiveSQLLoaded",
                "time": 9.668,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveRunnerAnnotationsTest",
                "name": "testLoadPathResources",
                "time": 3.872,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveRunnerAnnotationsTest",
                "name": "testPropertiesLoaded",
                "time": 2.643,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveRunnerAnnotationsTest",
                "name": "testLoadFileResources",
                "time": 2.545,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveRunnerAnnotationsTest",
                "name": "testSetupScriptFromFile",
                "time": 2.413,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveRunnerAnnotationsTest",
                "name": "testSetupScriptFromPath",
                "time": 3.123,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveRunnerAnnotationsTest",
                "name": "testLoadStringResources",
                "time": 2.883,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.HiveRunnerAnnotationsTest",
                "name": "testSetupScript",
                "time": 2.676,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.BigResultSetTest",
                "name": "bigResultSetTest",
                "time": 10.716,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.AnnotatedFieldsInSuperClassTest",
                "name": "testShellInitializedInAbstractTestClass",
                "time": 1.492,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.BeelineRunTest",
                "name": "testNestedImport",
                "time": 17.383,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.junit4.SetHiveConfValuesTest",
                "name": "useHiveConfValues",
                "time": 13.965,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.junit4.InsertTestDataTest",
                "name": "insertRowsFromCode",
                "time": 13.148,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.junit4.InsertTestDataTest",
                "name": "insertRowsFromTsvFile",
                "time": 4.025,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.junit4.InsertTestDataTest",
                "name": "insertRowsFromCodeWithSelectedColumns",
                "time": 3.343,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.junit4.InsertTestDataTest",
                "name": "insertRowsIntoPartitionedTableStoredAsSequencefileWithCustomDelimiterAndNullValue",
                "time": 3.638,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.junit4.InsertTestDataTest",
                "name": "insertRowsFromTsvFileWithSubsetHeader",
                "time": 3.416,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.junit4.InsertTestDataTest",
                "name": "insertRowsFromTsvFileWithHeader",
                "time": 3.022,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.SerdeTest",
                "name": "testWithCustomSerde",
                "time": 12.16,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.SerdeTest",
                "name": "testWithProvidedRegexSerde",
                "time": 3.025,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.SetHiveConfValuesTest",
                "name": "useHiveConfValues",
                "time": 10.576,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.HelloAnnotatedHiveRunnerTest",
                "name": "testSelectFromCtas",
                "time": 0.263,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.HelloAnnotatedHiveRunnerTest",
                "name": "testSelectFromFooWithCustomDelimiter",
                "time": 0.099,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.HelloAnnotatedHiveRunnerTest",
                "name": "testTablesCreated",
                "time": 0.068,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.HelloAnnotatedHiveRunnerTest",
                "name": "testSelectFromFooWithTypeCheck",
                "time": 1.552,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.HelloHiveRunnerParamaterizedTest",
                "name": "testFileFormats{String}[1]",
                "time": 3.917,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.HelloHiveRunnerParamaterizedTest",
                "name": "testFileFormats{String}[2]",
                "time": 2.329,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.examples.HelloHiveRunnerParamaterizedTest",
                "name": "testFileFormats{String}[3]",
                "time": 2.855,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.InteractiveHiveShellTest",
                "name": "setupScriptShouldBeExecuted",
                "time": 11.407,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.InteractiveHiveShellTest",
                "name": "setupScriptsShouldBeExecutedInOrder",
                "time": 3.656,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            },
            {
                "classname": "com.klarna.hiverunner.InteractiveHiveShellTest",
                "name": "setupScriptsShouldBeExecuted",
                "time": 2.993,
                "results": [
                    {
                        "result": "Passed",
                        "message": "",
                        "type": ""
                    }
                ],
                "stdout": null,
                "stderr": null
            }
        ],
        "stdout": "[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests] \ud83d\ude80  Start image=crawlergpt:latest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \ud83d\udc33  docker pull image=crawlergpt:latest platform= username= forcePull=false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \ud83d\udc33  docker create image=crawlergpt:latest platform= entrypoint=[\"tail\" \"-f\" \"/dev/null\"] cmd=[]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \ud83d\udc33  docker run image=crawlergpt:latest platform= entrypoint=[\"tail\" \"-f\" \"/dev/null\"] cmd=[]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \ud83d\udc33  docker exec cmd=[chown -R 1012:1013 /tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner] user=0 workdir=\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \u2601  git clone 'https://github.com/actions/setup-java' # ref=v2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests] \u2b50 Run Main actions/checkout@v2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \u2705  Success - Main actions/checkout@v2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests] \u2b50 Run Main Set up JDK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \ud83d\udc33  docker cp src=/tmp/act-cache/5a9c5891-56c5-476d-9066-b88d4f09028a/act/actions-setup-java@v2/ dst=/var/run/act/actions/actions-setup-java@v2/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \ud83d\udc33  docker exec cmd=[chown -R 1012:1013 /var/run/act/actions/actions-setup-java@v2/] user=0 workdir=\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \ud83d\udc33  docker exec cmd=[node /var/run/act/actions/actions-setup-java@v2/dist/setup/index.js] user= workdir=\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \ud83d\udcac  ::debug::isExplicit: 11.0.11-9\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \ud83d\udcac  ::debug::explicit? true\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \ud83d\udcac  ::debug::isExplicit: 8.0.292-1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \ud83d\udcac  ::debug::explicit? true\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Resolved Java 8.0.292+1 from tool-cache\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Setting Java 8.0.292+1 as the default\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Java configuration:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   Distribution: adopt\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   Version: 8.0.292+1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   Path: /opt/hostedtoolcache/Java_Adopt_jdk/8.0.292-1/x64\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \u2753 add-matcher /run/act/actions/actions-setup-java@v2/.github/java.json\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Creating settings.xml with server-id: github\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Writing to /home/runneradmin/.m2/settings.xml\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \u2705  Success - Main Set up JDK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \u2699  ::set-env:: JAVA_HOME=/opt/hostedtoolcache/Java_Adopt_jdk/8.0.292-1/x64\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \u2699  ::set-output:: distribution=Adopt-Hotspot\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \u2699  ::set-output:: path=/opt/hostedtoolcache/Java_Adopt_jdk/8.0.292-1/x64\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \u2699  ::set-output:: version=8.0.292+1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \u2699  ::add-path:: /opt/hostedtoolcache/Java_Adopt_jdk/8.0.292-1/x64/bin\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests] \u2b50 Run Main Run Maven Targets\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \ud83d\udc33  docker exec cmd=[bash --noprofile --norc -e -o pipefail /var/run/act/workflow/2] user= workdir=\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Apache Maven 3.8.2 (ea98e05a04480131370aa0c110b8c54cf726c06f)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Maven home: /usr/share/apache-maven-3.8.2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Java version: 1.8.0_292, vendor: AdoptOpenJDK, runtime: /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Default locale: en, platform encoding: UTF-8\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OS name: \"linux\", version: \"5.4.0-146-generic\", arch: \"amd64\", family: \"unix\"\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Scanning for projects...\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Inspecting build with total of 1 modules...\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Installing Nexus Staging features:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO]   ... total of 1 executions of maven-deploy-plugin replaced with nexus-staging-maven-plugin\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] ------------------< io.github.hiverunner:hiverunner >-------------------\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Building HiveRunner 6.1.1-SNAPSHOT\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] --------------------------------[ jar ]---------------------------------\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] --- license-maven-plugin:3.0:format (default) @ hiverunner ---\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Updating license headers...\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hiverunner ---\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Using 'UTF-8' encoding to copy filtered resources.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] skip non existing resourceDirectory /tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/src/main/resources\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] --- maven-compiler-plugin:3.7.0:compile (default-compile) @ hiverunner ---\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Changes detected - recompiling the module!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Compiling 58 source files to /tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/classes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] /tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/src/main/java/com/klarna/hiverunner/data/Converters.java: Some input files use or override a deprecated API.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] /tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/src/main/java/com/klarna/hiverunner/data/Converters.java: Recompile with -Xlint:deprecation for details.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] /tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/src/main/java/com/klarna/hiverunner/HiveRunnerCore.java: Some input files use unchecked or unsafe operations.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] /tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/src/main/java/com/klarna/hiverunner/HiveRunnerCore.java: Recompile with -Xlint:unchecked for details.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hiverunner ---\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Using 'UTF-8' encoding to copy filtered resources.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Copying 32 resources\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] --- maven-compiler-plugin:3.7.0:testCompile (default-testCompile) @ hiverunner ---\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Changes detected - recompiling the module!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Compiling 79 source files to /tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/test-classes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] /tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/src/test/java/com/klarna/hiverunner/SlowlyFailingUdf.java: Some input files use or override a deprecated API.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] /tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/src/test/java/com/klarna/hiverunner/SlowlyFailingUdf.java: Recompile with -Xlint:deprecation for details.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] /tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/src/test/java/com/klarna/hiverunner/data/InsertIntoTableTest.java: Some input files use unchecked or unsafe operations.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] /tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/src/test/java/com/klarna/hiverunner/data/InsertIntoTableTest.java: Recompile with -Xlint:unchecked for details.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] --- maven-surefire-plugin:2.22.2:test (default-test) @ hiverunner ---\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] -------------------------------------------------------\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO]  T E S T S\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] -------------------------------------------------------\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.ExecuteScriptIntegrationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:42,755 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:42,781 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.ExecuteScriptIntegrationTest in /\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 3800a776-bd41-4300-8395-73f227d7fab8\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:43,899 INFO  SessionState:1227 - Hive Session ID = 3800a776-bd41-4300-8395-73f227d7fab8\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:44,255 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:44,943 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:46,168 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:47,306 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:50,051 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:50,051 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:50,234 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 687e3f36-2c4c-4e01-bb0e-53fae9af5cdf\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:50,958 INFO  SessionState:1227 - Hive Session ID = 687e3f36-2c4c-4e01-bb0e-53fae9af5cdf\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:52,338 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.test_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:53,170 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110352_e1026c46-2258-42c6-9db3-616c77e164e9\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:53,420 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:53,435 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:53,449 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:53,449 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:53,477 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:53,639 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:53,930 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:53,963 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,018 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,111 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local2057462538_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,117 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,470 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,474 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,479 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,492 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,500 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local2057462538_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,603 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,616 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_tests6921406654968216613/localscratchdir6404906122639011363/29b89aed-d660-4cd0-8c6e-5de7b506b52a/hive_2023-06-29_11-03-52_650_3773735371072534188-1/dummy_path/dummy_file:0+1InputFormatClass: org.apache.hadoop.hive.ql.io.NullRowsInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,691 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,691 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,692 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,692 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,711 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,713 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,731 INFO  org.apache.orc.impl.MemoryManagerImpl:85 - orc.rows.between.memory.checks=5000\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,886 INFO  org.apache.orc.impl.PhysicalFsWriter:92 - ORC writer created for path: file:/tmp/hiverunner_tests6921406654968216613/warehouse6495775495678510588/test_db.db/test_table/.hive-staging_hive_2023-06-29_11-03-52_650_3773735371072534188-1/_task_tmp.-ext-10002/_tmp.000000_0 with stripeSize: 67108864 blockSize: 268435456 compression: ZLIB bufferSize: 262144\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,904 INFO  org.apache.orc.impl.OrcCodecPool:56 - Got brand-new codec ZLIB\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,938 INFO  org.apache.orc.impl.WriterImpl:188 - ORC writer created for path: file:/tmp/hiverunner_tests6921406654968216613/warehouse6495775495678510588/test_db.db/test_table/.hive-staging_hive_2023-06-29_11-03-52_650_3773735371072534188-1/_task_tmp.-ext-10002/_tmp.000000_0 with stripeSize: 67108864 blockSize: 268435456 compression: ZLIB bufferSize: 262144\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,980 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,987 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local2057462538_0001_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,993 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - map\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,994 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local2057462538_0001_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,997 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local2057462538_0001_m_000000_0: Counters: 25\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=40624366\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=41754640\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=353\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=95\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2001207296\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_1_test_db.test_table=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_3=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_1=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_UDTF_2=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,997 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local2057462538_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:54,999 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:03:55,494 Stage-1 map = 100%,  reduce = 0%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local2057462538_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_tests6921406654968216613/warehouse6495775495678510588/test_db.db/test_table/.hive-staging_hive_2023-06-29_11-03-52_650_3773735371072534188-1/-ext-10000\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Loading data to table test_db.test_table\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:55,524 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:55,738 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:55,865 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:55,878 ERROR org.apache.hadoop.hive.ql.io.AcidUtils:1003 - Failed to get files with ID; using regular API: Only supported for DFS; got class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:55,997 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.ExecuteScriptIntegrationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:56,049 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.439 s - in com.klarna.hiverunner.ExecuteScriptIntegrationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.HiveShellBeeLineEmulationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:58,531 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 5df25e32-8e08-4648-9ca0-22621e559765\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:59,209 INFO  SessionState:1227 - Hive Session ID = 5df25e32-8e08-4648-9ca0-22621e559765\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:03:59,631 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:00,676 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:02,181 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:03,264 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:06,200 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:06,200 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:06,331 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = c24bdd6f-a2ce-4b0f-8ca7-9984fd06fd19\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:06,733 INFO  SessionState:1227 - Hive Session ID = c24bdd6f-a2ce-4b0f-8ca7-9984fd06fd19\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:08,188 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellBeeLineEmulationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:08,233 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 1055ef1c-dda3-4887-9759-bd0b70a09e52\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:08,288 INFO  SessionState:1227 - Hive Session ID = 1055ef1c-dda3-4887-9759-bd0b70a09e52\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:08,315 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:08,319 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:08,960 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:11,149 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 7e8db9e0-0c8b-426a-bfb9-00b4416dfa6c\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:11,299 INFO  SessionState:1227 - Hive Session ID = 7e8db9e0-0c8b-426a-bfb9-00b4416dfa6c\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:11,387 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellBeeLineEmulationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:11,412 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 9b17d3cf-ce1b-40b2-bc5a-dd5a08fe60f6\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:11,447 INFO  SessionState:1227 - Hive Session ID = 9b17d3cf-ce1b-40b2-bc5a-dd5a08fe60f6\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:11,469 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:11,472 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:12,026 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:13,903 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 3350741f-1a21-4b98-8a47-d423f7dd90ae\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:14,033 INFO  SessionState:1227 - Hive Session ID = 3350741f-1a21-4b98-8a47-d423f7dd90ae\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:14,119 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellBeeLineEmulationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:14,133 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = e8e25312-14ce-4f8d-8a32-f7728ab46989\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:14,170 INFO  SessionState:1227 - Hive Session ID = e8e25312-14ce-4f8d-8a32-f7728ab46989\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:14,188 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:14,190 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:14,617 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:16,342 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 048df589-b72e-418b-8c9e-898780bbebb0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:16,449 INFO  SessionState:1227 - Hive Session ID = 048df589-b72e-418b-8c9e-898780bbebb0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:16,512 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellBeeLineEmulationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:16,527 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 1cb99d0e-59cb-47f1-8082-f34cc108c531\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:16,561 INFO  SessionState:1227 - Hive Session ID = 1cb99d0e-59cb-47f1-8082-f34cc108c531\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:16,580 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:16,582 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:16,959 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:18,618 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 79d66dfc-cfa7-43a5-84a5-b7eb35c63795\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:18,716 INFO  SessionState:1227 - Hive Session ID = 79d66dfc-cfa7-43a5-84a5-b7eb35c63795\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:18,778 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellBeeLineEmulationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:18,791 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 0285288c-6ca8-4541-bd85-732e80e49ee9\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:18,827 INFO  SessionState:1227 - Hive Session ID = 0285288c-6ca8-4541-bd85-732e80e49ee9\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:18,850 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:18,852 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:19,352 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:20,971 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 2189bc76-1a2c-4b84-bb79-fbd7a1f01b08\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:21,080 INFO  SessionState:1227 - Hive Session ID = 2189bc76-1a2c-4b84-bb79-fbd7a1f01b08\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:21,146 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellBeeLineEmulationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:21,161 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = f3dbf5ab-79bc-4536-ad07-07099fa665d1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:21,201 INFO  SessionState:1227 - Hive Session ID = f3dbf5ab-79bc-4536-ad07-07099fa665d1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:21,227 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:21,229 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:21,593 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:23,454 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 015dccd9-542c-4921-8597-391966ecce2f\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:23,583 INFO  SessionState:1227 - Hive Session ID = 015dccd9-542c-4921-8597-391966ecce2f\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | NoViableAltException(-1@[])\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1387)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:220)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:74)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:616)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:197)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:260)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:247)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:541)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:510)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:267)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.HiveServerContainer.executeStatement(HiveServerContainer.java:127)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatementsWithCommandShellEmulation(HiveShellBase.java:116)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatementWithCommandShellEmulation(HiveShellBase.java:110)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatement(HiveShellBase.java:100)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:89)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:82)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.HiveShellBeeLineEmulationTest.lambda$testQueryStripFullLineComment$0(HiveShellBeeLineEmulationTest.java:63)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:55)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:37)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3007)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.HiveShellBeeLineEmulationTest.testQueryStripFullLineComment(HiveShellBeeLineEmulationTest.java:63)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat java.lang.reflect.Method.invoke(Method.java:498)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat java.util.ArrayList.forEach(ArrayList.java:1259)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat java.util.ArrayList.forEach(ArrayList.java:1259)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | FAILED: ParseException line 1:0 cannot recognize input near '<EOF>' '<EOF>' '<EOF>'\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:23,686 ERROR org.apache.hadoop.hive.ql.Driver:1250 - FAILED: ParseException line 1:0 cannot recognize input near '<EOF>' '<EOF>' '<EOF>'\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | org.apache.hadoop.hive.ql.parse.ParseException: line 1:0 cannot recognize input near '<EOF>' '<EOF>' '<EOF>'\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:223)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:74)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:616)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:197)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:260)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:247)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:541)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:510)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:267)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.HiveServerContainer.executeStatement(HiveServerContainer.java:127)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatementsWithCommandShellEmulation(HiveShellBase.java:116)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatementWithCommandShellEmulation(HiveShellBase.java:110)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatement(HiveShellBase.java:100)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:89)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:82)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.HiveShellBeeLineEmulationTest.lambda$testQueryStripFullLineComment$0(HiveShellBeeLineEmulationTest.java:63)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:55)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:37)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3007)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.HiveShellBeeLineEmulationTest.testQueryStripFullLineComment(HiveShellBeeLineEmulationTest.java:63)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat java.lang.reflect.Method.invoke(Method.java:498)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat java.util.ArrayList.forEach(ArrayList.java:1259)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat java.util.ArrayList.forEach(ArrayList.java:1259)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:23,694 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellBeeLineEmulationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:23,703 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 25.769 s - in com.klarna.hiverunner.HiveShellBeeLineEmulationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.examples.HelloHiveRunnerParamaterizedTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:25,971 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 89ac654e-1ff9-4d1f-8cb5-f4497481b261\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:26,436 INFO  SessionState:1227 - Hive Session ID = 89ac654e-1ff9-4d1f-8cb5-f4497481b261\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:27,090 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:28,105 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:29,369 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:33,756 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:33,757 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:33,942 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 828ef853-26da-4516-84f4-28141ebff018\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:34,617 INFO  SessionState:1227 - Hive Session ID = 828ef853-26da-4516-84f4-28141ebff018\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:36,411 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:36,785 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:36,801 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,113 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,222 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,283 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,320 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,321 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,321 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,331 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,332 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,346 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,347 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,359 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,359 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,366 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,366 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,376 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,377 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,378 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,379 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,437 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,438 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,440 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,440 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,443 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_2082262014' to file:/tmp/hiverunner_test5332976982188817878/warehouse2992902116081851297/source_db.db/test_table/_SCRATCH0.21293882233675987\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,449 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,450 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,452 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,453 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,496 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,539 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:37,911 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110437_a72f3601-5167-4d0d-994c-8444b1af5364\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks not specified. Estimated from input data size: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to limit the maximum number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to set a constant number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:38,187 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:38,226 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:38,240 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:38,241 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:38,267 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:38,370 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:38,618 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:38,655 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:38,696 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:38,861 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local1465229009_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:38,862 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,166 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,168 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,169 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,186 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,187 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local1465229009_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,224 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,226 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test5332976982188817878/warehouse2992902116081851297/source_db.db/test_table/part-m-2082262014:0+146InputFormatClass: org.apache.hadoop.mapred.SequenceFileInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,310 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,310 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,310 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,310 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,330 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,331 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,331 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,331 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,331 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,334 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,389 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,389 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,389 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,389 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 34; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,389 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,510 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,526 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1465229009_0001_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,528 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test5332976982188817878/warehouse2992902116081851297/source_db.db/test_table/part-m-2082262014:0+146\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,528 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1465229009_0001_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,531 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1465229009_0001_m_000000_0: Counters: 25\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=40624607\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=41757974\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output bytes=34\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output materialized bytes=44\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=277\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=1999634432\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_8=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_9=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_7=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,531 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local1465229009_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,532 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,537 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,537 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local1465229009_0001_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,549 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,552 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@326fdec8\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,555 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,577 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1399744128, maxSingleShuffleLimit=349936032, mergeThreshold=923831168, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,581 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local1465229009_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,616 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#1 about to shuffle output of map attempt_local1465229009_0001_m_000000_0 decomp: 40 len: 44 to MEMORY\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,619 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 40 bytes from map-output for attempt_local1465229009_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,620 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 40, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->40\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,621 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,625 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,625 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,749 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,750 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,757 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 40 bytes to disk to satisfy reduce memory limit\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,758 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 44 bytes from disk\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,759 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,759 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,760 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,761 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,764 INFO  ExecReducer:99 - conf classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter805240053742278015.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,765 INFO  ExecReducer:101 - thread classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter805240053742278015.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,772 INFO  ExecReducer:147 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <GBY>Id =4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <FS>Id =6\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Parent>Id = 4 null<\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <\\FS>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Parent><\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <\\GBY>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,778 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,797 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1465229009_0001_r_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,799 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,799 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1465229009_0001_r_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,800 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1465229009_0001_r_000000_0: Counters: 29\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=40624727\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=41758155\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input groups=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce shuffle bytes=44\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tShuffled Maps =1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=1999634432\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_0=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_4=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tShuffle Errors\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBAD_ID=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCONNECTION=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tIO_ERROR=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_MAP=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,800 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local1465229009_0001_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:39,800 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:04:40,193 Stage-1 map = 100%,  reduce = 100%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local1465229009_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:40,214 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:40,221 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:40,240 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.HelloHiveRunnerParamaterizedTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:40,283 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 6423107e-56fe-4029-94c1-056a838a2125\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:40,331 INFO  SessionState:1227 - Hive Session ID = 6423107e-56fe-4029-94c1-056a838a2125\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:40,354 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:40,357 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:40,825 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:42,672 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = af3ad3e4-965e-4162-8d4d-15c2ced72c40\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:42,774 INFO  SessionState:1227 - Hive Session ID = af3ad3e4-965e-4162-8d4d-15c2ced72c40\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:42,849 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,046 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,170 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,260 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,273 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,273 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,276 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,276 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,291 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,292 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,295 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,296 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,300 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,300 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,341 INFO  org.apache.orc.impl.MemoryManagerImpl:85 - orc.rows.between.memory.checks=5000\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,384 INFO  org.apache.orc.impl.PhysicalFsWriter:92 - ORC writer created for path: file:/tmp/hiverunner_test5281521478747972993/warehouse834055433111218368/source_db.db/test_table/_SCRATCH0.914161165605004/_temporary/0/_temporary/attempt__0000_m_000000_1602768513/part-m-1602768513 with stripeSize: 67108864 blockSize: 268435456 compression: ZLIB bufferSize: 262144\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,412 INFO  org.apache.orc.impl.OrcCodecPool:56 - Got brand-new codec ZLIB\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,444 INFO  org.apache.orc.impl.WriterImpl:188 - ORC writer created for path: file:/tmp/hiverunner_test5281521478747972993/warehouse834055433111218368/source_db.db/test_table/_SCRATCH0.914161165605004/_temporary/0/_temporary/attempt__0000_m_000000_1602768513/part-m-1602768513 with stripeSize: 67108864 blockSize: 268435456 compression: ZLIB bufferSize: 262144\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,475 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,475 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,477 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,477 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,479 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_1602768513' to file:/tmp/hiverunner_test5281521478747972993/warehouse834055433111218368/source_db.db/test_table/_SCRATCH0.914161165605004\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,483 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,483 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,485 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,485 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,534 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,596 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,651 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110443_50009f63-73f1-49d2-a1a5-0cf84ba9cbe0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks not specified. Estimated from input data size: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to limit the maximum number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to set a constant number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,683 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,689 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,747 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,879 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,880 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,918 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,950 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local139759182_0002\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:43,951 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,120 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,121 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,122 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,126 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,126 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local139759182_0002_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,131 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,132 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test5281521478747972993/warehouse834055433111218368/source_db.db/test_table/part-m-1602768513:0+322InputFormatClass: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,193 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,201 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,202 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,202 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,202 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,202 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,204 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,209 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,209 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,209 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,209 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 34; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,209 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,254 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,264 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local139759182_0002_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,265 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test5281521478747972993/warehouse834055433111218368/source_db.db/test_table/part-m-1602768513:0+322\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,266 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local139759182_0002_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,266 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local139759182_0002_m_000000_0: Counters: 25\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=81250168\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=83512464\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output bytes=34\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output materialized bytes=44\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=275\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2086666240\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_8=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_9=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_7=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,266 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local139759182_0002_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,268 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,270 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,271 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local139759182_0002_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,275 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,276 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@18920eac\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,276 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,277 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1460666368, maxSingleShuffleLimit=365166592, mergeThreshold=964039872, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,278 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local139759182_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,281 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#2 about to shuffle output of map attempt_local139759182_0002_m_000000_0 decomp: 40 len: 44 to MEMORY\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,281 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 40 bytes from map-output for attempt_local139759182_0002_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,282 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 40, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->40\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,282 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,286 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,287 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,318 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,319 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,336 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 40 bytes to disk to satisfy reduce memory limit\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,337 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 44 bytes from disk\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,337 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,337 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,338 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,338 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,338 INFO  ExecReducer:99 - conf classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter805240053742278015.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,339 INFO  ExecReducer:101 - thread classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter805240053742278015.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,344 INFO  ExecReducer:147 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <GBY>Id =4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <FS>Id =6\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Parent>Id = 4 null<\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <\\FS>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Parent><\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <\\GBY>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,365 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local139759182_0002_r_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,367 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,367 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local139759182_0002_r_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,368 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local139759182_0002_r_000000_0: Counters: 29\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=81250288\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=83512645\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input groups=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce shuffle bytes=44\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tShuffled Maps =1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2086666240\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_0=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_4=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tShuffle Errors\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBAD_ID=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCONNECTION=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tIO_ERROR=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_MAP=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,368 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local139759182_0002_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:44,369 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:04:45,130 Stage-1 map = 100%,  reduce = 100%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local139759182_0002\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:45,145 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:45,147 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.HelloHiveRunnerParamaterizedTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:45,164 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:45,202 INFO  SessionState:1227 - Hive Session ID = 8ff38144-2c4f-42e0-94e8-213c743696ad\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 8ff38144-2c4f-42e0-94e8-213c743696ad\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:45,223 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:45,225 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:45,724 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:47,504 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 52ffb86d-9178-449b-8197-61ba9a5fad34\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:47,620 INFO  SessionState:1227 - Hive Session ID = 52ffb86d-9178-449b-8197-61ba9a5fad34\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:47,699 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:47,876 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:47,967 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,067 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,079 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,080 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,082 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,083 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,103 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,103 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,106 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,106 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,109 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,109 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,118 INFO  org.apache.parquet.hadoop.codec.CodecConfig:91 - Compression set to false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,119 INFO  org.apache.parquet.hadoop.codec.CodecConfig:95 - Compression: UNCOMPRESSED\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,128 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:376 - Parquet block size to 134217728\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,129 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:377 - Parquet page size to 1048576\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,129 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:378 - Parquet dictionary page size to 1048576\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,129 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:379 - Dictionary is on\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,130 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:380 - Validation is off\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,130 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:381 - Writer version is: PARQUET_1_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,130 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:382 - Maximum row group padding size is 8388608 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,130 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:383 - Page size checking is: estimated\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,131 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:384 - Min row count for page size check is: 100\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,131 INFO  org.apache.parquet.hadoop.ParquetOutputFormat:385 - Max row count for page size check is: 10000\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,587 INFO  org.apache.parquet.hadoop.InternalParquetRecordWriter:165 - Flushing mem columnStore to file. allocated memory: 64\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,851 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,852 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,854 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,854 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,857 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_1923629082' to file:/tmp/hiverunner_test5060777657219735576/warehouse3283914136936701446/source_db.db/test_table/_SCRATCH0.6625477646814335\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,861 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,862 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,865 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,865 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,921 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:48,975 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,038 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110448_0b9200a2-aacf-49b4-995b-0db20296707e\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks not specified. Estimated from input data size: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to limit the maximum number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to set a constant number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,067 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,074 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,084 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,238 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,239 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,268 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,301 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local481119395_0003\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,301 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,497 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,498 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,499 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,502 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,502 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local481119395_0003_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,506 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,508 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test5060777657219735576/warehouse3283914136936701446/source_db.db/test_table/part-m-1923629082:0+452InputFormatClass: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,556 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,567 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,567 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,567 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,567 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,568 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,570 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,616 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,616 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,616 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,617 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 34; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,617 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,746 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,755 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local481119395_0003_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,757 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - map\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,757 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local481119395_0003_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,757 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local481119395_0003_m_000000_0: Counters: 25\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=121876122\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=125267095\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output bytes=34\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output materialized bytes=44\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=290\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2086141952\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_8=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_9=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_7=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,758 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local481119395_0003_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,758 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,760 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,760 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local481119395_0003_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,764 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,764 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@524446b8\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,765 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,766 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1460299392, maxSingleShuffleLimit=365074848, mergeThreshold=963797632, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,769 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local481119395_0003_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,774 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#3 about to shuffle output of map attempt_local481119395_0003_m_000000_0 decomp: 40 len: 44 to MEMORY\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,775 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 40 bytes from map-output for attempt_local481119395_0003_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,775 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 40, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->40\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,776 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,776 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,777 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,907 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,907 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,913 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 40 bytes to disk to satisfy reduce memory limit\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,913 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 44 bytes from disk\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,914 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,914 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,914 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,915 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,915 INFO  ExecReducer:99 - conf classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter805240053742278015.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,915 INFO  ExecReducer:101 - thread classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter805240053742278015.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,922 INFO  ExecReducer:147 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <GBY>Id =4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <FS>Id =6\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Parent>Id = 4 null<\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <\\FS>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Parent><\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <\\GBY>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,947 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local481119395_0003_r_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,949 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,949 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local481119395_0003_r_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,950 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local481119395_0003_r_000000_0: Counters: 29\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=121876242\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=125267276\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input groups=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce shuffle bytes=44\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tShuffled Maps =1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2086141952\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_0=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_4=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tShuffle Errors\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBAD_ID=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCONNECTION=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tIO_ERROR=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_MAP=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,950 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local481119395_0003_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:49,951 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:04:50,503 Stage-1 map = 100%,  reduce = 100%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local481119395_0003\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:50,516 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:50,519 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.HelloHiveRunnerParamaterizedTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:50,542 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 25.152 s - in com.klarna.hiverunner.examples.HelloHiveRunnerParamaterizedTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.examples.HelloHiveRunnerTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:53,257 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = d50135f7-3359-47a1-96f2-fd4c700a6756\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:53,862 INFO  SessionState:1227 - Hive Session ID = d50135f7-3359-47a1-96f2-fd4c700a6756\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:54,318 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:55,177 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:04:56,749 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:00,865 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:00,866 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:00,994 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = c686e60c-6ab1-4c7a-92e9-4cd5e0b99f81\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:01,431 INFO  SessionState:1227 - Hive Session ID = c686e60c-6ab1-4c7a-92e9-4cd5e0b99f81\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:03,170 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:03,614 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.my_schema, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:03,855 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:03,882 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,246 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,419 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,486 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,522 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,523 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,523 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,535 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,536 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,577 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,578 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,589 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,589 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,594 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,595 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,600 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,600 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,601 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,601 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,633 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,634 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,640 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,641 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,649 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_876261421' to file:/tmp/hiverunner_test6928077389644176152/warehouse6171634869098696946/source_db.db/test_table/_SCRATCH0.8995404803248858\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,664 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,665 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,671 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,671 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,712 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:04,751 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:05,529 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110504_0efd2736-e7bc-45c6-9e59-94597b57fc4a\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks not specified. Estimated from input data size: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to limit the maximum number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to set a constant number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:05,927 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:05,947 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:05,964 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:05,965 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:05,989 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:06,061 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:06,366 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:06,386 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:06,435 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:06,587 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local475347142_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:06,592 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:06,920 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:06,921 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:06,923 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:06,937 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:06,938 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local475347142_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:06,983 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:06,992 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test6928077389644176152/warehouse6171634869098696946/source_db.db/test_table/part-m-876261421:0+28InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,057 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,058 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,058 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,058 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,079 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,080 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,080 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,080 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,080 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,084 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,163 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,164 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,164 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,164 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 34; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,164 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,354 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,375 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local475347142_0001_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,377 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test6928077389644176152/warehouse6171634869098696946/source_db.db/test_table/part-m-876261421:0+28\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,378 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local475347142_0001_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,382 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local475347142_0001_m_000000_0: Counters: 25\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=40624325\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=41758396\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output bytes=34\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output materialized bytes=44\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=268\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=1999110144\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_8=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_9=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_7=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,382 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local475347142_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,385 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,390 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,390 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local475347142_0001_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,404 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,407 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@629da73b\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,409 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,425 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1399377024, maxSingleShuffleLimit=349844256, mergeThreshold=923588864, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,429 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local475347142_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,473 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#1 about to shuffle output of map attempt_local475347142_0001_m_000000_0 decomp: 40 len: 44 to MEMORY\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,479 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 40 bytes from map-output for attempt_local475347142_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,482 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 40, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->40\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,485 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,486 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,487 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,652 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,652 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,658 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 40 bytes to disk to satisfy reduce memory limit\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,659 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 44 bytes from disk\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,661 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,662 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,663 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,664 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,668 INFO  ExecReducer:99 - conf classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter3062328728941834072.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,668 INFO  ExecReducer:101 - thread classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter3062328728941834072.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,684 INFO  ExecReducer:147 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <GBY>Id =4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <FS>Id =6\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Parent>Id = 4 null<\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <\\FS>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Parent><\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <\\GBY>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,693 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,737 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local475347142_0001_r_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,738 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,738 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local475347142_0001_r_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,739 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local475347142_0001_r_000000_0: Counters: 29\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=40624445\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=41758577\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input groups=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce shuffle bytes=44\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tShuffled Maps =1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=1999110144\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_1_my_schema.result=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_4=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tShuffle Errors\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBAD_ID=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCONNECTION=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tIO_ERROR=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_MAP=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,739 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local475347142_0001_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,740 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:05:07,946 Stage-1 map = 100%,  reduce = 100%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local475347142_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Loading data to table my_schema.result\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:07,975 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:08,177 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:08,252 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:08,263 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:08,289 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.HelloHiveRunnerTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:08,335 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.746 s - in com.klarna.hiverunner.examples.HelloHiveRunnerTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.examples.HelloAnnotatedHiveRunnerTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:10,772 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 61392d06-9c27-4d8b-b3ce-917a33f03799\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:11,295 INFO  SessionState:1227 - Hive Session ID = 61392d06-9c27-4d8b-b3ce-917a33f03799\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:11,693 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:12,504 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:14,154 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:15,292 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:18,612 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:18,613 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:18,786 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:19,472 INFO  SessionState:1227 - Hive Session ID = 4e368821-e90f-4e9e-866e-1a16b6e805c9\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 4e368821-e90f-4e9e-866e-1a16b6e805c9\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:20,916 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.bar, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:21,591 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110521_7fda0f46-5181-4caa-8754-56bac8fbc771\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:21,878 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:21,895 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:21,914 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:21,915 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:21,942 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:22,004 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:22,302 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:22,321 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:22,363 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:22,422 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local1539483959_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:22,422 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:22,646 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:22,650 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:22,654 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:22,667 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:22,672 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local1539483959_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:22,720 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:22,733 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test5232698839381258074/hadooptmp7983828868909262452/foo/data_from_string.csv:0+11,/tmp/hiverunner_test5232698839381258074/hadooptmp7983828868909262452/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:22,823 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:22,824 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:22,824 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:22,825 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:22,954 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:22,957 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:23,033 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:23,043 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1539483959_0001_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:23,046 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test5232698839381258074/hadooptmp7983828868909262452/foo/data_from_file.csv:0+12\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:23,046 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1539483959_0001_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:23,051 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1539483959_0001_m_000000_0: Counters: 24\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=40624401\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=41752082\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=365\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=64\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2001207296\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_1_bar.foo_prim=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:23,052 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local1539483959_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:23,054 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:05:23,668 Stage-1 map = 100%,  reduce = 0%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local1539483959_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test5232698839381258074/warehouse6160263173413453552/bar.db/.hive-staging_hive_2023-06-29_11-05-21_192_3706384189100566968-1/-ext-10001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test5232698839381258074/warehouse6160263173413453552/bar.db/foo_prim\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:23,725 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:23,849 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | WARNING: Order/Sort by without limit in sub query or view [a] is removed, as it's pointless and bad for performance.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:23,945 WARN  org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer:531 - Cannot determine basic stats for table: bar@foo_prim from metastore. Falling back.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:23,971 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:23,980 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:23,996 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.HelloAnnotatedHiveRunnerTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:24,063 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 5a230a33-110c-4403-9183-a84a98710f91\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:24,127 INFO  SessionState:1227 - Hive Session ID = 5a230a33-110c-4403-9183-a84a98710f91\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:24,154 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:24,156 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:24,615 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:26,868 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 9d05dfce-7fed-47a8-b6ad-024e9475cabe\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:27,012 INFO  SessionState:1227 - Hive Session ID = 9d05dfce-7fed-47a8-b6ad-024e9475cabe\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:27,145 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.bar, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:27,489 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110527_7d59e8d1-4204-468d-9d5c-8d04d80915cc\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:27,518 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:27,526 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:27,537 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:27,696 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:27,697 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:27,724 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:27,768 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local2077869552_0002\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:27,769 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:27,958 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:27,960 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:27,960 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:27,963 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:27,963 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local2077869552_0002_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:27,976 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:27,984 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test5017886836834364806/hadooptmp7081423271849239199/foo/data_from_string.csv:0+11,/tmp/hiverunner_test5017886836834364806/hadooptmp7081423271849239199/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:27,991 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:28,022 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:28,024 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local2077869552_0002_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:28,025 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test5017886836834364806/hadooptmp7081423271849239199/foo/data_from_file.csv:0+12\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:28,025 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local2077869552_0002_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:28,026 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local2077869552_0002_m_000000_0: Counters: 24\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=81248845\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=83504158\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=365\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2087190528\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_1_bar.foo_prim=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:28,026 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local2077869552_0002_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:28,026 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:05:28,964 Stage-1 map = 100%,  reduce = 0%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local2077869552_0002\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test5017886836834364806/warehouse3345230554533168277/bar.db/.hive-staging_hive_2023-06-29_11-05-27_323_784551362522509240-1/-ext-10001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test5017886836834364806/warehouse3345230554533168277/bar.db/foo_prim\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:28,973 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:29,017 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:29,087 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:29,090 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.HelloAnnotatedHiveRunnerTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:29,109 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 58a068ff-759e-4d25-835e-65bf87327103\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:29,150 INFO  SessionState:1227 - Hive Session ID = 58a068ff-759e-4d25-835e-65bf87327103\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:29,170 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:29,173 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:29,456 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:31,291 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 8742b4bd-b484-4fe8-8c55-fac5742c6874\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:31,429 INFO  SessionState:1227 - Hive Session ID = 8742b4bd-b484-4fe8-8c55-fac5742c6874\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:31,505 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.bar, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:31,818 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110531_4409acbc-9694-4d90-a865-377d4445fe35\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:31,842 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:31,850 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:31,860 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:31,978 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:31,982 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:32,008 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:32,038 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local1034989648_0003\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:32,039 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:32,320 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:32,320 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:32,321 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:32,323 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:32,324 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local1034989648_0003_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:32,328 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:32,329 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test5860881676964146793/hadooptmp6130133231759306292/foo/data_from_string.csv:0+11,/tmp/hiverunner_test5860881676964146793/hadooptmp6130133231759306292/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:32,335 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:32,355 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:32,357 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1034989648_0003_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:32,358 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test5860881676964146793/hadooptmp6130133231759306292/foo/data_from_file.csv:0+12\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:32,359 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1034989648_0003_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:32,359 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1034989648_0003_m_000000_0: Counters: 24\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=121873269\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=125256248\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=365\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2087714816\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_1_bar.foo_prim=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:32,360 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local1034989648_0003_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:32,360 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:05:33,324 Stage-1 map = 100%,  reduce = 0%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local1034989648_0003\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test5860881676964146793/warehouse5707087919082644449/bar.db/.hive-staging_hive_2023-06-29_11-05-31_651_5583546854028095976-1/-ext-10001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test5860881676964146793/warehouse5707087919082644449/bar.db/foo_prim\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:33,332 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:33,374 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:33,410 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:33,413 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.HelloAnnotatedHiveRunnerTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:33,433 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = da520782-b57e-4727-a30c-4487e0c9a897\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:33,478 INFO  SessionState:1227 - Hive Session ID = da520782-b57e-4727-a30c-4487e0c9a897\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:33,498 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:33,501 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:33,842 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:35,665 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = de9cf675-24c1-4e36-b52d-70f357f9af6f\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:35,767 INFO  SessionState:1227 - Hive Session ID = de9cf675-24c1-4e36-b52d-70f357f9af6f\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:35,831 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.bar, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,085 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110535_ea2ea820-e506-41bb-a669-3a18f3d18ba3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,109 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,116 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,126 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,268 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,269 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,304 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,350 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local1718659944_0004\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,358 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,592 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,593 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,593 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,596 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,596 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local1718659944_0004_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,601 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,602 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test7743544944011622797/hadooptmp4721207545583570344/foo/data_from_string.csv:0+11,/tmp/hiverunner_test7743544944011622797/hadooptmp4721207545583570344/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,609 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,630 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,631 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1718659944_0004_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,632 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test7743544944011622797/hadooptmp4721207545583570344/foo/data_from_file.csv:0+12\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,633 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1718659944_0004_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,633 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1718659944_0004_m_000000_0: Counters: 24\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=162497699\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=167008335\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=365\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2086666240\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_1_bar.foo_prim=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,633 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local1718659944_0004_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:36,634 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:05:37,598 Stage-1 map = 100%,  reduce = 0%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local1718659944_0004\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test7743544944011622797/warehouse317597711930296599/bar.db/.hive-staging_hive_2023-06-29_11-05-35_969_5086931559758704063-1/-ext-10001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test7743544944011622797/warehouse317597711930296599/bar.db/foo_prim\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:37,607 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:37,650 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:37,716 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110537_4d366ef6-da06-45c7-894f-45fe69acfd6f\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks determined at compile time: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to limit the maximum number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to set a constant number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:37,741 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:37,747 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:37,754 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:37,876 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:37,878 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:37,912 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:37,976 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local1258586783_0005\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:37,977 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,116 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,117 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,117 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,121 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,125 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local1258586783_0005_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,129 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,130 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test7743544944011622797/hadooptmp4721207545583570344/foo/data_from_string.csv:0+11,/tmp/hiverunner_test7743544944011622797/hadooptmp4721207545583570344/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,137 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,158 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,158 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,158 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,159 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,159 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,162 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,179 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,180 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,180 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,180 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 74; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,180 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214384(104857536); length = 13/6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,245 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,252 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1258586783_0005_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,254 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test7743544944011622797/hadooptmp4721207545583570344/foo/data_from_file.csv:0+12\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,254 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1258586783_0005_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,255 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1258586783_0005_m_000000_0: Counters: 24\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=203122100\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=208764614\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output bytes=74\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output materialized bytes=88\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=365\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2086666240\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_6=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,255 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local1258586783_0005_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,255 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,261 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,261 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local1258586783_0005_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,269 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,274 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@60469018\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,275 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,297 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1460666368, maxSingleShuffleLimit=365166592, mergeThreshold=964039872, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,302 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local1258586783_0005_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,336 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#1 about to shuffle output of map attempt_local1258586783_0005_m_000000_0 decomp: 84 len: 88 to MEMORY\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,340 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 84 bytes from map-output for attempt_local1258586783_0005_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,342 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 84, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->84\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,344 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,347 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,347 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,425 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,426 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 76 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,431 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 84 bytes to disk to satisfy reduce memory limit\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,431 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 88 bytes from disk\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,433 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,434 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,434 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 76 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,435 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,438 INFO  ExecReducer:99 - conf classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter9137418750208289226.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,438 INFO  ExecReducer:101 - thread classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter9137418750208289226.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,448 INFO  ExecReducer:147 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <SEL>Id =3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <FS>Id =4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Parent>Id = 3 null<\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <\\FS>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Parent><\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <\\SEL>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,481 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1258586783_0005_r_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,482 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,482 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1258586783_0005_r_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,482 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1258586783_0005_r_000000_0: Counters: 29\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=203122308\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=208764876\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input groups=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce shuffle bytes=88\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tShuffled Maps =1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2086666240\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_0=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_4=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_3=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tShuffle Errors\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBAD_ID=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCONNECTION=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tIO_ERROR=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_MAP=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,482 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local1258586783_0005_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:38,483 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:05:39,121 Stage-1 map = 100%,  reduce = 100%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local1258586783_0005\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:39,135 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:39,180 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.HelloAnnotatedHiveRunnerTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:39,194 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 28.927 s - in com.klarna.hiverunner.examples.HelloAnnotatedHiveRunnerTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.examples.junit4.HelloHiveRunnerTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:40,943 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:40,960 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.HelloHiveRunnerTest in /\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 7f9c9232-5b7a-409f-b552-b043c0bfb444\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:41,939 INFO  SessionState:1227 - Hive Session ID = 7f9c9232-5b7a-409f-b552-b043c0bfb444\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:42,380 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:43,243 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:44,632 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:48,816 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:48,816 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:48,977 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = b90604cb-fefe-488d-bef0-818ac593de65\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:49,465 INFO  SessionState:1227 - Hive Session ID = b90604cb-fefe-488d-bef0-818ac593de65\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:50,959 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,243 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.my_schema, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,382 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,401 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,690 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,782 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,850 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,885 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,885 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,886 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,899 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,899 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,914 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,914 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,927 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,928 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,935 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,936 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,942 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,943 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,943 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,944 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,974 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,974 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,977 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,977 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,979 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_915666614' to file:/tmp/hiverunner_tests8594055798754290418/warehouse5409900524870540540/source_db.db/test_table/_SCRATCH0.0969306190044803\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,985 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,986 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,988 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:51,988 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:52,032 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:52,072 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:52,493 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110552_f5ae808d-1d45-4104-aa67-0a6e15d53c83\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks not specified. Estimated from input data size: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to limit the maximum number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to set a constant number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:52,831 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:52,850 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:52,864 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:52,865 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:52,915 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:52,978 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,220 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,235 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,277 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,349 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local1675525365_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,350 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,715 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,716 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,719 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,730 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,731 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local1675525365_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,773 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,780 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_tests8594055798754290418/warehouse5409900524870540540/source_db.db/test_table/part-m-915666614:0+28InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,843 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,843 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,843 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,844 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,859 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,860 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,860 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,860 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,860 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,864 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,924 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,925 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,925 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,925 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 34; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:53,925 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,024 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,041 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1675525365_0001_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,042 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_tests8594055798754290418/warehouse5409900524870540540/source_db.db/test_table/part-m-915666614:0+28\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,043 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1675525365_0001_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,046 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1675525365_0001_m_000000_0: Counters: 25\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=40624326\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=41762359\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output bytes=34\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output materialized bytes=44\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=269\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2000158720\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_8=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_9=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_7=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,047 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local1675525365_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,048 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,053 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,054 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local1675525365_0001_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,066 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,071 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6dce1b03\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,073 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,090 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1400111104, maxSingleShuffleLimit=350027776, mergeThreshold=924073344, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,094 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local1675525365_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,132 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#1 about to shuffle output of map attempt_local1675525365_0001_m_000000_0 decomp: 40 len: 44 to MEMORY\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,134 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 40 bytes from map-output for attempt_local1675525365_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,137 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 40, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->40\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,138 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,140 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,141 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,164 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,165 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,170 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 40 bytes to disk to satisfy reduce memory limit\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,171 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 44 bytes from disk\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,172 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,173 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,173 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,174 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,177 INFO  ExecReducer:99 - conf classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter9123384787400782105.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,178 INFO  ExecReducer:101 - thread classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter9123384787400782105.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,188 INFO  ExecReducer:147 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <GBY>Id =4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <FS>Id =6\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Parent>Id = 4 null<\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <\\FS>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Parent><\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <\\GBY>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,193 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,230 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1675525365_0001_r_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,232 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,232 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1675525365_0001_r_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,233 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1675525365_0001_r_000000_0: Counters: 29\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=40624446\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=41762540\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input groups=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce shuffle bytes=44\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tShuffled Maps =1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2000158720\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_1_my_schema.result=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_4=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tShuffle Errors\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBAD_ID=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCONNECTION=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tIO_ERROR=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_MAP=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,233 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local1675525365_0001_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,234 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:05:54,739 Stage-1 map = 100%,  reduce = 100%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local1675525365_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Loading data to table my_schema.result\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,760 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,914 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,960 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,970 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:54,992 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.HelloHiveRunnerTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:55,036 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.167 s - in com.klarna.hiverunner.examples.junit4.HelloHiveRunnerTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:56,902 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:56,925 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest in /\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = d455215e-fd12-42c3-8917-eba17c5bbe1d\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:57,722 INFO  SessionState:1227 - Hive Session ID = d455215e-fd12-42c3-8917-eba17c5bbe1d\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:58,149 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:05:59,106 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:00,646 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:01,679 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:05,149 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:05,150 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:05,297 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = fb5ca944-2e03-46ac-9c79-01b88c9fc4a7\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:05,749 INFO  SessionState:1227 - Hive Session ID = fb5ca944-2e03-46ac-9c79-01b88c9fc4a7\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:07,224 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.bar, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:08,053 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110607_5d8506cd-931d-428d-b347-29add054be19\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:08,311 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:08,327 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:08,341 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:08,341 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:08,370 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:08,453 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:08,709 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:08,727 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:08,765 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:08,825 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local1214976456_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:08,826 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,016 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,019 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,020 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,035 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,039 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local1214976456_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,085 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,093 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_tests354137249580602099/hadooptmp8927450579751390239/foo/data_from_string.csv:0+11,/tmp/hiverunner_tests354137249580602099/hadooptmp8927450579751390239/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,160 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,161 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,161 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,161 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,198 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,201 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,323 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,333 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1214976456_0001_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,335 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_tests354137249580602099/hadooptmp8927450579751390239/foo/data_from_file.csv:0+12\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,335 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1214976456_0001_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,338 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1214976456_0001_m_000000_0: Counters: 24\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=40624401\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=41752072\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=365\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=55\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2001207296\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_1_bar.foo_prim=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,338 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local1214976456_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:09,340 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:06:10,035 Stage-1 map = 100%,  reduce = 0%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local1214976456_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_tests354137249580602099/warehouse736499016493899363/bar.db/.hive-staging_hive_2023-06-29_11-06-07_577_8880244683533165081-1/-ext-10001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_tests354137249580602099/warehouse736499016493899363/bar.db/foo_prim\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:10,061 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:10,123 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | WARNING: Order/Sort by without limit in sub query or view [a] is removed, as it's pointless and bad for performance.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:10,196 WARN  org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer:531 - Cannot determine basic stats for table: bar@foo_prim from metastore. Falling back.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:10,218 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:10,224 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:10,237 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:10,282 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:10,289 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest in /\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = fe014289-9986-4a01-a43d-99d8cf0d7524\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:10,361 INFO  SessionState:1227 - Hive Session ID = fe014289-9986-4a01-a43d-99d8cf0d7524\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:10,389 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:10,391 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:10,877 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:13,012 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 6f7adfe5-1fa6-4bcf-a464-f654df5ccfd3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:13,221 INFO  SessionState:1227 - Hive Session ID = 6f7adfe5-1fa6-4bcf-a464-f654df5ccfd3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:13,360 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.bar, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:14,486 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110613_3d51e132-fda4-47d3-bfa4-5cdca1792fa2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:14,562 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:14,573 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:14,580 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,190 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,191 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,229 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,262 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local16958038_0002\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,262 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,445 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,446 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,447 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,450 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,451 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local16958038_0002_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,457 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,458 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_tests4365337236820423735/hadooptmp6048711409769066705/foo/data_from_string.csv:0+11,/tmp/hiverunner_tests4365337236820423735/hadooptmp6048711409769066705/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,464 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,486 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,489 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local16958038_0002_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,490 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_tests4365337236820423735/hadooptmp6048711409769066705/foo/data_from_file.csv:0+12\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,490 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local16958038_0002_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,491 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local16958038_0002_m_000000_0: Counters: 24\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=81248847\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=83496420\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=367\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2087714816\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_1_bar.foo_prim=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,491 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local16958038_0002_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:15,491 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:06:16,456 Stage-1 map = 100%,  reduce = 0%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local16958038_0002\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_tests4365337236820423735/warehouse1381502060337322705/bar.db/.hive-staging_hive_2023-06-29_11-06-13_683_5521504377277982607-2/-ext-10001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_tests4365337236820423735/warehouse1381502060337322705/bar.db/foo_prim\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:16,473 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:16,550 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:16,651 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:16,656 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:16,700 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:16,709 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest in /\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 7ffb0079-8de0-4a85-9225-59ff95fdf41d\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:16,747 INFO  SessionState:1227 - Hive Session ID = 7ffb0079-8de0-4a85-9225-59ff95fdf41d\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:16,805 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:16,807 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:17,929 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:21,237 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = dff85d8b-6485-4976-9d07-1a41db43cf89\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:21,408 INFO  SessionState:1227 - Hive Session ID = dff85d8b-6485-4976-9d07-1a41db43cf89\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:21,753 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.bar, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:22,692 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110622_a4f045ae-61c9-4555-951b-c6fb323a3e88\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:22,718 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:22,724 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:22,741 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:22,944 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:22,946 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:23,061 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:23,113 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local1929787344_0003\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:23,113 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:23,252 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:23,252 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:23,253 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:23,260 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:23,261 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local1929787344_0003_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:23,264 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:23,265 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_tests1291983629377549493/hadooptmp1860457548420109263/foo/data_from_string.csv:0+11,/tmp/hiverunner_tests1291983629377549493/hadooptmp1860457548420109263/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:23,271 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:23,303 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:23,306 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1929787344_0003_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:23,308 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_tests1291983629377549493/hadooptmp1860457548420109263/foo/data_from_file.csv:0+12\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:23,309 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1929787344_0003_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:23,309 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1929787344_0003_m_000000_0: Counters: 24\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=121873273\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=125248560\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=367\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2088239104\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_1_bar.foo_prim=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:23,310 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local1929787344_0003_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:23,312 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:06:24,261 Stage-1 map = 100%,  reduce = 0%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local1929787344_0003\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_tests1291983629377549493/warehouse1570130435376011696/bar.db/.hive-staging_hive_2023-06-29_11-06-22_549_4940777337087823859-3/-ext-10001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_tests1291983629377549493/warehouse1570130435376011696/bar.db/foo_prim\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:24,307 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:24,497 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:24,535 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:24,536 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:24,553 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:24,561 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest in /\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 663fb9c2-8453-4a24-93fb-0ab34bfb13f6\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:24,593 INFO  SessionState:1227 - Hive Session ID = 663fb9c2-8453-4a24-93fb-0ab34bfb13f6\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:24,646 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:24,647 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:25,787 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:28,882 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 445f8da1-f6f3-4831-bc93-386ba2caef9d\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:29,042 INFO  SessionState:1227 - Hive Session ID = 445f8da1-f6f3-4831-bc93-386ba2caef9d\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:29,278 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.bar, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:29,588 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110629_c6454fd3-95eb-4123-97fd-6060b48df65e\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:29,660 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:29,682 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:29,710 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:29,944 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:29,945 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:30,029 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:30,102 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local147106560_0004\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:30,102 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:30,504 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:30,506 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:30,507 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:30,527 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:30,529 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local147106560_0004_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:30,535 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:30,536 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_tests4009676644069480347/hadooptmp7609161884424790562/foo/data_from_string.csv:0+11,/tmp/hiverunner_tests4009676644069480347/hadooptmp7609161884424790562/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:30,543 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:30,596 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:30,598 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local147106560_0004_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:30,609 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_tests4009676644069480347/hadooptmp7609161884424790562/foo/data_from_file.csv:0+12\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:30,609 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local147106560_0004_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:30,610 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local147106560_0004_m_000000_0: Counters: 24\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=162497705\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=166996845\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=367\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2088239104\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_1_bar.foo_prim=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:30,610 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local147106560_0004_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:30,610 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:06:31,530 Stage-1 map = 100%,  reduce = 0%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local147106560_0004\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_tests4009676644069480347/warehouse1857173901452454987/bar.db/.hive-staging_hive_2023-06-29_11-06-29_411_5819467752296217510-4/-ext-10001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_tests4009676644069480347/warehouse1857173901452454987/bar.db/foo_prim\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:31,553 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:31,651 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:31,714 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110631_9abeb3a7-0d32-44d4-ba9d-e5538d4a75de\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks determined at compile time: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to limit the maximum number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to set a constant number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:31,813 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:31,830 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:31,861 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,064 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,065 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,181 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,273 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local2085563161_0005\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,273 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,492 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,505 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,505 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,528 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,529 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local2085563161_0005_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,533 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,534 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_tests4009676644069480347/hadooptmp7609161884424790562/foo/data_from_string.csv:0+11,/tmp/hiverunner_tests4009676644069480347/hadooptmp7609161884424790562/foo/data_from_file.csv:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,543 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,570 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,570 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,571 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,571 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,571 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,581 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,606 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,606 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,606 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,607 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 74; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:32,607 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214384(104857536); length = 13/6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,125 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,142 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local2085563161_0005_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,145 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_tests4009676644069480347/hadooptmp7609161884424790562/foo/data_from_file.csv:0+12\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,146 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local2085563161_0005_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,146 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local2085563161_0005_m_000000_0: Counters: 24\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=203122108\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=208753212\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output bytes=74\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output materialized bytes=88\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=367\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2088239104\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_6=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,146 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local2085563161_0005_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,147 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,156 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,161 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local2085563161_0005_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,169 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,172 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2f1e0528\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,173 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,220 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1461767296, maxSingleShuffleLimit=365441824, mergeThreshold=964766464, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,259 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local2085563161_0005_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,393 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#1 about to shuffle output of map attempt_local2085563161_0005_m_000000_0 decomp: 84 len: 88 to MEMORY\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,417 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 84 bytes from map-output for attempt_local2085563161_0005_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,436 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 84, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->84\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,448 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,469 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,471 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:06:33,498 Stage-1 map = 100%,  reduce = 0%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,542 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,543 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 76 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,566 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 84 bytes to disk to satisfy reduce memory limit\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,567 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 88 bytes from disk\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,568 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,568 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,568 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 76 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,569 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,571 INFO  ExecReducer:99 - conf classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter2217438648988851992.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,571 INFO  ExecReducer:101 - thread classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter2217438648988851992.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,577 INFO  ExecReducer:147 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <SEL>Id =3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <FS>Id =4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Parent>Id = 3 null<\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <\\FS>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Parent><\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <\\SEL>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,671 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local2085563161_0005_r_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,682 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,683 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local2085563161_0005_r_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,683 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local2085563161_0005_r_000000_0: Counters: 29\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=203122316\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=208753474\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input groups=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce shuffle bytes=88\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tShuffled Maps =1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2088239104\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_0=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_4=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_3=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tShuffle Errors\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBAD_ID=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCONNECTION=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tIO_ERROR=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_MAP=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,684 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local2085563161_0005_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:33,692 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:06:34,500 Stage-1 map = 100%,  reduce = 100%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local2085563161_0005\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:34,521 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:34,533 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:34,549 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 37.826 s - in com.klarna.hiverunner.examples.junit4.HelloAnnotatedHiveRunnerTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.examples.junit4.SetHiveConfValuesTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:37,685 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:37,715 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.SetHiveConfValuesTest in /\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:39,848 INFO  SessionState:1227 - Hive Session ID = 72481225-2fea-4fcf-8c6e-198020ec3812\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 72481225-2fea-4fcf-8c6e-198020ec3812\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:40,706 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:42,206 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:43,766 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:44,906 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:47,952 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:47,952 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:48,091 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = b8e1ac67-73f2-48cb-b532-31c2fce691d2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:48,510 INFO  SessionState:1227 - Hive Session ID = b8e1ac67-73f2-48cb-b532-31c2fce691d2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:49,805 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,133 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,155 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,529 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,670 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,730 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,768 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,769 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,770 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,785 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,785 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,805 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,806 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,820 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,821 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,827 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,828 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,837 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,837 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,838 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,839 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,871 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,872 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,874 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,874 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,876 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_121737554' to file:/tmp/hiverunner_tests4275881295083402943/warehouse3814759243658574992/source_db.db/table_a/_SCRATCH0.8215271146922377\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,882 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,882 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,884 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,884 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,922 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:50,958 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:51,251 WARN  org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer:531 - Cannot determine basic stats for table: source_db@table_a from metastore. Falling back.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:51,297 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:51,362 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hello World 2023-06-29T11:06:51,389 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.SetHiveConfValuesTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:51,529 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.991 s - in com.klarna.hiverunner.examples.junit4.SetHiveConfValuesTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.examples.junit4.InsertTestDataTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:53,388 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:53,411 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.InsertTestDataTest in /\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 508127ec-b02c-49f9-b50b-abf2e36ee502\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:54,593 INFO  SessionState:1227 - Hive Session ID = 508127ec-b02c-49f9-b50b-abf2e36ee502\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:55,021 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:56,010 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:57,632 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:06:58,958 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:01,742 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:01,742 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:01,927 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = a348adca-fb04-4130-81dd-ea30102d3d76\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:02,581 INFO  SessionState:1227 - Hive Session ID = a348adca-fb04-4130-81dd-ea30102d3d76\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:04,666 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,101 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,129 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,516 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,631 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,730 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,770 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,770 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,771 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,789 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,790 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,803 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,803 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,816 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,817 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,824 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,824 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,834 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,834 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,835 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,836 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,866 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,866 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,869 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,869 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,873 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_411124968' to file:/tmp/hiverunner_tests7757702283908582809/warehouse6839820731411679141/source_db.db/test_table/_SCRATCH0.3027917508731722\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,883 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,883 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,887 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,887 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,932 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:05,974 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:06,274 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:06,362 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Result from insertRowsFromCode:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [Value1, 1, true]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [Value2, 99, false]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:06,393 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.InsertTestDataTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:06,445 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:06,453 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.InsertTestDataTest in /\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = b4432f37-c400-4cd5-8b4d-1dcaac19aa9b\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:06,506 INFO  SessionState:1227 - Hive Session ID = b4432f37-c400-4cd5-8b4d-1dcaac19aa9b\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:06,535 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:06,538 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:07,052 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:09,380 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = f7b22260-c2a1-427b-8dd0-b5cd30332877\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:09,541 INFO  SessionState:1227 - Hive Session ID = f7b22260-c2a1-427b-8dd0-b5cd30332877\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:09,628 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:09,867 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:09,871 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,062 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,200 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,215 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,216 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,220 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,220 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,239 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,239 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,243 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,243 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,247 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,248 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,272 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,272 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,274 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,274 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,276 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_381295937' to file:/tmp/hiverunner_tests360818585462194681/warehouse8742453494190816434/source_db.db/test_table/_SCRATCH0.4078732373148841\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,281 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,281 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,284 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,284 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,341 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,388 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,448 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Result from insertRowsFromTsvFile:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textA, 42, true]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textB, 3, true]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textC, 99, false]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,452 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.InsertTestDataTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,473 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,478 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.InsertTestDataTest in /\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = f246ee07-6613-4505-87e9-76c28f9f7030\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,516 INFO  SessionState:1227 - Hive Session ID = f246ee07-6613-4505-87e9-76c28f9f7030\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,565 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:10,567 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:11,105 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:12,895 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 8413073b-0eb2-4472-b0bf-6c0fa3062602\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,027 INFO  SessionState:1227 - Hive Session ID = 8413073b-0eb2-4472-b0bf-6c0fa3062602\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,105 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,337 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,341 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,495 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,595 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,608 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,609 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,611 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,611 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,629 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,630 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,634 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,634 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,638 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,639 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,661 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,661 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,663 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,664 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,666 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_396631256' to file:/tmp/hiverunner_tests9032735138884696851/warehouse7743390883962717492/source_db.db/test_table/_SCRATCH0.6901808165426297\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,671 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,671 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,674 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,674 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,716 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,749 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,792 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Result from insertRowsFromCodeWithSelectedColumns:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [Value1, null, true]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [Value2, null, false]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,796 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.InsertTestDataTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,817 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,821 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.InsertTestDataTest in /\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = b490fb02-3cf7-4068-8787-033164d4d4c9\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,856 INFO  SessionState:1227 - Hive Session ID = b490fb02-3cf7-4068-8787-033164d4d4c9\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,876 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:13,879 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:14,242 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,123 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = cd6c3c6a-94b1-4bc6-bb76-95403cd58e08\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,224 INFO  SessionState:1227 - Hive Session ID = cd6c3c6a-94b1-4bc6-bb76-95403cd58e08\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,293 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,549 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,558 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,692 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,778 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,785 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,785 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,787 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,787 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,805 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,805 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,807 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,808 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,811 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,811 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,866 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,867 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,869 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,869 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,871 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_2079549178' to file:/tmp/hiverunner_tests4396260301647085135/warehouse3406754340781016785/source_db.db/test_table2/_SCRATCH0.6693883662294182/col_c=A\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,875 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,875 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,878 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,878 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:16,921 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,032 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,067 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,139 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,146 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,146 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,148 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,148 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,167 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,167 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,169 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,170 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,173 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,173 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,191 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,192 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,194 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,194 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,198 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_1081969321' to file:/tmp/hiverunner_tests4396260301647085135/warehouse3406754340781016785/source_db.db/test_table2/_SCRATCH0.8470713983646218/col_c=B\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,203 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,203 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,206 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,206 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,247 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,329 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,421 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,439 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Result from insertRowsIntoPartitionedTableStoredAsSequencefileWithCustomDelimiterAndNullValue:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textA, 42, A]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [null, 3, A]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textC, 99, B]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,442 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.InsertTestDataTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,456 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,460 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.InsertTestDataTest in /\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 7791902b-abe8-429d-b408-62cc8681eb1c\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,504 INFO  SessionState:1227 - Hive Session ID = 7791902b-abe8-429d-b408-62cc8681eb1c\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,525 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:17,527 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:18,076 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,077 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = f2e149ba-6641-489b-9d77-3c269af5c289\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,189 INFO  SessionState:1227 - Hive Session ID = f2e149ba-6641-489b-9d77-3c269af5c289\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,270 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,448 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,451 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,572 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,656 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,664 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,664 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,666 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,666 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,685 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,685 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,688 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,688 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,691 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,691 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,710 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,710 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,712 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,712 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,715 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_835683620' to file:/tmp/hiverunner_tests3859422734675700365/warehouse2695639770375222186/source_db.db/test_table/_SCRATCH0.6968590045099575\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,720 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,721 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,728 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,729 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,767 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,800 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,856 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Result from insertRowsFromTsvFileWithSubsetHeader:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textA, null, true]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textB, null, true]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textC, null, false]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,858 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.InsertTestDataTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,873 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,877 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.examples.junit4.InsertTestDataTest in /\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 7ee18b4d-5751-4159-bdc0-39f3b10af8ea\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,918 INFO  SessionState:1227 - Hive Session ID = 7ee18b4d-5751-4159-bdc0-39f3b10af8ea\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,959 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:20,961 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:21,554 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,238 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 3f165e78-668e-4a22-9d7f-1decb608ca09\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,364 INFO  SessionState:1227 - Hive Session ID = 3f165e78-668e-4a22-9d7f-1decb608ca09\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,424 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,561 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,563 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,650 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,721 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,727 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,728 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,730 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,730 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,743 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,744 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,747 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,747 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,751 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,751 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,769 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,770 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,772 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,772 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,774 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_2140862348' to file:/tmp/hiverunner_tests8454727830028327647/warehouse2186813871042757006/source_db.db/test_table/_SCRATCH0.5321176728371624\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,777 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,778 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,780 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,780 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,819 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,846 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,882 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Result from insertRowsFromTsvFileWithHeader:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textA, 42, true]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textB, 3, true]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textC, 99, false]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,884 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.examples.junit4.InsertTestDataTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:23,895 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 30.61 s - in com.klarna.hiverunner.examples.junit4.InsertTestDataTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.examples.SetHiveConfValuesTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:26,266 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = da8495b7-8878-4661-a992-2b57b766d81a\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:26,760 INFO  SessionState:1227 - Hive Session ID = da8495b7-8878-4661-a992-2b57b766d81a\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:27,190 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:27,959 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:29,121 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:32,913 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:32,914 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:33,119 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = f4181dc9-e674-446d-8f18-ef5c2753d1f7\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:33,605 INFO  SessionState:1227 - Hive Session ID = f4181dc9-e674-446d-8f18-ef5c2753d1f7\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:35,155 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:35,620 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:35,640 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,057 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,189 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,283 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,332 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,332 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,334 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,350 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,350 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,364 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,365 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,377 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,377 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,383 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,383 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,389 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,390 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,390 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,391 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,430 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,430 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,438 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,439 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,445 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_235854844' to file:/tmp/hiverunner_test34051932041834801/warehouse3240476704661428149/source_db.db/table_a/_SCRATCH0.3868535713567365\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,458 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,459 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,465 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,466 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,509 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,543 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,860 WARN  org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer:531 - Cannot determine basic stats for table: source_db@table_a from metastore. Falling back.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:36,904 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:37,005 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hello World 2023-06-29T11:07:37,070 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.SetHiveConfValuesTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:37,113 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.355 s - in com.klarna.hiverunner.examples.SetHiveConfValuesTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.examples.InsertTestDataTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:39,463 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 0ba9b3fb-604d-44b3-946f-7a07ea18b9f2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:39,958 INFO  SessionState:1227 - Hive Session ID = 0ba9b3fb-604d-44b3-946f-7a07ea18b9f2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:40,454 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:41,497 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:43,047 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:46,732 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:46,732 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:46,873 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 40b39d96-a599-4111-bf34-19898a2bb0b2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:47,290 INFO  SessionState:1227 - Hive Session ID = 40b39d96-a599-4111-bf34-19898a2bb0b2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:48,775 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,086 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,101 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,419 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,511 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,581 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,622 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,622 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,623 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,639 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,639 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,675 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,676 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,687 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,687 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,693 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,693 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,700 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,700 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,701 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,702 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,753 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,753 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,755 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,756 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,758 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_1552516212' to file:/tmp/hiverunner_test4093121775963004285/warehouse1678362337886953024/source_db.db/test_table/_SCRATCH0.6284986271533021\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,765 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,765 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,768 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,768 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,824 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:49,868 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:50,087 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:50,146 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Result from from code:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [Value1, 1, true]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [Value2, 99, false]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:50,174 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.InsertTestDataTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:50,224 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 0b4c600e-b4dc-47ae-81f9-046306246905\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:50,274 INFO  SessionState:1227 - Hive Session ID = 0b4c600e-b4dc-47ae-81f9-046306246905\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:50,297 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:50,299 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:50,697 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:52,793 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = fa1f2a62-9e35-4e62-8fd7-dd7eba3011b9\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:52,909 INFO  SessionState:1227 - Hive Session ID = fa1f2a62-9e35-4e62-8fd7-dd7eba3011b9\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:52,994 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,171 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,299 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,404 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,414 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,414 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,417 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,417 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,437 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,438 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,441 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,441 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,444 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,444 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,461 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,462 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,464 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,464 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,466 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_795218217' to file:/tmp/hiverunner_test9138069439527928060/warehouse6323116049084594399/source_db.db/test_table/_SCRATCH0.22748402926403388\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,471 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,471 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,473 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,473 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,510 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,540 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,582 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Result from TSV file:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textA, 42, true]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textB, 3, true]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textC, 99, false]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,585 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.InsertTestDataTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,602 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 9e04b29b-4cd9-454b-bf27-1cc952009ce5\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,635 INFO  SessionState:1227 - Hive Session ID = 9e04b29b-4cd9-454b-bf27-1cc952009ce5\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,657 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,658 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:53,946 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:55,649 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 17f584fc-8119-440b-bf4b-178f48bf9e26\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:55,763 INFO  SessionState:1227 - Hive Session ID = 17f584fc-8119-440b-bf4b-178f48bf9e26\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:55,884 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,127 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,282 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,394 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,407 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,407 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,410 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,411 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,428 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,429 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,433 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,433 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,438 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,438 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,456 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,456 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,457 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,458 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,467 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_388032252' to file:/tmp/hiverunner_test5070702870290051071/warehouse5056911363929321685/source_db.db/test_table/_SCRATCH0.6761414224937569\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,473 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,473 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,476 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,476 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,541 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,589 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,640 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Result from from code selected columns:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [Value1, null, true]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [Value2, null, false]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,643 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.InsertTestDataTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,660 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 4f3e0229-607d-40ba-8dd1-13f21cc120c6\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,734 INFO  SessionState:1227 - Hive Session ID = 4f3e0229-607d-40ba-8dd1-13f21cc120c6\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,783 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:56,786 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:57,141 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:59,279 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = c323f0e7-eea8-47a7-8c38-12bef68a4674\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:59,421 INFO  SessionState:1227 - Hive Session ID = c323f0e7-eea8-47a7-8c38-12bef68a4674\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:59,523 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:59,766 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:07:59,907 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,009 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,018 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,020 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,023 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,023 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,041 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,043 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,046 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,046 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,049 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,049 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,098 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,098 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,100 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,100 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,103 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_941263743' to file:/tmp/hiverunner_test8014945494342464940/warehouse6033553194455729232/source_db.db/test_table2/_SCRATCH0.751425198834983/col_c=A\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,107 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,107 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,110 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,110 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,157 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,323 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,362 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,414 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,423 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,423 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,425 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,425 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,451 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,452 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,454 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,455 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,458 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,459 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,475 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,475 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,477 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,477 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,478 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_2145789259' to file:/tmp/hiverunner_test8014945494342464940/warehouse6033553194455729232/source_db.db/test_table2/_SCRATCH0.25541755331444516/col_c=B\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,481 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,481 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,483 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,484 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,522 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,659 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,746 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,767 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Result from long method name:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textA, 42, A]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [null, 3, A]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textC, 99, B]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,770 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.InsertTestDataTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,787 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 44832b83-2672-4964-8e0c-1539d8e0f58d\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,820 INFO  SessionState:1227 - Hive Session ID = 44832b83-2672-4964-8e0c-1539d8e0f58d\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,840 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:00,842 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:01,239 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:02,878 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 1e93e0da-a2b8-43f1-a4bb-219fe42f0c22\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:02,975 INFO  SessionState:1227 - Hive Session ID = 1e93e0da-a2b8-43f1-a4bb-219fe42f0c22\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,042 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,228 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,331 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,418 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,428 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,428 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,430 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,431 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,469 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,469 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,473 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,474 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,477 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,477 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,495 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,495 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,498 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,498 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,502 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_1284091271' to file:/tmp/hiverunner_test1619374421127642756/warehouse1863135078935531194/source_db.db/test_table/_SCRATCH0.4084545744666316\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,505 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,506 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,508 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,508 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,579 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,619 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,706 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Result from TSV file subset header:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textA, null, true]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textB, null, true]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textC, null, false]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,709 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.InsertTestDataTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,737 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 566b429a-8631-4dcb-9714-b9124de751c1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,784 INFO  SessionState:1227 - Hive Session ID = 566b429a-8631-4dcb-9714-b9124de751c1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,815 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:03,816 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:04,227 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:06,488 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 007717af-c489-4a43-b726-982a378a7dc3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:06,631 INFO  SessionState:1227 - Hive Session ID = 007717af-c489-4a43-b726-982a378a7dc3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:06,739 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.source_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:06,962 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,099 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,211 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,219 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,220 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,222 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,223 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,239 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,239 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,242 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,243 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,246 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,247 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,267 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,268 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,270 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,270 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,272 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_101097758' to file:/tmp/hiverunner_test2841256830063259569/warehouse5169515204704830423/source_db.db/test_table/_SCRATCH0.659608256442293\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,276 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,277 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,280 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,280 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,327 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,371 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,418 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Result from TSV file header:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textA, 42, true]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textB, 3, true]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [textC, 99, false]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,421 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.examples.InsertTestDataTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:07,435 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 28.731 s - in com.klarna.hiverunner.examples.InsertTestDataTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:10,223 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 6dc9fde0-93ce-4b41-9473-21a017ab5c2b\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:10,725 INFO  SessionState:1227 - Hive Session ID = 6dc9fde0-93ce-4b41-9473-21a017ab5c2b\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:11,108 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:11,997 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:13,218 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:16,884 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:16,886 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:17,092 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = d7615295-316a-48da-bc50-742272f73f15\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:17,698 INFO  SessionState:1227 - Hive Session ID = d7615295-316a-48da-bc50-742272f73f15\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:19,974 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:20,037 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:20,071 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:20,127 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 83c5a2ba-885b-4d90-8104-1e2fe0dcde5a\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:20,213 INFO  SessionState:1227 - Hive Session ID = 83c5a2ba-885b-4d90-8104-1e2fe0dcde5a\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:20,242 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:20,246 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:20,812 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:23,047 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 33c43a0c-4547-4109-8b2f-395d84c12720\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:23,212 INFO  SessionState:1227 - Hive Session ID = 33c43a0c-4547-4109-8b2f-395d84c12720\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:24,041 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:24,046 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:24,072 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 25825c12-7865-40b6-b1c4-79d8829e0cc3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:24,120 INFO  SessionState:1227 - Hive Session ID = 25825c12-7865-40b6-b1c4-79d8829e0cc3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:24,144 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:24,146 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:24,682 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:26,296 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = f8e2656e-357c-46d9-bb24-f11652657a16\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:26,393 INFO  SessionState:1227 - Hive Session ID = f8e2656e-357c-46d9-bb24-f11652657a16\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:26,745 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:26,758 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = ce9f9339-a5b4-4570-a9dd-c2f1661b8f32\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:26,802 INFO  SessionState:1227 - Hive Session ID = ce9f9339-a5b4-4570-a9dd-c2f1661b8f32\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:26,823 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:26,826 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:27,223 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:28,841 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = f0d85c30-9874-4715-99f6-7d39fbe81f90\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:28,963 INFO  SessionState:1227 - Hive Session ID = f0d85c30-9874-4715-99f6-7d39fbe81f90\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:29,326 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:29,330 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:29,342 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 3f9a40c9-4f75-4fc3-b00f-b4e485dc4b9e\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:29,377 INFO  SessionState:1227 - Hive Session ID = 3f9a40c9-4f75-4fc3-b00f-b4e485dc4b9e\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:29,405 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:29,407 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:29,677 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:31,263 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 84cb08ca-a718-4f93-bdd6-b6cf8b86c335\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:31,358 INFO  SessionState:1227 - Hive Session ID = 84cb08ca-a718-4f93-bdd6-b6cf8b86c335\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:31,766 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:31,768 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:31,785 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 103646c9-d117-4198-a87e-88c16435fa2c\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:31,853 INFO  SessionState:1227 - Hive Session ID = 103646c9-d117-4198-a87e-88c16435fa2c\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:31,876 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:31,878 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:32,259 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:34,391 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = b237b2ab-d03f-4ebc-818d-e0e27c10d25e\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:34,540 INFO  SessionState:1227 - Hive Session ID = b237b2ab-d03f-4ebc-818d-e0e27c10d25e\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:34,951 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:34,953 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:34,968 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = c18c4c2f-2a6b-4c13-8795-c4d34b735083\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:35,009 INFO  SessionState:1227 - Hive Session ID = c18c4c2f-2a6b-4c13-8795-c4d34b735083\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:35,052 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:35,054 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:35,639 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:37,423 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 7621981c-a12f-40a7-b2cf-58a07745ccdc\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:37,526 INFO  SessionState:1227 - Hive Session ID = 7621981c-a12f-40a7-b2cf-58a07745ccdc\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:37,854 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:37,861 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:37,875 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 3cabd372-281b-4c8a-b7eb-840288fadf04\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:37,934 INFO  SessionState:1227 - Hive Session ID = 3cabd372-281b-4c8a-b7eb-840288fadf04\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:37,965 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:37,967 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:38,370 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:40,145 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:40,266 INFO  SessionState:1227 - Hive Session ID = 9d202991-903d-4c48-bb7f-5b1ae2ed9d62\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 9d202991-903d-4c48-bb7f-5b1ae2ed9d62\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:40,592 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:40,594 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:40,606 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 31.083 s - in com.klarna.hiverunner.HiveRunnerAnnotationsTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.HiveShellHiveCliEmulationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:42,941 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 12e3e4dc-03df-4315-b5f1-b5be1da00393\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:43,533 INFO  SessionState:1227 - Hive Session ID = 12e3e4dc-03df-4315-b5f1-b5be1da00393\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:43,953 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:44,894 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:46,235 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:50,186 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:50,186 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:50,322 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 4ab8bfaa-e86b-408c-afd7-6f1bfcac655a\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:50,798 INFO  SessionState:1227 - Hive Session ID = 4ab8bfaa-e86b-408c-afd7-6f1bfcac655a\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:52,344 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellHiveCliEmulationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:52,395 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 583d83d8-e175-4527-ab39-8c90beba8175\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:52,458 INFO  SessionState:1227 - Hive Session ID = 583d83d8-e175-4527-ab39-8c90beba8175\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:52,487 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:52,491 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:53,010 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:55,167 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 9d226eb3-6513-450a-aa51-c86fa2861060\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:55,300 INFO  SessionState:1227 - Hive Session ID = 9d226eb3-6513-450a-aa51-c86fa2861060\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:55,405 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellHiveCliEmulationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:55,428 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 2ce09ce4-e8d3-48cb-a1a8-a27021e5dab4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:55,474 INFO  SessionState:1227 - Hive Session ID = 2ce09ce4-e8d3-48cb-a1a8-a27021e5dab4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:55,510 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:55,513 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:55,946 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:58,120 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:58,298 INFO  SessionState:1227 - Hive Session ID = 03cc73b7-3c04-42a2-87af-e536a4fef799\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 03cc73b7-3c04-42a2-87af-e536a4fef799\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:58,406 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellHiveCliEmulationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:58,424 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:58,483 INFO  SessionState:1227 - Hive Session ID = 70460f2e-cc4e-41ae-8775-596dcbb7b329\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 70460f2e-cc4e-41ae-8775-596dcbb7b329\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:58,519 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:58,521 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:08:59,072 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:00,983 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 0f48dc56-eaa1-4172-a62c-e4426fd2f7b2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:01,124 INFO  SessionState:1227 - Hive Session ID = 0f48dc56-eaa1-4172-a62c-e4426fd2f7b2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:01,200 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellHiveCliEmulationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:01,234 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = ffe15741-2b32-4498-b737-deaf2ef40c56\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:01,279 INFO  SessionState:1227 - Hive Session ID = ffe15741-2b32-4498-b737-deaf2ef40c56\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:01,304 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:01,305 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:01,620 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:03,232 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = cb34d6e4-9772-4cab-81aa-fd537d94ac82\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:03,346 INFO  SessionState:1227 - Hive Session ID = cb34d6e4-9772-4cab-81aa-fd537d94ac82\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:03,408 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellHiveCliEmulationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:03,419 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 089b0ed4-e7df-4d19-8cff-561cff99cd48\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:03,449 INFO  SessionState:1227 - Hive Session ID = 089b0ed4-e7df-4d19-8cff-561cff99cd48\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:03,469 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:03,472 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:03,742 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:06,260 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = c2ec1854-9485-4d5c-b3c0-00e8c7528346\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:06,399 INFO  SessionState:1227 - Hive Session ID = c2ec1854-9485-4d5c-b3c0-00e8c7528346\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:06,466 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellHiveCliEmulationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:06,480 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = a06081e2-d27a-48e5-b014-e7f28071f59e\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:06,515 INFO  SessionState:1227 - Hive Session ID = a06081e2-d27a-48e5-b014-e7f28071f59e\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:06,540 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:06,543 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:06,969 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:08,845 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 93f965d6-de2f-424c-8553-71496e043c14\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:08,991 INFO  SessionState:1227 - Hive Session ID = 93f965d6-de2f-424c-8553-71496e043c14\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | NoViableAltException(-1@[])\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1387)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:220)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:74)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:616)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:197)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:260)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:247)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:541)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:510)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:267)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.HiveServerContainer.executeStatement(HiveServerContainer.java:127)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatementsWithCommandShellEmulation(HiveShellBase.java:116)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatementWithCommandShellEmulation(HiveShellBase.java:110)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatement(HiveShellBase.java:100)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:89)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:82)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.HiveShellHiveCliEmulationTest.lambda$testQueryStripFullLineComment$0(HiveShellHiveCliEmulationTest.java:61)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:55)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:37)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3007)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.HiveShellHiveCliEmulationTest.testQueryStripFullLineComment(HiveShellHiveCliEmulationTest.java:61)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat java.lang.reflect.Method.invoke(Method.java:498)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat java.util.ArrayList.forEach(ArrayList.java:1259)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat java.util.ArrayList.forEach(ArrayList.java:1259)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | FAILED: ParseException line 1:0 cannot recognize input near '<EOF>' '<EOF>' '<EOF>'\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:09,130 ERROR org.apache.hadoop.hive.ql.Driver:1250 - FAILED: ParseException line 1:0 cannot recognize input near '<EOF>' '<EOF>' '<EOF>'\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | org.apache.hadoop.hive.ql.parse.ParseException: line 1:0 cannot recognize input near '<EOF>' '<EOF>' '<EOF>'\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:223)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:74)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:616)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:197)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:260)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:247)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:541)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:510)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:267)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.HiveServerContainer.executeStatement(HiveServerContainer.java:127)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatementsWithCommandShellEmulation(HiveShellBase.java:116)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatementWithCommandShellEmulation(HiveShellBase.java:110)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeStatement(HiveShellBase.java:100)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:89)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.builder.HiveShellBase.executeQuery(HiveShellBase.java:82)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.HiveShellHiveCliEmulationTest.lambda$testQueryStripFullLineComment$0(HiveShellHiveCliEmulationTest.java:61)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:55)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:37)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3007)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat com.klarna.hiverunner.HiveShellHiveCliEmulationTest.testQueryStripFullLineComment(HiveShellHiveCliEmulationTest.java:61)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat java.lang.reflect.Method.invoke(Method.java:498)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat java.util.ArrayList.forEach(ArrayList.java:1259)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat java.util.ArrayList.forEach(ArrayList.java:1259)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:09,155 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.HiveShellHiveCliEmulationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:09,168 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 26.726 s - in com.klarna.hiverunner.HiveShellHiveCliEmulationTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.CtasTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:11,334 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = de6ac72e-3042-4ff6-8ec7-a73a44a868c5\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:11,908 INFO  SessionState:1227 - Hive Session ID = de6ac72e-3042-4ff6-8ec7-a73a44a868c5\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:12,318 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:13,097 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:14,281 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:18,050 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:18,051 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:18,197 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 1678a429-e223-442d-97e8-3e52a229b75a\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:18,671 INFO  SessionState:1227 - Hive Session ID = 1678a429-e223-442d-97e8-3e52a229b75a\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:20,844 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110920_11a3b718-25e8-4f2f-af3c-e5cbf63c8406\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:21,093 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:21,109 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:21,124 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:21,125 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:21,157 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:21,231 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:21,456 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:21,474 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:21,515 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:21,578 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local180824649_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:21,581 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:21,892 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:21,894 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:21,896 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:21,915 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:21,921 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local180824649_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:21,965 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:21,975 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test3689519803261744264/hadooptmp7202629157771860404/foo/data.csv:0+11InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:22,054 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:22,054 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:22,054 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:22,055 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:22,092 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:22,094 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:22,177 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:22,186 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local180824649_0001_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:22,188 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test3689519803261744264/hadooptmp7202629157771860404/foo/data.csv:0+11\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:22,189 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local180824649_0001_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:22,192 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local180824649_0001_m_000000_0: Counters: 24\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=40624264\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=41747186\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=240\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=45\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2001731584\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_1_default.foo_prim=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:22,192 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local180824649_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:22,194 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:09:22,907 Stage-1 map = 100%,  reduce = 0%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local180824649_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test3689519803261744264/warehouse7193618152572095749/.hive-staging_hive_2023-06-29_11-09-20_381_713584459830263741-1/-ext-10001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test3689519803261744264/warehouse7193618152572095749/foo_prim\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:22,942 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:23,051 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:23,116 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:23,127 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:23,152 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.CtasTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:23,202 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = c16d2fa2-5700-4da8-8d49-f25998692be2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:23,252 INFO  SessionState:1227 - Hive Session ID = c16d2fa2-5700-4da8-8d49-f25998692be2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:23,273 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:23,277 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:23,906 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:26,080 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 0bb99b1b-d035-4db3-a2a8-07c48ec69ba3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:26,189 INFO  SessionState:1227 - Hive Session ID = 0bb99b1b-d035-4db3-a2a8-07c48ec69ba3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:26,553 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110926_616ae319-9d12-4266-8494-ffb2ce4335bb\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:26,590 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:26,597 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:26,623 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:26,803 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:26,804 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:26,849 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:26,888 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local1814497641_0002\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:26,888 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:27,122 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:27,123 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:27,124 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:27,129 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:27,129 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local1814497641_0002_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:27,134 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:27,136 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test2434709584225617306/hadooptmp1200595953067971253/foo/data.csv:0+11InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:27,144 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:27,170 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:27,172 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1814497641_0002_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:27,174 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test2434709584225617306/hadooptmp1200595953067971253/foo/data.csv:0+11\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:27,174 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1814497641_0002_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:27,174 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1814497641_0002_m_000000_0: Counters: 24\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=81248556\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=83498264\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=240\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2087714816\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_1_default.foo_prim=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:27,175 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local1814497641_0002_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:27,175 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:09:28,130 Stage-1 map = 100%,  reduce = 0%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local1814497641_0002\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test2434709584225617306/warehouse7605404297721075668/.hive-staging_hive_2023-06-29_11-09-26_396_8158282170563817682-1/-ext-10001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test2434709584225617306/warehouse7605404297721075668/foo_prim\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:28,140 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:28,181 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:28,221 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:28,224 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.CtasTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:28,239 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 6d852270-d7fa-4bb4-aa7a-cd1134c0a1fb\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:28,280 INFO  SessionState:1227 - Hive Session ID = 6d852270-d7fa-4bb4-aa7a-cd1134c0a1fb\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:28,304 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:28,307 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:28,690 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:30,396 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = e9cdf853-7eb1-4d60-ad00-e33560049634\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:30,542 INFO  SessionState:1227 - Hive Session ID = e9cdf853-7eb1-4d60-ad00-e33560049634\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:30,898 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110930_56f62a18-d29a-4ddf-8644-1eb8be57b9d4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:30,928 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:30,936 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:30,946 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,066 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,068 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,141 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,173 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local992191761_0003\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,174 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,361 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,360 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,362 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,364 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,364 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local992191761_0003_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,369 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,370 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test9219909767892947392/hadooptmp8202420415529140021/foo/data.csv:0+11InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,378 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,399 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,402 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local992191761_0003_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,403 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test9219909767892947392/hadooptmp8202420415529140021/foo/data.csv:0+11\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,403 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local992191761_0003_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,404 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local992191761_0003_m_000000_0: Counters: 24\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=121872849\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=125245475\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=240\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2088239104\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_1_default.foo_prim=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_5=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,404 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local992191761_0003_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:31,405 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:09:32,366 Stage-1 map = 100%,  reduce = 0%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local992191761_0003\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test9219909767892947392/warehouse7307515430572771025/.hive-staging_hive_2023-06-29_11-09-30_741_8741996809466041390-1/-ext-10001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test9219909767892947392/warehouse7307515430572771025/foo_prim\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:32,382 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:32,441 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:32,611 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110932_8d98cb94-c46b-4521-9165-0b444152d37f\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks determined at compile time: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to limit the maximum number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to set a constant number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:32,651 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:32,657 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:32,667 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:32,799 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:32,800 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:32,839 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:32,868 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local47719828_0004\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:32,869 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,077 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,077 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,078 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,081 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,081 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local47719828_0004_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,083 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,093 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test9219909767892947392/warehouse7307515430572771025/foo_prim/000000_0:0+12InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,111 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,130 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,130 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,130 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,130 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,130 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,134 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,161 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,164 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,164 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,164 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 11; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,164 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,196 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,203 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local47719828_0004_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,204 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_test9219909767892947392/warehouse7307515430572771025/foo_prim/000000_0:0+12\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,204 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local47719828_0004_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,205 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local47719828_0004_m_000000_0: Counters: 25\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=162497135\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=166994595\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output bytes=11\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output materialized bytes=19\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=245\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2088239104\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_8=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_9=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_7=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,205 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local47719828_0004_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,205 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,210 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,211 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local47719828_0004_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,217 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,221 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@521a90bb\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,223 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,243 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1461767296, maxSingleShuffleLimit=365441824, mergeThreshold=964766464, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,247 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local47719828_0004_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,283 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#1 about to shuffle output of map attempt_local47719828_0004_m_000000_0 decomp: 15 len: 19 to MEMORY\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,290 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 15 bytes from map-output for attempt_local47719828_0004_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,294 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 15, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->15\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,296 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,298 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,299 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,320 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,321 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 8 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,325 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 15 bytes to disk to satisfy reduce memory limit\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,326 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 19 bytes from disk\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,327 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,327 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,327 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 8 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,328 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,330 INFO  ExecReducer:99 - conf classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter952304452249917518.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,330 INFO  ExecReducer:101 - thread classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter952304452249917518.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,336 INFO  ExecReducer:147 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <GBY>Id =4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <FS>Id =6\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Parent>Id = 4 null<\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <\\FS>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Parent><\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <\\GBY>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,371 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local47719828_0004_r_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,372 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,372 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local47719828_0004_r_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,373 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local47719828_0004_r_000000_0: Counters: 29\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=162497205\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=166994727\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input groups=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce shuffle bytes=19\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input records=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tShuffled Maps =1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2088239104\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_0=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_6=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_4=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tShuffle Errors\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBAD_ID=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCONNECTION=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tIO_ERROR=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_MAP=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,373 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local47719828_0004_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:33,374 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:09:34,087 Stage-1 map = 100%,  reduce = 100%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local47719828_0004\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:34,099 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:34,110 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.CtasTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:34,132 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 23.322 s - in com.klarna.hiverunner.CtasTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.HiveCliSourceTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:35,990 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:36,009 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.HiveCliSourceTest in /\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 84ca0066-2b01-4ce7-87c9-83222f640317\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:37,086 INFO  SessionState:1227 - Hive Session ID = 84ca0066-2b01-4ce7-87c9-83222f640317\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:37,537 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:38,554 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:39,880 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:43,772 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:43,772 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:43,907 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = e98e495e-a75b-4775-8855-7125cdd11f18\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:44,327 INFO  SessionState:1227 - Hive Session ID = e98e495e-a75b-4775-8855-7125cdd11f18\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:45,767 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.test_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,135 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,157 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,520 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,629 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,732 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,774 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,774 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,775 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,785 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,786 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,810 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,810 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,825 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,828 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,837 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,837 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,845 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,846 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,846 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,847 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,885 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,886 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,890 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,890 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,893 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_1289927252' to file:/tmp/hiverunner_tests1336777605218148996/warehouse9198369080660526342/test_db.db/src/_SCRATCH0.9354411415522803\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,900 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,900 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,905 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,905 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,960 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:46,996 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:47,169 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.db_b, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:47,315 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.db_c, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:47,377 WARN  org.apache.hadoop.hive.ql.parse.RowResolver:132 - Duplicate column info for a.c0 was overwritten in RowResolver map: _col0: string by _col0: string\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:47,494 WARN  org.apache.hadoop.hive.ql.parse.RowResolver:132 - Duplicate column info for a.c0 was overwritten in RowResolver map: _col0: string by _col0: string\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:47,752 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629110947_894f679d-2589-4395-aeb8-9afac578aa44\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks not specified. Estimated from input data size: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to limit the maximum number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to set a constant number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,062 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,080 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,099 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,100 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,125 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,214 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,486 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,500 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,537 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,606 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local1480966532_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,606 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,817 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,818 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,819 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,831 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,832 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local1480966532_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,885 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,895 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_tests1336777605218148996/warehouse9198369080660526342/test_db.db/src/part-m-1289927252:0+20InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,980 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,981 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,981 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,981 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,998 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,998 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,998 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,998 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:48,998 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,001 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,069 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,069 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,069 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,069 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 28; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,069 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,249 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,266 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1480966532_0001_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,270 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_tests1336777605218148996/warehouse9198369080660526342/test_db.db/src/part-m-1289927252:0+20\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,270 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1480966532_0001_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,274 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1480966532_0001_m_000000_0: Counters: 26\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=40624310\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=41759001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=5\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output bytes=28\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output materialized bytes=38\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=261\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=1998061568\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=5\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FIL_14=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_16=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_17=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_15=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=5\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,274 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local1480966532_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,276 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,281 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,281 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local1480966532_0001_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,294 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,298 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@27783dc7\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,300 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,320 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1398643072, maxSingleShuffleLimit=349660768, mergeThreshold=923104448, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,326 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local1480966532_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,362 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#1 about to shuffle output of map attempt_local1480966532_0001_m_000000_0 decomp: 34 len: 38 to MEMORY\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,365 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 34 bytes from map-output for attempt_local1480966532_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,367 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 34, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->34\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,368 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,369 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,370 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,424 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,424 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 24 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,434 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 34 bytes to disk to satisfy reduce memory limit\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,434 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 38 bytes from disk\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,435 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,436 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,436 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 24 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,437 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,439 INFO  ExecReducer:99 - conf classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter503443911359073568.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,440 INFO  ExecReducer:101 - thread classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter503443911359073568.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,455 INFO  ExecReducer:147 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <GBY>Id =6\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <FIL>Id =12\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |         <FS>Id =11\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |           <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |           <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |           <Parent>Id = 12 null<\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |         <\\FS>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Parent>Id = 6 null<\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <\\FIL>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Parent><\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <\\GBY>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,460 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,502 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local1480966532_0001_r_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,504 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,504 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local1480966532_0001_r_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,504 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local1480966532_0001_r_000000_0: Counters: 30\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=40624418\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=41759154\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input groups=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce shuffle bytes=38\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tShuffled Maps =1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=1998061568\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_0=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FIL_12=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_11=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_6=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tShuffle Errors\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBAD_ID=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCONNECTION=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tIO_ERROR=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_MAP=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,504 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local1480966532_0001_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,505 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:09:49,836 Stage-1 map = 100%,  reduce = 100%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local1480966532_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,854 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,862 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,885 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.HiveCliSourceTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:49,930 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.029 s - in com.klarna.hiverunner.HiveCliSourceTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.NoTimeoutTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:52,336 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = a516c5a3-26fa-41fb-a809-0336351e7478\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:53,054 INFO  SessionState:1227 - Hive Session ID = a516c5a3-26fa-41fb-a809-0336351e7478\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:53,542 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:54,362 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:55,526 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:59,263 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:59,264 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:59,422 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = ea6c16d1-6afc-42ba-befe-122201e60b4e\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:09:59,914 INFO  SessionState:1227 - Hive Session ID = ea6c16d1-6afc-42ba-befe-122201e60b4e\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:01,371 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.baz, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:02,149 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629111001_3af836ad-93af-4279-88c0-3c98c87d3577\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks is set to 0 since there's no reduce operator\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:02,425 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:02,440 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:02,453 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:02,454 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:02,490 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:02,551 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:02,809 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:02,835 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:02,897 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:02,951 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local339342291_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:02,951 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,266 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,269 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,271 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,285 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,290 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local339342291_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,330 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,340 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_test3915834494327485114/localscratchdir8843731898898161806/081e13e3-74a9-478e-9410-260a5daa826e/hive_2023-06-29_11-10-01_682_35362762569396539-1/dummy_path/dummy_file:0+1InputFormatClass: org.apache.hadoop.hive.ql.io.NullRowsInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,407 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,408 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,408 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,408 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,422 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,423 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,534 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,543 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local339342291_0001_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,545 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - map\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,546 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local339342291_0001_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,550 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local339342291_0001_m_000000_0: Counters: 25\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=40624363\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=41750311\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=350\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=43\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=2001207296\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_1_baz.foo=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_3=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_1=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_UDTF_2=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,551 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local339342291_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:03,553 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:10:04,285 Stage-1 map = 100%,  reduce = 0%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local339342291_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-3 is selected by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-2 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-4 is filtered out by condition resolver.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Moving data to directory file:/tmp/hiverunner_test3915834494327485114/warehouse7722789722376856494/baz.db/foo/.hive-staging_hive_2023-06-29_11-10-01_682_35362762569396539-1/-ext-10000\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Loading data to table baz.foo\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:04,315 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:04,546 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:04,626 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:04,638 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:05,682 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.NoTimeoutTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:05,729 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.039 s - in com.klarna.hiverunner.NoTimeoutTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.BeelineRunTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:07,453 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:07,470 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.BeelineRunTest in /\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = d7d39332-cc3c-4377-8103-ecc4243133b2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:08,445 INFO  SessionState:1227 - Hive Session ID = d7d39332-cc3c-4377-8103-ecc4243133b2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:08,963 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:10,191 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:11,606 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:12,712 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:16,864 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:16,865 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:17,054 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = d73d2889-3ccc-44d9-9433-1fdb4f2da959\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:17,667 INFO  SessionState:1227 - Hive Session ID = d73d2889-3ccc-44d9-9433-1fdb4f2da959\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:19,616 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.test_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,122 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,147 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,560 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,669 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,747 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,804 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,804 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,805 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,818 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,819 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,851 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,851 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,871 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,871 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,889 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,890 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,896 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,896 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,917 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,919 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,974 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,975 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,996 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:20,999 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:21,002 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_1342691302' to file:/tmp/hiverunner_tests2790540169820566850/warehouse2463355407084851724/test_db.db/src/_SCRATCH0.07766992763644587\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:21,009 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:21,010 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:21,013 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:21,014 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:21,064 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:21,104 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:21,298 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.db_b, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:21,508 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.db_c, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:21,638 WARN  org.apache.hadoop.hive.ql.parse.RowResolver:132 - Duplicate column info for a.c0 was overwritten in RowResolver map: _col0: string by _col0: string\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:21,817 WARN  org.apache.hadoop.hive.ql.parse.RowResolver:132 - Duplicate column info for a.c0 was overwritten in RowResolver map: _col0: string by _col0: string\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:22,116 WARN  org.apache.hadoop.hive.ql.Driver:2591 - Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Query ID = runneradmin_20230629111021_ca80cf35-9692-47fb-b270-dcbf6b52fe6b\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total jobs = 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Launching Job 1 out of 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Number of reduce tasks not specified. Estimated from input data size: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to change the average load for a reducer (in bytes):\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.bytes.per.reducer=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to limit the maximum number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set hive.exec.reducers.max=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | In order to set a constant number of reducers:\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   set mapreduce.job.reduces=<number>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:22,491 INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector:147 - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:22,510 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig:134 - Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties,hadoop-metrics2.properties\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:22,550 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:22,551 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:191 - JobTracker metrics system started\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:22,597 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:22,696 WARN  org.apache.hadoop.mapreduce.JobResourceUploader:147 - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,125 INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat:290 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,142 INFO  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:428 - DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,222 INFO  org.apache.hadoop.mapreduce.JobSubmitter:205 - number of splits:1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,316 INFO  org.apache.hadoop.mapreduce.JobSubmitter:301 - Submitting tokens for job: job_local912580130_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,317 INFO  org.apache.hadoop.mapreduce.JobSubmitter:302 - Executing with tokens: []\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,597 INFO  org.apache.hadoop.mapreduce.Job:1574 - The url to track the job: http://localhost:8080/\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,599 INFO  org.apache.hadoop.mapred.LocalJobRunner:501 - OutputCommitter set in config org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,600 INFO  org.apache.hadoop.mapred.LocalJobRunner:519 - OutputCommitter is org.apache.hadoop.hive.ql.io.HiveFileFormatUtils$NullOutputCommitter\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Job running in-process (local Hadoop)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,628 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for map tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,631 INFO  org.apache.hadoop.mapred.LocalJobRunner:252 - Starting task: attempt_local912580130_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,697 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,705 INFO  org.apache.hadoop.mapred.MapTask:497 - Processing split: Paths:/tmp/hiverunner_tests2790540169820566850/warehouse2463355407084851724/test_db.db/src/part-m-1342691302:0+20InputFormatClass: org.apache.hadoop.mapred.TextInputFormat\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,784 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.file is deprecated. Instead, use mapreduce.map.input.file\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,784 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.start is deprecated. Instead, use mapreduce.map.input.start\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,784 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - map.input.length is deprecated. Instead, use mapreduce.map.input.length\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,785 INFO  org.apache.hadoop.mapred.MapTask:451 - numReduceTasks: 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,814 INFO  org.apache.hadoop.mapred.MapTask:1219 - (EQUATOR) 0 kvi 26214396(104857584)\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,815 INFO  org.apache.hadoop.mapred.MapTask:1012 - mapreduce.task.io.sort.mb: 100\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,815 INFO  org.apache.hadoop.mapred.MapTask:1013 - soft limit at 83886080\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,815 INFO  org.apache.hadoop.mapred.MapTask:1014 - bufstart = 0; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,815 INFO  org.apache.hadoop.mapred.MapTask:1015 - kvstart = 26214396; length = 6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,822 INFO  org.apache.hadoop.mapred.MapTask:409 - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,911 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,911 INFO  org.apache.hadoop.mapred.MapTask:1476 - Starting flush of map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,912 INFO  org.apache.hadoop.mapred.MapTask:1498 - Spilling map output\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,912 INFO  org.apache.hadoop.mapred.MapTask:1499 - bufstart = 0; bufend = 28; bufvoid = 104857600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:23,912 INFO  org.apache.hadoop.mapred.MapTask:1501 - kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,101 INFO  org.apache.hadoop.mapred.MapTask:1696 - Finished spill 0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,117 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local912580130_0001_m_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,120 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - file:/tmp/hiverunner_tests2790540169820566850/warehouse2463355407084851724/test_db.db/src/part-m-1342691302:0+20\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,120 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local912580130_0001_m_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,123 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local912580130_0001_m_000000_0: Counters: 26\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=40624310\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=41755115\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap input records=5\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output bytes=28\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMap output materialized bytes=38\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tInput split bytes=261\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=1998585856\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tDESERIALIZE_ERRORS=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_IN=5\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FIL_14=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_16=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_MAP_0=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_RS_17=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_SEL_15=3\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_TS_0=5\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Input Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Read=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,123 INFO  org.apache.hadoop.mapred.LocalJobRunner:277 - Finishing task: attempt_local912580130_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,125 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - map task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,129 INFO  org.apache.hadoop.mapred.LocalJobRunner:478 - Waiting for reduce tasks\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,130 INFO  org.apache.hadoop.mapred.LocalJobRunner:330 - Starting task: attempt_local912580130_0001_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,146 INFO  org.apache.hadoop.mapred.Task:625 -  Using ResourceCalculatorProcessTree : [ ]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,150 INFO  org.apache.hadoop.mapred.ReduceTask:363 - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@38552101\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,152 WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl:151 - JobTracker metrics system already initialized!\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,171 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:208 - MergerManager: memoryLimit=1399010048, maxSingleShuffleLimit=349752512, mergeThreshold=923346688, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,179 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:61 - attempt_local912580130_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,229 INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:145 - localfetcher#1 about to shuffle output of map attempt_local912580130_0001_m_000000_0 decomp: 34 len: 38 to MEMORY\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,233 INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:94 - Read 34 bytes from map-output for attempt_local912580130_0001_m_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,236 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:323 - closeInMemoryFile -> map-output of size: 34, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->34\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,239 INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher:76 - EventFetcher is interrupted.. Returning\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,240 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,240 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:695 - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,389 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,389 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 24 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,395 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:762 - Merged 1 segments, 34 bytes to disk to satisfy reduce memory limit\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,395 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:792 - Merging 1 files, 38 bytes from disk\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,397 INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:807 - Merging 0 segments, 0 bytes from memory into reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,397 INFO  org.apache.hadoop.mapred.Merger:606 - Merging 1 sorted segments\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,397 INFO  org.apache.hadoop.mapred.Merger:705 - Down to the last merge-pass, with 1 segments left of total size: 24 bytes\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,398 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - 1 / 1 copied.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,401 INFO  ExecReducer:99 - conf classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter7923454984031459377.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,401 INFO  ExecReducer:101 - thread classpath = [file:/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/target/surefire/surefirebooter7923454984031459377.jar]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,415 INFO  ExecReducer:147 - \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <GBY>Id =6\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <FIL>Id =12\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |         <FS>Id =11\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |           <Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |           <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |           <Parent>Id = 12 null<\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |         <\\FS>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |       <Parent>Id = 6 null<\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |     <\\FIL>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <\\Children>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   |   <Parent><\\Parent>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | <\\GBY>\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,421 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.healthChecker.script.timeout is deprecated. Instead, use mapreduce.tasktracker.healthchecker.script.timeout\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,486 INFO  org.apache.hadoop.mapred.Task:1232 - Task:attempt_local912580130_0001_r_000000_0 is done. And is in the process of committing\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,489 INFO  org.apache.hadoop.mapred.LocalJobRunner:628 - reduce > reduce\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,489 INFO  org.apache.hadoop.mapred.Task:1368 - Task 'attempt_local912580130_0001_r_000000_0' done.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,490 INFO  org.apache.hadoop.mapred.Task:1264 - Final Counters for attempt_local912580130_0001_r_000000_0: Counters: 30\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile System Counters\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes read=40624418\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of bytes written=41755268\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of large read operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFILE: Number of write operations=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tMap-Reduce Framework\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine input records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCombine output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input groups=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce shuffle bytes=38\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce input records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tReduce output records=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tSpilled Records=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tShuffled Maps =1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tFailed Shuffles=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tMerged Map outputs=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tGC time elapsed (ms)=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tTotal committed heap usage (bytes)=1998585856\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tHIVE\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCREATED_FILES=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_0=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_INTERMEDIATE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FIL_12=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_FS_11=1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tRECORDS_OUT_OPERATOR_GBY_6=2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tShuffle Errors\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBAD_ID=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tCONNECTION=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tIO_ERROR=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_LENGTH=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_MAP=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tWRONG_REDUCE=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \tFile Output Format Counters \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | \t\tBytes Written=0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,490 INFO  org.apache.hadoop.mapred.LocalJobRunner:353 - Finishing task: attempt_local912580130_0001_r_000000_0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,491 INFO  org.apache.hadoop.mapred.LocalJobRunner:486 - reduce task executor complete.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29 11:10:24,630 Stage-1 map = 100%,  reduce = 100%\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Ended Job = job_local912580130_0001\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | MapReduce Jobs Launched: \n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Total MapReduce CPU Time Spent: 0 msec\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,655 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,665 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,691 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.BeelineRunTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:24,776 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 17.392 s - in com.klarna.hiverunner.BeelineRunTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.AnnotatedFieldsInSuperClassTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:27,054 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 7527f4f7-5a4b-4023-9317-dbd7af063759\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:27,578 INFO  SessionState:1227 - Hive Session ID = 7527f4f7-5a4b-4023-9317-dbd7af063759\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:28,043 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:29,096 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:30,601 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:31,619 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:34,812 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:34,813 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:34,956 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 36fa10b4-9500-4df4-897b-70e0d16b01a9\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:35,412 INFO  SessionState:1227 - Hive Session ID = 36fa10b4-9500-4df4-897b-70e0d16b01a9\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:36,826 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.test_db, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,147 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,162 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,460 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,560 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,628 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,662 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,663 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,664 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,678 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,679 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,693 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,694 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,705 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,706 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,714 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,714 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,722 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,722 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,723 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,724 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,754 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,754 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,758 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,758 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,762 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:598 - Saved output of task 'attempt__0000_m_000000_1626822923' to file:/tmp/hiverunner_test6335840382331203459/warehouse8312249657836664061/test_db.db/test_table/_SCRATCH0.9232010523916229\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,769 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,770 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,773 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:140 - File Output Committer Algorithm version is 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,773 INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:155 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,819 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:37,871 WARN  org.apache.hadoop.hive.conf.HiveConf:5220 - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:38,126 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:38,194 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:38,223 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.AnnotatedFieldsInSuperClassTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:38,280 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.769 s - in com.klarna.hiverunner.AnnotatedFieldsInSuperClassTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.BigResultSetTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:40,708 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 5158ffd3-d323-45ca-b426-fac3115479c8\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:41,677 INFO  SessionState:1227 - Hive Session ID = 5158ffd3-d323-45ca-b426-fac3115479c8\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:42,166 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:43,099 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:44,572 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:48,674 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:48,674 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:48,839 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 954bff6f-5f07-429a-a176-f6d105a57f67\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:49,401 INFO  SessionState:1227 - Hive Session ID = 954bff6f-5f07-429a-a176-f6d105a57f67\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:51,306 WARN  org.apache.hadoop.hive.metastore.HiveMetaStore:1849 - Location: file:/tmp/hiverunner_test5186254324662360249/hadooptmp4749917731069031601/foo specified for non-external table:FOO\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:51,908 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:51,965 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:52,033 INFO  com.klarna.hiverunner.HiveRunnerExtension:94 - Tearing down class com.klarna.hiverunner.BigResultSetTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:52,089 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.007 s - in com.klarna.hiverunner.BigResultSetTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.InteractiveHiveShellTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:53,895 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:53,912 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.InteractiveHiveShellTest in /\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 3645a834-cc91-41bf-8685-96bfa35954de\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:55,566 INFO  SessionState:1227 - Hive Session ID = 3645a834-cc91-41bf-8685-96bfa35954de\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:56,118 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:57,518 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:10:59,185 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:02,950 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9051 - Version information not found in metastore. metastore.schema.verification is not enabled so recording the schema version 3.1.0\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:02,950 WARN  org.apache.hadoop.hive.metastore.ObjectStore:9137 - setMetaStoreSchemaVersion called but recording version is disabled: version = 3.1.0, comment = Set by MetaStore UNKNOWN@127.0.1.1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:03,069 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 864344df-fcc4-4c17-a04f-e857c5728cc8\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:03,477 INFO  SessionState:1227 - Hive Session ID = 864344df-fcc4-4c17-a04f-e857c5728cc8\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:05,013 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.foo, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:05,063 INFO  hive.ql.exec.DDLTask:2790 - results : 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:05,086 INFO  org.apache.hadoop.conf.Configuration.deprecation:1391 - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:05,148 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:05,175 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.InteractiveHiveShellTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:05,229 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:05,235 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.InteractiveHiveShellTest in /\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 7e72b11a-1608-44bf-be23-9e4b4fe4e201\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:05,313 INFO  SessionState:1227 - Hive Session ID = 7e72b11a-1608-44bf-be23-9e4b4fe4e201\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:05,347 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:05,350 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:05,736 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:08,305 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 0306d527-7f0c-498a-8da4-9079d9f6b268\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:08,454 INFO  SessionState:1227 - Hive Session ID = 0306d527-7f0c-498a-8da4-9079d9f6b268\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:08,551 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.foo, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:08,862 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:08,865 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.InteractiveHiveShellTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:08,887 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:08,891 INFO  com.klarna.hiverunner.StandaloneHiveRunner:170 - Setting up com.klarna.hiverunner.InteractiveHiveShellTest in /\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = ad4b0ef3-198b-4ada-bd28-751131087de4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:08,933 INFO  SessionState:1227 - Hive Session ID = ad4b0ef3-198b-4ada-bd28-751131087de4\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:08,963 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:08,966 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:09,364 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:11,385 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.default, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 48020e28-4596-46be-95c3-5c470f20421d\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:11,532 INFO  SessionState:1227 - Hive Session ID = 48020e28-4596-46be-95c3-5c470f20421d\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:11,614 WARN  org.apache.hadoop.hive.metastore.ObjectStore:999 - Failed to get database hive.foo, returning NoSuchObjectException\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:11,799 INFO  hive.ql.exec.DDLTask:2790 - results : 2\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:11,823 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:11,862 INFO  org.apache.hadoop.mapred.FileInputFormat:256 - Total input files to process : 1\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:11,865 INFO  com.klarna.hiverunner.StandaloneHiveRunner:188 - Tearing down com.klarna.hiverunner.InteractiveHiveShellTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | OK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:11,881 INFO  com.klarna.hiverunner.HiveServerContainer:203 - Tore down HiveServer instance\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 18.067 s - in com.klarna.hiverunner.InteractiveHiveShellTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Class path contains multiple SLF4J bindings.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.10.0/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Found binding in [jar:file:/home/runneradmin/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | [INFO] Running com.klarna.hiverunner.SerdeTest\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:14,585 WARN  org.apache.hadoop.util.NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | Hive Session ID = 256587e8-d5a1-423d-a7ca-1d812f23eb2f\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:15,313 INFO  SessionState:1227 - Hive Session ID = 256587e8-d5a1-423d-a7ca-1d812f23eb2f\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:15,823 WARN  org.apache.hadoop.hive.ql.session.SessionState:950 - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:16,962 WARN  org.apache.hadoop.hive.metastore.ObjectStore:638 - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:18,657 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   | 2023-06-29T11:11:19,814 WARN  DataNucleus.MetaData:96 - Metadata has jdbc-type of null yet this is not valid. Ignored\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \u274c  Failure - Main Run Maven Targets\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests] Get \"http://%2Fvar%2Frun%2Fdocker.sock/v1.41/containers/1bef4d5d6d4625cd05b52f5eed3bf14f3078d1778f0ed428c19ac10ed40fbde7/archive?path=%2Fvar%2Frun%2Fact%2Fworkflow%2Fpathcmd.txt\": context canceled\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests] \u2b50 Run Post Set up JDK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \ud83d\udc33  docker exec cmd=[node /var/run/act/actions/actions-setup-java@v2/dist/cleanup/index.js] user= workdir=\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests]   \u2705  Success - Post Set up JDK\n[54e1625f-d57b-49b0-a5c9-d11ba920537a/Package and run all tests] \ud83c\udfc1  Job succeeded\n",
        "stderr": "Error: context canceled\n",
        "workflow": "/tmp/3d783ea4-1661-11ee-8a50-bb14de238602/HiveRunner-HiveRunner/.github/workflows/main-crawler.yml",
        "build_tool": "maven",
        "elapsed_time": 613.9691400527954
    }
}