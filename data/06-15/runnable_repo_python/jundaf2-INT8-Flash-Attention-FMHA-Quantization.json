{
    "repository": "jundaf2/INT8-Flash-Attention-FMHA-Quantization",
    "stars": 97,
    "language": "python",
    "size": 3269,
    "clone_url": "https://github.com/jundaf2/INT8-Flash-Attention-FMHA-Quantization.git",
    "timestamp": "2023-06-16T15:57:47.520790Z",
    "clone_success": true,
    "number of actions": 0,
    "number_of_test_actions": 0,
    "actions_successful": false,
    "number_of_actions": 0,
    "actions_build_tools": [],
    "actions_test_build_tools": []
}