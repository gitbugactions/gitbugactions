{
    "repository": "hyperopt/hyperopt-sklearn",
    "clone_url": "https://github.com/hyperopt/hyperopt-sklearn.git",
    "timestamp": "2023-05-29T14:42:37.787650Z",
    "clone_success": true,
    "number of actions": 1,
    "number_of_test_actions": 1,
    "actions_successful": false,
    "actions_stdout": "[Tests/test] \ud83d\ude80  Start image=crawlergpt:latest\n[Tests/test]   \ud83d\udc33  docker pull image=crawlergpt:latest platform= username= forcePull=false\n[Tests/test]   \ud83d\udc33  docker create image=crawlergpt:latest platform= entrypoint=[\"tail\" \"-f\" \"/dev/null\"] cmd=[]\n[Tests/test]   \ud83d\udc33  docker run image=crawlergpt:latest platform= entrypoint=[\"tail\" \"-f\" \"/dev/null\"] cmd=[]\n[Tests/test]   \ud83d\udc33  docker exec cmd=[chown -R 1012:1000 /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn] user=0 workdir=\n[Tests/test]   \u2601  git clone 'https://github.com/actions/setup-python' # ref=v2\n[Tests/test] \ud83e\uddea  Matrix: map[os:ubuntu-latest python-version:3.7]\n[Tests/test] \u2b50 Run Main actions/checkout@v2\n[Tests/test]   \u2705  Success - Main actions/checkout@v2\n[Tests/test] \u2b50 Run Main Set up Python 3.7\n[Tests/test]   \ud83d\udc33  docker cp src=/home/andre-silva/.cache/act/actions-setup-python@v2/ dst=/var/run/act/actions/actions-setup-python@v2/\n[Tests/test]   \ud83d\udc33  docker exec cmd=[chown -R 1012:1000 /var/run/act/actions/actions-setup-python@v2/] user=0 workdir=\n[Tests/test]   \ud83d\udc33  docker exec cmd=[node /var/run/act/actions/actions-setup-python@v2/dist/setup/index.js] user= workdir=\n[Tests/test]   \ud83d\udcac  ::debug::Semantic version spec of 3.7 is 3.7\n[Tests/test]   \ud83d\udcac  ::debug::isExplicit: \n[Tests/test]   \ud83d\udcac  ::debug::explicit? false\n[Tests/test]   \ud83d\udcac  ::debug::isExplicit: 2.7.18\n[Tests/test]   \ud83d\udcac  ::debug::explicit? true\n[Tests/test]   \ud83d\udcac  ::debug::isExplicit: 3.5.10\n[Tests/test]   \ud83d\udcac  ::debug::explicit? true\n[Tests/test]   \ud83d\udcac  ::debug::isExplicit: 3.6.14\n[Tests/test]   \ud83d\udcac  ::debug::explicit? true\n[Tests/test]   \ud83d\udcac  ::debug::isExplicit: 3.7.11\n[Tests/test]   \ud83d\udcac  ::debug::explicit? true\n[Tests/test]   \ud83d\udcac  ::debug::isExplicit: 3.8.11\n[Tests/test]   \ud83d\udcac  ::debug::explicit? true\n[Tests/test]   \ud83d\udcac  ::debug::isExplicit: 3.9.6\n[Tests/test]   \ud83d\udcac  ::debug::explicit? true\n[Tests/test]   \ud83d\udcac  ::debug::evaluating 6 versions\n[Tests/test]   \ud83d\udcac  ::debug::matched: 3.7.11\n[Tests/test]   \ud83d\udcac  ::debug::checking cache: /opt/hostedtoolcache/Python/3.7.11/x64\n[Tests/test]   \ud83d\udcac  ::debug::Found tool in cache Python 3.7.11 x64\n[Tests/test]   | Successfully setup CPython (3.7.11)\n[Tests/test]   \u2753 add-matcher /run/act/actions/actions-setup-python@v2/.github/python.json\n[Tests/test]   \u2705  Success - Main Set up Python 3.7\n[Tests/test]   \u2699  ::set-env:: pythonLocation=/opt/hostedtoolcache/Python/3.7.11/x64\n[Tests/test]   \u2699  ::set-env:: LD_LIBRARY_PATH=/opt/hostedtoolcache/Python/3.7.11/x64/lib\n[Tests/test]   \u2699  ::set-output:: python-version=3.7.11\n[Tests/test]   \u2699  ::add-path:: /opt/hostedtoolcache/Python/3.7.11/x64\n[Tests/test]   \u2699  ::add-path:: /opt/hostedtoolcache/Python/3.7.11/x64/bin\n[Tests/test] \u2b50 Run Main Install dependencies\n[Tests/test]   \ud83d\udc33  docker exec cmd=[bash --noprofile --norc -e -o pipefail /var/run/act/workflow/2] user= workdir=\n[Tests/test]   | Requirement already satisfied: pip in /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages (21.2.4)\n[Tests/test]   | Collecting pip\n[Tests/test]   |   Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)\n[Tests/test]   | Installing collected packages: pip\n[Tests/test]   |   Attempting uninstall: pip\n[Tests/test]   |     Found existing installation: pip 21.2.4\n[Tests/test]   |     Uninstalling pip-21.2.4:\n[Tests/test]   |       Successfully uninstalled pip-21.2.4\n[Tests/test]   | Successfully installed pip-23.1.2\n[Tests/test]   | Collecting tox\n[Tests/test]   |   Downloading tox-4.5.2-py3-none-any.whl (148 kB)\n[Tests/test]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 149.0/149.0 kB 18.6 MB/s eta 0:00:00\n[Tests/test]   | Collecting tox-gh-actions\n[Tests/test]   |   Downloading tox_gh_actions-3.1.1-py2.py3-none-any.whl (9.8 kB)\n[Tests/test]   | Collecting cachetools>=5.3 (from tox)\n[Tests/test]   |   Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n[Tests/test]   | Collecting chardet>=5.1 (from tox)\n[Tests/test]   |   Downloading chardet-5.1.0-py3-none-any.whl (199 kB)\n[Tests/test]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.1/199.1 kB 23.0 MB/s eta 0:00:00\n[Tests/test]   | Collecting colorama>=0.4.6 (from tox)\n[Tests/test]   |   Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n[Tests/test]   | Collecting filelock>=3.12 (from tox)\n[Tests/test]   |   Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n[Tests/test]   | Collecting importlib-metadata>=6.6 (from tox)\n[Tests/test]   |   Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n[Tests/test]   | Collecting packaging>=23.1 (from tox)\n[Tests/test]   |   Downloading packaging-23.1-py3-none-any.whl (48 kB)\n[Tests/test]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 48.9/48.9 kB 4.0 MB/s eta 0:00:00\n[Tests/test]   | Collecting platformdirs>=3.5.1 (from tox)\n[Tests/test]   |   Downloading platformdirs-3.5.1-py3-none-any.whl (15 kB)\n[Tests/test]   | Collecting pluggy>=1 (from tox)\n[Tests/test]   |   Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n[Tests/test]   | Collecting pyproject-api>=1.5.1 (from tox)\n[Tests/test]   |   Downloading pyproject_api-1.5.1-py3-none-any.whl (12 kB)\n[Tests/test]   | Collecting tomli>=2.0.1 (from tox)\n[Tests/test]   |   Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n[Tests/test]   | Collecting typing-extensions>=4.6.2 (from tox)\n[Tests/test]   |   Downloading typing_extensions-4.6.2-py3-none-any.whl (31 kB)\n[Tests/test]   | Collecting virtualenv>=20.23 (from tox)\n[Tests/test]   |   Downloading virtualenv-20.23.0-py3-none-any.whl (3.3 MB)\n[Tests/test]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.3/3.3 MB 31.2 MB/s eta 0:00:00\n[Tests/test]   | Collecting zipp>=0.5 (from importlib-metadata>=6.6->tox)\n[Tests/test]   |   Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n[Tests/test]   | Collecting distlib<1,>=0.3.6 (from virtualenv>=20.23->tox)\n[Tests/test]   |   Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n[Tests/test]   |      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 468.5/468.5 kB 23.3 MB/s eta 0:00:00\n[Tests/test]   | Installing collected packages: distlib, zipp, typing-extensions, tomli, packaging, filelock, colorama, chardet, cachetools, pyproject-api, platformdirs, importlib-metadata, virtualenv, pluggy, tox, tox-gh-actions\n[Tests/test]   | Successfully installed cachetools-5.3.1 chardet-5.1.0 colorama-0.4.6 distlib-0.3.6 filelock-3.12.0 importlib-metadata-6.6.0 packaging-23.1 platformdirs-3.5.1 pluggy-1.0.0 pyproject-api-1.5.1 tomli-2.0.1 tox-4.5.2 tox-gh-actions-3.1.1 typing-extensions-4.6.2 virtualenv-20.23.0 zipp-3.15.0\n[Tests/test]   | \n[Tests/test]   | [notice] A new release of pip is available: 20.1.1 -> 23.1.2\n[Tests/test]   | [notice] To update, run: pip install --upgrade pip\n[Tests/test]   \u2705  Success - Main Install dependencies\n[Tests/test] \u2b50 Run Main Test with tox\n[Tests/test]   \ud83d\udc33  docker exec cmd=[bash --noprofile --norc -e -o pipefail /var/run/act/workflow/3] user= workdir=\n[Tests/test]   | ROOT: tox-gh-actions couldn't understand the parallel option. ignoring the given option: 0\n[Tests/test]   | py37: install_deps> python -I -m pip install -r /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/requirements_dev.txt\n[Tests/test]   | .pkg: install_requires> python -I -m pip install 'setuptools>=58.2.0' wheel\n[Tests/test]   | .pkg: _optional_hooks> python /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/pyproject_api/_backend.py True setuptools.build_meta\n[Tests/test]   | .pkg: get_requires_for_build_sdist> python /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/pyproject_api/_backend.py True setuptools.build_meta\n[Tests/test]   | .pkg: freeze> python -m pip freeze --all\n[Tests/test]   | .pkg: pip==23.1.2,setuptools==67.7.2,wheel==0.40.0\n[Tests/test]   | .pkg: prepare_metadata_for_build_wheel> python /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/pyproject_api/_backend.py True setuptools.build_meta\n[Tests/test]   | .pkg: build_sdist> python /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/pyproject_api/_backend.py True setuptools.build_meta\n[Tests/test]   | py37: install_package_deps> python -I -m pip install 'hyperopt>=0.2.6' 'numpy>=1.21.2' 'scikit-learn>=1.0' 'scipy>=1.7.1'\n[Tests/test]   | py37: install_package> python -I -m pip install --force-reinstall --no-deps /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/.tmp/package/1/hpsklearn-1.0.3.tar.gz\n[Tests/test]   | py37: freeze> python -m pip freeze --all\n[Tests/test]   | py37: cloudpickle==2.2.1,coverage==6.2,distlib==0.3.6,filelock==3.12.0,future==0.18.3,hpsklearn==1.0.3,hyperopt==0.2.7,importlib-metadata==6.6.0,joblib==1.2.0,lightgbm==3.3.5,networkx==2.6.3,numpy==1.21.6,packaging==23.1,pandas==1.3.5,pip==23.1.2,platformdirs==3.5.1,pluggy==1.0.0,py==1.11.0,py4j==0.10.9.7,python-dateutil==2.8.2,pytz==2023.3,scikit-learn==1.0.2,scipy==1.7.3,setuptools==67.7.2,six==1.16.0,threadpoolctl==3.1.0,toml==0.10.2,tox==3.24.5,tqdm==4.65.0,typing_extensions==4.6.2,virtualenv==20.23.0,wheel==0.40.0,xgboost==1.6.2,zipp==3.15.0\n[Tests/test]   | py37: tox-gh-actions couldn't understand the parallel option. ignoring the given option: 0\n[Tests/test]   \u2753  ::group::tox: py37\n[Tests/test]   | py37: commands[0]> coverage run -m unittest discover\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.67s/trial, best loss: 3.31958762886598]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.67s/trial, best loss: 3.31958762886598]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:03<00:00,  3.91s/trial, best loss: 3.31958762886598]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:03<00:00,  3.91s/trial, best loss: 3.31958762886598]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:01<00:00,  1.85s/trial, best loss: 3.31958762886598]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:01<00:00,  1.86s/trial, best loss: 3.31958762886598]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:03<00:00,  3.51s/trial, best loss: 3.31958762886598]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:03<00:00,  3.51s/trial, best loss: 3.31958762886598]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:02<00:00,  2.19s/trial, best loss: 3.31958762886598]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:02<00:00,  2.19s/trial, best loss: 3.31958762886598]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:10<00:00, 10.06s/trial, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:10<00:00, 10.06s/trial, best loss=?]\n[Tests/test]   | \n[Tests/test]   | ---\n[Tests/test]   | AllTrialsFailed was raised, np.isnan(t['result']['loss']) is True.\n[Tests/test]   | ---\n[Tests/test]   | \n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 33.13trial/s, best loss: 0.0]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 44.77trial/s, best loss: 0.0]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 46.46trial/s, best loss: 0.0]\n[Tests/test]   | .\r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 64.37trial/s, best loss: 0.0]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 66.10trial/s, best loss: 0.0]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 11.48trial/s, best loss: 0.8260869565217391]\n[Tests/test]   | .\r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 10.66trial/s, best loss: 0.8260869565217391]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  6.54trial/s, best loss: 0.8260869565217391]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  6.52trial/s, best loss: 0.8260869565217391]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  6.54trial/s, best loss: 0.8260869565217391]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  6.52trial/s, best loss: 0.8260869565217391]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 10.76trial/s, best loss: 0.8260869565217391]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 35.45trial/s, best loss: 0.0]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 41.53trial/s, best loss: 0.0]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 51.21trial/s, best loss: 0.0]\n[Tests/test]   | .\r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 50.80trial/s, best loss: 0.0]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 55.52trial/s, best loss: 0.0]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 60.23trial/s, best loss: 0.0005437512887879192]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 48.95trial/s, best loss: 0.0005437512887879192]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 45.14trial/s, best loss: 0.0005437512887879192]\n[Tests/test]   | .\r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 44.00trial/s, best loss: 0.0005437512887879192]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 37.13trial/s, best loss: 0.0005437512887879192]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 58.38trial/s, best loss: 0.0005458866967493758]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 57.61trial/s, best loss: 0.0005458866967493758]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 63.15trial/s, best loss: 0.0005458866967493758]\n[Tests/test]   | .\r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 58.69trial/s, best loss: 0.0005458866967493758]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 40.77trial/s, best loss: 0.0]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 77.29trial/s, best loss: 0.08999999999999997]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 53.81trial/s, best loss: 0.08999999999999997]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 44.48trial/s, best loss: 0.08999999999999997]\n[Tests/test]   | .\r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 43.79trial/s, best loss: 0.08999999999999997]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 53.60trial/s, best loss: 0.08999999999999997]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 44.75trial/s, best loss: 0.0]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 60.24trial/s, best loss: 0.0]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 43.57trial/s, best loss: 0.0]\n[Tests/test]   | .\r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 47.37trial/s, best loss: 0.0]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 38.86trial/s, best loss: 0.0]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 65.91trial/s, best loss: 0.0050000000000000044]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 55.46trial/s, best loss: 0.0050000000000000044]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 42.36trial/s, best loss: 0.0050000000000000044]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 59.05trial/s, best loss: 0.0050000000000000044]\n[Tests/test]   | .\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 54.26trial/s, best loss: 0.0]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 47.61trial/s, best loss: 0.43999999999999995]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 46.79trial/s, best loss: 0.43999999999999995]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 56.46trial/s, best loss: 0.43999999999999995]\n[Tests/test]   | .\r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 58.44trial/s, best loss: 0.43999999999999995]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 64.43trial/s, best loss: 0.43999999999999995]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 46.53trial/s, best loss: 1.026713947786368]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 52.25trial/s, best loss: 1.026713947786368]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 72.80trial/s, best loss: 1.026713947786368]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 55.07trial/s, best loss: 1.026713947786368]\n[Tests/test]   | .\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 55.15trial/s, best loss: 1.026713947786368]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 20.39trial/s, best loss: 0.0]\n[Tests/test]   | .\r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 18.98trial/s, best loss: 0.0]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 21.82trial/s, best loss: 0.0]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 20.22trial/s, best loss: 0.0]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 25.82trial/s, best loss: 0.0]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 15.41trial/s, best loss: 0.35547736788061757]\n[Tests/test]   | .\r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 20.06trial/s, best loss: 0.004570253487704901]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 21.23trial/s, best loss: 0.004570253487704901]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 21.41trial/s, best loss: 0.004570253487704901]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 16.42trial/s, best loss: 0.002921682868432529]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.62trial/s, best loss: 0.015000000000000013]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.61trial/s, best loss: 0.015000000000000013]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02<00:00,  2.39s/trial, best loss: 0.015000000000000013]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02<00:00,  2.39s/trial, best loss: 0.015000000000000013]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  3.12trial/s, best loss: 0.015000000000000013]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  3.12trial/s, best loss: 0.015000000000000013]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 11.07trial/s, best loss: 0.015000000000000013]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:05<00:00,  5.31s/trial, best loss: 0.0050000000000000044]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:05<00:00,  5.31s/trial, best loss: 0.0050000000000000044]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.27s/trial, best loss: 0.0]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.27s/trial, best loss: 0.0]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  3.69trial/s, best loss: 0.0]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  3.68trial/s, best loss: 0.0]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  3.00trial/s, best loss: 0.0]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  2.99trial/s, best loss: 0.0]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  1.49trial/s, best loss: 0.0]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  1.48trial/s, best loss: 0.0]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,  1.11s/trial, best loss: 0.0]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,  1.11s/trial, best loss: 0.0]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:05<00:00,  5.50s/trial, best loss: 0.013837637461939667]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:05<00:00,  5.50s/trial, best loss: 0.013837637461939667]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  8.15trial/s, best loss: 0.013837637461939667]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  8.10trial/s, best loss: 0.013837637461939667]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 14.92trial/s, best loss: 0.013837637461939667]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  9.54trial/s, best loss: 0.013502682739294891]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  9.47trial/s, best loss: 0.013502682739294891]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,  1.70s/trial, best loss: 0.013502682739294891]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,  1.70s/trial, best loss: 0.013502682739294891]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.05s/trial, best loss: 0.6425125692596959]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.05s/trial, best loss: 0.6425125692596959]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02<00:00,  2.45s/trial, best loss: 0.6425125692596959]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02<00:00,  2.45s/trial, best loss: 0.6425125692596959]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  7.46trial/s, best loss: 0.6425125692596959]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  7.43trial/s, best loss: 0.6425125692596959]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  8.51trial/s, best loss: 0.1752670839285636]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  8.45trial/s, best loss: 0.1752670839285636]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  5.92trial/s, best loss: 0.1752670839285636]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  5.89trial/s, best loss: 0.1752670839285636]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.28trial/s, best loss: 0.01412288855728061]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.27trial/s, best loss: 0.01412288855728061]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  2.98trial/s, best loss: 0.01412288855728061]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  2.98trial/s, best loss: 0.01412288855728061]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  3.13trial/s, best loss: 0.01412288855728061]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  3.13trial/s, best loss: 0.01412288855728061]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:01<00:00,  1.87s/trial, best loss: 0.01412288855728061]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:01<00:00,  1.87s/trial, best loss: 0.01412288855728061]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,  1.52s/trial, best loss: 0.01412288855728061]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,  1.53s/trial, best loss: 0.01412288855728061]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 21.26trial/s, best loss: 0.0]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  2.01trial/s, best loss: 0.0]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  2.01trial/s, best loss: 0.0]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 11.04trial/s, best loss: 0.0]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 14.02trial/s, best loss: 0.0]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  9.99trial/s, best loss: 0.0]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  9.92trial/s, best loss: 0.0]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.44trial/s, best loss: 0.4927339831525943]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.44trial/s, best loss: 0.4927339831525943]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  2.09trial/s, best loss: 0.12558607703491065]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  2.09trial/s, best loss: 0.12558607703491065]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  2.22trial/s, best loss: 0.00878785713477992]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  2.21trial/s, best loss: 0.00878785713477992]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:05<00:00,  5.47s/trial, best loss: 0.00878785713477992]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:05<00:00,  5.47s/trial, best loss: 0.00878785713477992]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  1.61trial/s, best loss: 0.00878785713477992]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  1.61trial/s, best loss: 0.00878785713477992]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:10<00:00, 10.02s/trial, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:10<00:00, 10.03s/trial, best loss=?]\n[Tests/test]   | \n[Tests/test]   | ---\n[Tests/test]   | AllTrialsFailed was raised, np.isnan(t['result']['loss']) is True.\n[Tests/test]   | ---\n[Tests/test]   | \n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:10<00:00, 10.02s/trial, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:10<00:00, 10.02s/trial, best loss=?]\n[Tests/test]   | \n[Tests/test]   | ---\n[Tests/test]   | AllTrialsFailed was raised, np.isnan(t['result']['loss']) is True.\n[Tests/test]   | ---\n[Tests/test]   | \n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:10<00:00, 10.07s/trial, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:10<00:00, 10.07s/trial, best loss=?]\n[Tests/test]   | \n[Tests/test]   | ---\n[Tests/test]   | AllTrialsFailed was raised, np.isnan(t['result']['loss']) is True.\n[Tests/test]   | ---\n[Tests/test]   | \n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?].\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.17s/trial, best loss: 2.690366972477064]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.17s/trial, best loss: 2.690366972477064]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02<00:00,  2.25s/trial, best loss: 2.690366972477064]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02<00:00,  2.25s/trial, best loss: 2.690366972477064]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  4.28trial/s, best loss: 2.690366972477064]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  4.26trial/s, best loss: 2.690366972477064]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  2.85trial/s, best loss: 2.690366972477064]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  2.85trial/s, best loss: 2.690366972477064]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:05<00:00,  5.81s/trial, best loss: 2.690366972477064]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:05<00:00,  5.81s/trial, best loss: 2.690366972477064]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 38.44trial/s, best loss: 0.0]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 26.78trial/s, best loss: 0.0]\n[Tests/test]   | .\r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 30.03trial/s, best loss: 0.0]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 32.21trial/s, best loss: 0.0]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 28.59trial/s, best loss: 0.0]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.78s/trial, best loss: 0.013800688261667804]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.78s/trial, best loss: 0.013800688261667804]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  1.35trial/s, best loss: 0.013800688261667804]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  1.35trial/s, best loss: 0.013800688261667804]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  5.44trial/s, best loss: 0.013800688261667804]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  5.42trial/s, best loss: 0.013800688261667804]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:01<00:00,  1.18s/trial, best loss: 0.013800688261667804]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:01<00:00,  1.18s/trial, best loss: 0.013800688261667804]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,  1.46s/trial, best loss: 0.008160379665727024]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,  1.46s/trial, best loss: 0.008160379665727024]\n[Tests/test]   | ./tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:539: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n[Tests/test]   |   \"The parameter 'stop_words' will not be used\"\n[Tests/test]   | \n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 26.24trial/s, best loss: 0.0]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 33.58trial/s, best loss: 0.0]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 30.68trial/s, best loss: 0.0]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 29.44trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:539: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n[Tests/test]   |   \"The parameter 'stop_words' will not be used\"\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:539: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n[Tests/test]   |   \"The parameter 'stop_words' will not be used\"\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 26.42trial/s, best loss: 0.0]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.49trial/s, best loss: 0.0050000000000000044]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.48trial/s, best loss: 0.0050000000000000044]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  1.69trial/s, best loss: 0.0050000000000000044]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  1.69trial/s, best loss: 0.0050000000000000044]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  1.32trial/s, best loss: 0.0050000000000000044]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  1.31trial/s, best loss: 0.0050000000000000044]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  1.84trial/s, best loss: 0.0050000000000000044]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  1.83trial/s, best loss: 0.0050000000000000044]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  1.98trial/s, best loss: 0.0050000000000000044]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  1.98trial/s, best loss: 0.0050000000000000044]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.82trial/s, best loss: 0.001318727517463314]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.79trial/s, best loss: 0.001318727517463314]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  6.35trial/s, best loss: 0.001318727517463314]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  6.32trial/s, best loss: 0.001318727517463314]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  7.17trial/s, best loss: 0.00024275687561159387]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  7.13trial/s, best loss: 0.00024275687561159387]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  6.36trial/s, best loss: 0.00024275687561159387]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  6.33trial/s, best loss: 0.00024275687561159387]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  5.39trial/s, best loss: 0.00024275687561159387]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  5.37trial/s, best loss: 0.00024275687561159387]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]/tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:197: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n[Tests/test]   |   \"Singular matrix in solving dual problem. Using \"\n[Tests/test]   | \n[Tests/test]   | \r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.67trial/s, best loss: 0.9176752162188349]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.67trial/s, best loss: 0.9176752162188349]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]/tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:197: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n[Tests/test]   |   \"Singular matrix in solving dual problem. Using \"\n[Tests/test]   | \n[Tests/test]   | \r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  2.05trial/s, best loss: 0.9176752162188349]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  2.05trial/s, best loss: 0.9176752162188349]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]/tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:197: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n[Tests/test]   |   \"Singular matrix in solving dual problem. Using \"\n[Tests/test]   | \n[Tests/test]   | \r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  1.96trial/s, best loss: 0.9176752162188349]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  1.95trial/s, best loss: 0.9176752162188349]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  6.54trial/s, best loss: 0.9176752162188349]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  6.51trial/s, best loss: 0.9176752162188349]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]/tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:197: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n[Tests/test]   |   \"Singular matrix in solving dual problem. Using \"\n[Tests/test]   | \n[Tests/test]   | \r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  2.32trial/s, best loss: 0.8552735158557039]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  2.32trial/s, best loss: 0.8552735158557039]/tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:197: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n[Tests/test]   |   \"Singular matrix in solving dual problem. Using \"\n[Tests/test]   | \n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r                                                     \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r                                                     \rFitting LagSelector(lag_size=3) to X of shape (552, 10)\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTransforming Xfit (552, 10)\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTransforming Xval (138, 10)\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTraining learner LinearSVR(C=1.0353727999735856, epsilon=0, intercept_scaling=1.46706420995972,\n[Tests/test]   |           loss='squared_epsilon_insensitive', max_iter=100000.0, random_state=0,\n[Tests/test]   |           tol=0.0001035960067690352) on X/EX of dimension (552, 6)\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r                                                     \rScoring on X/EX validation of shape (138, 6)\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r                                                     \rOK trial with R2 score 6.83e-01\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.47trial/s, best loss: 0.316766181318084]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.41trial/s, best loss: 0.316766181318084]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r                                                     \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r                                                     \rFitting LagSelector(lag_size=4) to X of shape (552, 10)\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTransforming Xval (138, 10)\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTraining learner LinearSVR(C=1.3353565246605847, epsilon=0, intercept_scaling=1.4220432656431763,\n[Tests/test]   |           loss='squared_epsilon_insensitive', max_iter=100000.0, random_state=0,\n[Tests/test]   |           tol=0.00018070204723304384) on X/EX of dimension (552, 7)\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r                                                     \rScoring on X/EX validation of shape (138, 7)\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r                                                     \rOK trial with R2 score 6.84e-01\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  5.23trial/s, best loss: 0.31603589729408466]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  5.20trial/s, best loss: 0.31603589729408466]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r                                                     \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r                                                     \rFitting LagSelector(lag_size=10) to X of shape (552, 10)\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTransforming Xval (138, 10)\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTraining learner LinearSVR(C=0.9517061574600952, epsilon=0, intercept_scaling=0.924532100866232,\n[Tests/test]   |           max_iter=100000.0, random_state=2, tol=0.00030362539392211386) on X/EX of dimension (552, 13)\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r                                                     \rScoring on X/EX validation of shape (138, 13)\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r                                                     \rOK trial with R2 score 6.79e-01\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 13.46trial/s, best loss: 0.31603589729408466]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r                                                     \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r                                                     \rFitting LagSelector(lag_size=6) to X of shape (552, 10)\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTransforming Xval (138, 10)\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTraining learner LinearSVR(C=0.6186304411037259, epsilon=0, intercept_scaling=0.76777329208806,\n[Tests/test]   |           loss='squared_epsilon_insensitive', max_iter=100000.0, random_state=4,\n[Tests/test]   |           tol=0.00010228603301690119) on X/EX of dimension (552, 9)\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r                                                     \rScoring on X/EX validation of shape (138, 9)\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r                                                     \rOK trial with R2 score 6.83e-01\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  7.64trial/s, best loss: 0.31603589729408466]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  7.59trial/s, best loss: 0.31603589729408466]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r                                                     \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r                                                     \rFitting LagSelector(lag_size=9) to X of shape (552, 10)\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTransforming Xval (138, 10)\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTraining learner LinearSVR(C=0.6446601902982465, epsilon=0, intercept_scaling=1.4588673598291306,\n[Tests/test]   |           max_iter=100000.0, random_state=4, tol=0.00016081341008279283) on X/EX of dimension (552, 12)\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r                                                     \rScoring on X/EX validation of shape (138, 12)\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r                                                     \rOK trial with R2 score 6.77e-01\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 17.35trial/s, best loss: 0.31603589729408466]\n[Tests/test]   | \r 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 5/6 [00:00<?, ?trial/s, best loss=?]\r                                                     \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 5/6 [00:00<?, ?trial/s, best loss=?]\r                                                     \rFitting LagSelector(lag_size=2) to X of shape (552, 10)\n[Tests/test]   | \r 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 5/6 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 5/6 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTransforming Xval (138, 10)\n[Tests/test]   | \r 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 5/6 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTraining learner LinearSVR(C=1.207172273961305, epsilon=0, intercept_scaling=0.9167366792923904,\n[Tests/test]   |           max_iter=100000.0, random_state=4, tol=4.9768988714229583e-05) on X/EX of dimension (552, 5)\n[Tests/test]   | \r 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 5/6 [00:00<?, ?trial/s, best loss=?]\r                                                     \rScoring on X/EX validation of shape (138, 5)\n[Tests/test]   | \r 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 5/6 [00:00<?, ?trial/s, best loss=?]\r                                                     \rOK trial with R2 score 6.77e-01\n[Tests/test]   | \r 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 5/6 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:00<00:00, 17.86trial/s, best loss: 0.31603589729408466]\n[Tests/test]   | \r 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6/7 [00:00<?, ?trial/s, best loss=?]\r                                                     \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6/7 [00:00<?, ?trial/s, best loss=?]\r                                                     \rFitting LagSelector(lag_size=4) to X of shape (552, 10)\n[Tests/test]   | \r 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6/7 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6/7 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTransforming Xval (138, 10)\n[Tests/test]   | \r 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6/7 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTraining learner LinearSVR(C=0.693060913041244, epsilon=0, intercept_scaling=1.2657830735343034,\n[Tests/test]   |           max_iter=100000.0, random_state=0, tol=1.4723647810467368e-05) on X/EX of dimension (552, 7)\n[Tests/test]   | \r 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6/7 [00:00<?, ?trial/s, best loss=?]\r                                                     \rScoring on X/EX validation of shape (138, 7)\n[Tests/test]   | \r 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6/7 [00:00<?, ?trial/s, best loss=?]\r                                                     \rOK trial with R2 score 6.85e-01\n[Tests/test]   | \r 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6/7 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:00<00:00, 17.10trial/s, best loss: 0.3150352735155426]\n[Tests/test]   | \r 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 7/8 [00:00<?, ?trial/s, best loss=?]\r                                                     \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 7/8 [00:00<?, ?trial/s, best loss=?]\r                                                     \rFitting LagSelector(lag_size=6) to X of shape (552, 10)\n[Tests/test]   | \r 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 7/8 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 7/8 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTransforming Xval (138, 10)\n[Tests/test]   | \r 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 7/8 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTraining learner LinearSVR(C=0.687419039757772, epsilon=0, intercept_scaling=0.6512107933951883,\n[Tests/test]   |           loss='squared_epsilon_insensitive', max_iter=100000.0, random_state=0,\n[Tests/test]   |           tol=0.004860102484515389) on X/EX of dimension (552, 9)\n[Tests/test]   | \r 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 7/8 [00:00<?, ?trial/s, best loss=?]\r                                                     \rScoring on X/EX validation of shape (138, 9)\n[Tests/test]   | \r 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 7/8 [00:00<?, ?trial/s, best loss=?]\r                                                     \rOK trial with R2 score 6.83e-01\n[Tests/test]   | \r 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 7/8 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:00<00:00, 11.44trial/s, best loss: 0.3150352735155426]\n[Tests/test]   | \r 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 8/9 [00:00<?, ?trial/s, best loss=?]\r                                                     \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 8/9 [00:00<?, ?trial/s, best loss=?]\r                                                     \rFitting LagSelector(lag_size=4) to X of shape (552, 10)\n[Tests/test]   | \r 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 8/9 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 8/9 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTransforming Xval (138, 10)\n[Tests/test]   | \r 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 8/9 [00:00<?, ?trial/s, best loss=?]\r                                                     \rTraining learner LinearSVR(C=1.3523782424339634, epsilon=0, intercept_scaling=0.9674580244282601,\n[Tests/test]   |           max_iter=100000.0, random_state=1, tol=0.00243516485688368) on X/EX of dimension (552, 7)\n[Tests/test]   | \r 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 8/9 [00:00<?, ?trial/s, best loss=?]\r                                                     \rScoring on X/EX validation of shape (138, 7)\n[Tests/test]   | \r 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 8/9 [00:00<?, ?trial/s, best loss=?]\r                                                     \rOK trial with R2 score 6.81e-01\n[Tests/test]   | \r 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 8/9 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9/9 [00:00<00:00, 19.20trial/s, best loss: 0.3150352735155426]\n[Tests/test]   | \r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:00<?, ?trial/s, best loss=?]\r                                                      \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:00<?, ?trial/s, best loss=?]\r                                                      \rFitting LagSelector(lag_size=5) to X of shape (552, 10)\n[Tests/test]   | \r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:00<?, ?trial/s, best loss=?]\r                                                      \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:00<?, ?trial/s, best loss=?]\r                                                      \rTransforming Xval (138, 10)\n[Tests/test]   | \r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:00<?, ?trial/s, best loss=?]\r                                                      \rTraining learner LinearSVR(C=0.6639293470853301, epsilon=0, intercept_scaling=0.769507783720102,\n[Tests/test]   |           max_iter=100000.0, random_state=0, tol=0.0007249905235581015) on X/EX of dimension (552, 8)\n[Tests/test]   | \r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:00<?, ?trial/s, best loss=?]\r                                                      \rScoring on X/EX validation of shape (138, 8)\n[Tests/test]   | \r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:00<?, ?trial/s, best loss=?]\r                                                      \rOK trial with R2 score 6.75e-01\n[Tests/test]   | \r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00<00:00, 19.98trial/s, best loss: 0.3150352735155426]\n[Tests/test]   | \r 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 10/11 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 10/11 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=2) to X of shape (552, 10)\n[Tests/test]   | \r 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 10/11 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 10/11 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 10/11 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=0.6920893361825122, epsilon=0, intercept_scaling=1.3312991130186123,\n[Tests/test]   |           max_iter=100000.0, random_state=1, tol=0.00018962648266863906) on X/EX of dimension (552, 5)\n[Tests/test]   | \r 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 10/11 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 5)\n[Tests/test]   | \r 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 10/11 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.77e-01\n[Tests/test]   | \r 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 10/11 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:00<00:00, 20.48trial/s, best loss: 0.3150352735155426]\n[Tests/test]   | \r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 11/12 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 11/12 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=2) to X of shape (552, 10)\n[Tests/test]   | \r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 11/12 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 11/12 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 11/12 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=0.9792049739773547, epsilon=0, intercept_scaling=0.6494117603648957,\n[Tests/test]   |           max_iter=100000.0, random_state=1, tol=4.323892005450673e-05) on X/EX of dimension (552, 5)\n[Tests/test]   | \r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 11/12 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 5)\n[Tests/test]   | \r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 11/12 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.78e-01\n[Tests/test]   | \r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 11/12 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:00<00:00, 17.53trial/s, best loss: 0.3150352735155426]\n[Tests/test]   | \r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 12/13 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 12/13 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=6) to X of shape (552, 10)\n[Tests/test]   | \r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 12/13 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 12/13 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 12/13 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=0.679730644577278, epsilon=0, intercept_scaling=0.9203185492996027,\n[Tests/test]   |           loss='squared_epsilon_insensitive', max_iter=100000.0, random_state=4,\n[Tests/test]   |           tol=6.471265091278903e-05) on X/EX of dimension (552, 9)\n[Tests/test]   | \r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 12/13 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 9)\n[Tests/test]   | \r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 12/13 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.83e-01\n[Tests/test]   | \r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 12/13 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00<00:00,  7.12trial/s, best loss: 0.3150352735155426]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00<00:00,  7.09trial/s, best loss: 0.3150352735155426]\n[Tests/test]   | \r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 13/14 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 13/14 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=3) to X of shape (552, 10)\n[Tests/test]   | \r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 13/14 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 13/14 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 13/14 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=0.6225004440795248, epsilon=0, intercept_scaling=1.097716508287554,\n[Tests/test]   |           loss='squared_epsilon_insensitive', max_iter=100000.0, random_state=3,\n[Tests/test]   |           tol=0.0032480802362375924) on X/EX of dimension (552, 6)\n[Tests/test]   | \r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 13/14 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 6)\n[Tests/test]   | \r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 13/14 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.85e-01\n[Tests/test]   | \r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 13/14 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [00:00<00:00, 14.71trial/s, best loss: 0.3150352735155426]\n[Tests/test]   | \r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 14/15 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 14/15 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=8) to X of shape (552, 10)\n[Tests/test]   | \r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 14/15 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 14/15 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 14/15 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=0.6419031736057713, epsilon=0, intercept_scaling=1.4931134124162577,\n[Tests/test]   |           max_iter=100000.0, random_state=4, tol=0.00037919543792325937) on X/EX of dimension (552, 11)\n[Tests/test]   | \r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 14/15 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 11)\n[Tests/test]   | \r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 14/15 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.79e-01\n[Tests/test]   | \r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 14/15 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15/15 [00:00<00:00, 18.47trial/s, best loss: 0.3150352735155426]\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 15/16 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 15/16 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=7) to X of shape (552, 10)\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 15/16 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 15/16 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 15/16 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=1.2190239102722435, epsilon=0, intercept_scaling=1.232604070745241,\n[Tests/test]   |           max_iter=100000.0, random_state=2, tol=0.0006441855959802144) on X/EX of dimension (552, 10)\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 15/16 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 10)\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 15/16 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.78e-01\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 15/16 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:00<00:00, 16.75trial/s, best loss: 0.3150352735155426]\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 16/17 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 16/17 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=5) to X of shape (552, 10)\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 16/17 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 16/17 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 16/17 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=0.7289856527424524, epsilon=0, intercept_scaling=1.4086960650931766,\n[Tests/test]   |           max_iter=100000.0, random_state=1, tol=0.003529893314127064) on X/EX of dimension (552, 8)\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 16/17 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 8)\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 16/17 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.80e-01\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 16/17 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17/17 [00:00<00:00, 21.37trial/s, best loss: 0.3150352735155426]\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 17/18 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 17/18 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=6) to X of shape (552, 10)\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 17/18 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 17/18 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 17/18 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=1.435443697970113, epsilon=0, intercept_scaling=1.1733075876587875,\n[Tests/test]   |           loss='squared_epsilon_insensitive', max_iter=100000.0, random_state=2,\n[Tests/test]   |           tol=1.7923779355487587e-05) on X/EX of dimension (552, 9)\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 17/18 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 9)\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 17/18 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.82e-01\n[Tests/test]   | \r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 17/18 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [00:00<00:00,  3.33trial/s, best loss: 0.3150352735155426]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [00:00<00:00,  3.32trial/s, best loss: 0.3150352735155426]\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 18/19 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 18/19 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=9) to X of shape (552, 10)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 18/19 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 18/19 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 18/19 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=0.7145898529993686, epsilon=0, intercept_scaling=1.2784388059272316,\n[Tests/test]   |           max_iter=100000.0, random_state=0, tol=0.000115894804490335) on X/EX of dimension (552, 12)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 18/19 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 12)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 18/19 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.78e-01\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 18/19 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 19/19 [00:00<00:00, 16.62trial/s, best loss: 0.3150352735155426]\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 19/20 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 19/20 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=3) to X of shape (552, 10)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 19/20 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 19/20 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 19/20 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=0.5936379130488315, epsilon=0, intercept_scaling=0.6300936399264968,\n[Tests/test]   |           max_iter=100000.0, random_state=2, tol=3.088374777941079e-05) on X/EX of dimension (552, 6)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 19/20 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 6)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 19/20 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.88e-01\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 19/20 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:00<00:00, 21.71trial/s, best loss: 0.31182618488099345]\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 20/21 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 20/21 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=1) to X of shape (552, 10)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 20/21 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 20/21 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 20/21 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=0.5213559781558412, epsilon=0, intercept_scaling=0.5121284216447398,\n[Tests/test]   |           max_iter=100000.0, random_state=2, tol=1.2215335874388853e-05) on X/EX of dimension (552, 4)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 20/21 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 4)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 20/21 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.82e-01\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 20/21 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 21/21 [00:00<00:00, 15.88trial/s, best loss: 0.31182618488099345]\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 21/22 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 21/22 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=1) to X of shape (552, 10)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 21/22 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 21/22 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 21/22 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=0.5236086306261398, epsilon=0, intercept_scaling=1.0862333699956752,\n[Tests/test]   |           max_iter=100000.0, random_state=3, tol=2.5907199186449223e-05) on X/EX of dimension (552, 4)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 21/22 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 4)\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 21/22 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.82e-01\n[Tests/test]   | \r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 21/22 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 22/22 [00:00<00:00, 16.11trial/s, best loss: 0.31182618488099345]\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 22/23 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 22/23 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=4) to X of shape (552, 10)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 22/23 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 22/23 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 22/23 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=0.8785408698867798, epsilon=0, intercept_scaling=0.5160795485699917,\n[Tests/test]   |           max_iter=100000.0, random_state=2, tol=1.0615237547324233e-05) on X/EX of dimension (552, 7)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 22/23 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 7)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 22/23 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.86e-01\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 22/23 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 23/23 [00:00<00:00, 14.84trial/s, best loss: 0.31182618488099345]\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 23/24 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 23/24 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=3) to X of shape (552, 10)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 23/24 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 23/24 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 23/24 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=0.84561766655635, epsilon=0, intercept_scaling=0.5062659328295,\n[Tests/test]   |           max_iter=100000.0, random_state=2, tol=1.0481483123239717e-05) on X/EX of dimension (552, 6)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 23/24 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 6)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 23/24 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.88e-01\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 23/24 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:00<00:00, 14.39trial/s, best loss: 0.311725013185817]\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 24/25 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 24/25 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=3) to X of shape (552, 10)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 24/25 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 24/25 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 24/25 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=0.8432224281033329, epsilon=0, intercept_scaling=0.6119602284209699,\n[Tests/test]   |           max_iter=100000.0, random_state=2, tol=2.6928206038186536e-05) on X/EX of dimension (552, 6)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 24/25 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 6)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 24/25 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.87e-01\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 24/25 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:00<00:00, 15.56trial/s, best loss: 0.311725013185817]\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 25/26 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 25/26 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=1) to X of shape (552, 10)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 25/26 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 25/26 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 25/26 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=0.8038665807294471, epsilon=0, intercept_scaling=0.8038749165358277,\n[Tests/test]   |           max_iter=100000.0, random_state=2, tol=2.2704512832704072e-05) on X/EX of dimension (552, 4)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 25/26 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 4)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 25/26 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.80e-01\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 25/26 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26/26 [00:00<00:00, 14.93trial/s, best loss: 0.311725013185817]\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 26/27 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 26/27 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=2) to X of shape (552, 10)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 26/27 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 26/27 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 26/27 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=0.5056409698620413, epsilon=0, intercept_scaling=0.5789694178270932,\n[Tests/test]   |           max_iter=100000.0, random_state=2, tol=0.0015374850054665852) on X/EX of dimension (552, 5)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 26/27 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 5)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 26/27 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.79e-01\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 26/27 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 27/27 [00:00<00:00, 15.32trial/s, best loss: 0.311725013185817]\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 27/28 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 27/28 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=3) to X of shape (552, 10)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 27/28 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 27/28 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 27/28 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=1.1223318515020373, epsilon=0, intercept_scaling=0.7124073138306167,\n[Tests/test]   |           max_iter=100000.0, random_state=2, tol=3.523671666250358e-05) on X/EX of dimension (552, 6)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 27/28 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 6)\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 27/28 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.85e-01\n[Tests/test]   | \r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 27/28 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28/28 [00:00<00:00, 12.68trial/s, best loss: 0.311725013185817]\n[Tests/test]   | \r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 28/29 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 28/29 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=5) to X of shape (552, 10)\n[Tests/test]   | \r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 28/29 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 28/29 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 28/29 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=1.041431676201542, epsilon=0, intercept_scaling=0.5417607250613097,\n[Tests/test]   |           max_iter=100000.0, random_state=3, tol=7.77718997508331e-05) on X/EX of dimension (552, 8)\n[Tests/test]   | \r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 28/29 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 8)\n[Tests/test]   | \r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 28/29 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.79e-01\n[Tests/test]   | \r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 28/29 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29/29 [00:00<00:00, 13.66trial/s, best loss: 0.311725013185817]\n[Tests/test]   | \r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 29/30 [00:00<?, ?trial/s, best loss=?]\r                                                       \rWill use the last 0.2 portion of samples for validation\n[Tests/test]   | \r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 29/30 [00:00<?, ?trial/s, best loss=?]\r                                                       \rFitting LagSelector(lag_size=3) to X of shape (552, 10)\n[Tests/test]   | \r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 29/30 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xfit (552, 10)\n[Tests/test]   | \r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 29/30 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTransforming Xval (138, 10)\n[Tests/test]   | \r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 29/30 [00:00<?, ?trial/s, best loss=?]\r                                                       \rTraining learner LinearSVR(C=0.7955738191612758, epsilon=0, intercept_scaling=0.8422567738895119,\n[Tests/test]   |           max_iter=100000.0, random_state=2, tol=1.207819014916326e-05) on X/EX of dimension (552, 6)\n[Tests/test]   | \r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 29/30 [00:00<?, ?trial/s, best loss=?]\r                                                       \rScoring on X/EX validation of shape (138, 6)\n[Tests/test]   | \r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 29/30 [00:00<?, ?trial/s, best loss=?]\r                                                       \rOK trial with R2 score 6.86e-01\n[Tests/test]   | \n[Tests/test]   | ==== Time series forecast with SVM and lag selectors ====\n[Tests/test]   | \n[Tests/test]   | The best model found: {'learner': LinearSVR(C=0.84561766655635, epsilon=0, intercept_scaling=0.5062659328295,\n[Tests/test]   |           max_iter=100000.0, random_state=2, tol=1.0481483123239717e-05), 'preprocs': (LagSelector(lag_size=3),), 'ex_preprocs': ((),)}\n[Tests/test]   | ========================================\n[Tests/test]   | Actual parameter values\n[Tests/test]   |  lag size: 2 \n[Tests/test]   |  a: [ 0.666 -0.333] b: [ 1.5 -1.5  0.5] c: -0.5\n[Tests/test]   | ----\n[Tests/test]   | Estimated parameter values\n[Tests/test]   |  lag size: 3 \n[Tests/test]   |  a: [ 0.147 -0.004 -0.068] b: [ 1.491 -1.471  0.524] c: [-0.532]\n[Tests/test]   | ========================================\n[Tests/test]   | Best trial validation R2: 0.688274986814183\n[Tests/test]   | Test R2: 0.7334335830087015\n[Tests/test]   | \r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 29/30 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00<00:00, 14.67trial/s, best loss: 0.311725013185817]\n[Tests/test]   | Fitting LagSelector(lag_size=3) to X of shape (690, 10)\n[Tests/test]   | Transforming Xfit (690, 10)\n[Tests/test]   | Training learner LinearSVR(C=0.84561766655635, epsilon=0, intercept_scaling=0.5062659328295,\n[Tests/test]   |           max_iter=100000.0, random_state=2, tol=1.0481483123239717e-05) on X/EX of dimension (690, 6)\n[Tests/test]   | Fitting LagSelector(lag_size=3) to X of shape (300, 10)\n[Tests/test]   | Transforming Xfit (300, 10)\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.91s/trial, best loss: 0.0]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.91s/trial, best loss: 0.0]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  4.47trial/s, best loss: 0.0]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  4.45trial/s, best loss: 0.0]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  3.31trial/s, best loss: 0.0]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  3.30trial/s, best loss: 0.0]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  7.54trial/s, best loss: 0.0]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  7.48trial/s, best loss: 0.0]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,  1.04s/trial, best loss: 0.0]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,  1.04s/trial, best loss: 0.0]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.36s/trial, best loss: 0.6470371695901639]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.36s/trial, best loss: 0.6470371695901639]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02<00:00,  2.87s/trial, best loss: 0.019608231767159423]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:02<00:00,  2.87s/trial, best loss: 0.019608231767159423]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  3.78trial/s, best loss: 0.019608231767159423]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  3.77trial/s, best loss: 0.019608231767159423]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:01<00:00,  1.17s/trial, best loss: 0.008520671920325906]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:01<00:00,  1.17s/trial, best loss: 0.008520671920325906]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  2.53trial/s, best loss: 0.008520671920325906]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  2.53trial/s, best loss: 0.008520671920325906]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 35.00trial/s, best loss: 0.0]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 34.38trial/s, best loss: 0.0]\n[Tests/test]   | .\r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 33.22trial/s, best loss: 0.0]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 36.64trial/s, best loss: 0.0]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 32.93trial/s, best loss: 0.0]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 26.33trial/s, best loss: 0.0]\n[Tests/test]   | .\r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 26.84trial/s, best loss: 0.0]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 25.58trial/s, best loss: 0.0]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 23.68trial/s, best loss: 0.0]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 24.23trial/s, best loss: 0.0]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 20.38trial/s, best loss: 0.0]\n[Tests/test]   | .\r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 19.22trial/s, best loss: 0.0]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 24.39trial/s, best loss: 0.0]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 20.39trial/s, best loss: 0.0]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 30.06trial/s, best loss: 0.0]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 29.88trial/s, best loss: 1.7997990875429082e-08]\n[Tests/test]   | .\r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 27.10trial/s, best loss: 9.336975637097567e-13]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 26.60trial/s, best loss: 9.336975637097567e-13]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 30.21trial/s, best loss: 9.336975637097567e-13]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 30.65trial/s, best loss: 9.336975637097567e-13]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 13.46trial/s, best loss: 2.6828732082484485e-05]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 11.44trial/s, best loss: 6.493694471032541e-12]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 11.30trial/s, best loss: 6.493694471032541e-12]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 18.17trial/s, best loss: 6.493694471032541e-12]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  5.86trial/s, best loss: 6.493694471032541e-12]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  5.83trial/s, best loss: 6.493694471032541e-12]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 32.11trial/s, best loss: 3.02461009338284e-06]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 31.02trial/s, best loss: 8.892886427247504e-13]\n[Tests/test]   | .\r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 31.96trial/s, best loss: 8.892886427247504e-13]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 28.17trial/s, best loss: 8.892886427247504e-13]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 20.44trial/s, best loss: 8.892886427247504e-13]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.91trial/s, best loss: 1.5478550904446386e-05]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.83trial/s, best loss: 1.5478550904446386e-05]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 10.71trial/s, best loss: 1.731991538411748e-06]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 11.99trial/s, best loss: 1.731991538411748e-06]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  9.75trial/s, best loss: 1.193597443105432e-10]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  9.66trial/s, best loss: 1.193597443105432e-10]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 11.41trial/s, best loss: 1.193597443105432e-10]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 24.98trial/s, best loss: 2.9698377046472046e-07]\n[Tests/test]   | .\r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 27.31trial/s, best loss: 3.322452313270219e-10]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 30.15trial/s, best loss: 3.322452313270219e-10]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 30.20trial/s, best loss: 3.322452313270219e-10]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 25.79trial/s, best loss: 3.322452313270219e-10]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.33trial/s, best loss: 2.134042166546113e-05]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.31trial/s, best loss: 2.134042166546113e-05]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  6.57trial/s, best loss: 2.134042166546113e-05]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  6.54trial/s, best loss: 2.134042166546113e-05]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  6.27trial/s, best loss: 5.018668684408922e-06]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  6.25trial/s, best loss: 5.018668684408922e-06]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  5.96trial/s, best loss: 5.018668684408922e-06]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  5.93trial/s, best loss: 5.018668684408922e-06]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  9.19trial/s, best loss: 7.697606108081345e-08]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  9.12trial/s, best loss: 7.697606108081345e-08]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 29.48trial/s, best loss: 1.0561723717827931e-09]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 30.44trial/s, best loss: 1.0561723717827931e-09]\n[Tests/test]   | .\r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 29.16trial/s, best loss: 1.0561723717827931e-09]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 25.70trial/s, best loss: 1.0458300891968975e-12]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 31.63trial/s, best loss: 1.0458300891968975e-12]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.44trial/s, best loss: 4.236995476691874e-08]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.39trial/s, best loss: 4.236995476691874e-08]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  7.98trial/s, best loss: 7.807554047722931e-09]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  7.94trial/s, best loss: 7.807554047722931e-09]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  9.52trial/s, best loss: 7.807554047722931e-09]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  9.44trial/s, best loss: 7.807554047722931e-09]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  9.17trial/s, best loss: 7.807554047722931e-09]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  9.12trial/s, best loss: 7.807554047722931e-09]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  8.71trial/s, best loss: 7.807554047722931e-09]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  8.65trial/s, best loss: 7.807554047722931e-09]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 30.05trial/s, best loss: 0.9954410832840296]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 27.19trial/s, best loss: 0.9954410832840296]\n[Tests/test]   | .\r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 30.23trial/s, best loss: 0.9954410832840296]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 33.64trial/s, best loss: 0.9954410832840296]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 31.79trial/s, best loss: 0.9954410832840296]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 24.95trial/s, best loss: 0.9903655311703741]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 26.15trial/s, best loss: 0.9903655311703741]\n[Tests/test]   | .\r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 30.23trial/s, best loss: 0.9903655311703741]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 32.25trial/s, best loss: 0.9903655311703741]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 17.58trial/s, best loss: 0.9903655311703741]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 27.38trial/s, best loss: 0.992362527204535]\n[Tests/test]   | .\r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 24.95trial/s, best loss: 0.992362527204535]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 23.59trial/s, best loss: 0.9903655311703741]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 29.84trial/s, best loss: 0.9903655311703741]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 31.06trial/s, best loss: 0.9903655311703741]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 11.29trial/s, best loss: 0.0]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 11.52trial/s, best loss: 0.0]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  8.84trial/s, best loss: 0.0]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  8.78trial/s, best loss: 0.0]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 13.16trial/s, best loss: 0.0]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 12.46trial/s, best loss: 0.0]\n[Tests/test]   | ./tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), Lars())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.52trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), Lars())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 16.51trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), Lars())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 19.50trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), Lars())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 25.21trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), Lars())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), Lars())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 11.02trial/s, best loss: 0.0]\n[Tests/test]   | ./tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LarsCV())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.38trial/s, best loss: 0.0]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.34trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LarsCV())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 17.47trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LarsCV())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 19.06trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LarsCV())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 19.01trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LarsCV())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  8.29trial/s, best loss: 0.0]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  8.23trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LarsCV())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | ./tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LassoLars())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 25.78trial/s, best loss: 1.026713947786368]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LassoLars())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 25.99trial/s, best loss: 1.026713947786368]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LassoLars())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 25.27trial/s, best loss: 1.026713947786368]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LassoLars())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 24.26trial/s, best loss: 1.026713947786368]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LassoLars())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LassoLars())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 10.08trial/s, best loss: 1.026713947786368]\n[Tests/test]   | ./tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LassoLarsCV())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 20.64trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LassoLarsCV())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 19.01trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LassoLarsCV())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 22.69trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LassoLarsCV())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 10.14trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LassoLarsCV())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LassoLarsCV())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 19.34trial/s, best loss: 0.0]\n[Tests/test]   | ./tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 28.45trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 24.12trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 34.49trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 26.38trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 32.02trial/s, best loss: 0.0]\n[Tests/test]   | ./tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n[Tests/test]   |   ConvergenceWarning,\n[Tests/test]   | \n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  8.40trial/s, best loss: 0.05275229357798161]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  8.35trial/s, best loss: 0.05275229357798161]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n[Tests/test]   |   ConvergenceWarning,\n[Tests/test]   | \n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 14.66trial/s, best loss: 0.05275229357798161]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n[Tests/test]   |   ConvergenceWarning,\n[Tests/test]   | \n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 11.93trial/s, best loss: 0.05275229357798161]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 19.84trial/s, best loss: 0.05275229357798161]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n[Tests/test]   |   ConvergenceWarning,\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 19.25trial/s, best loss: 0.05275229357798161]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.14trial/s, best loss: 0.11794871794871786]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.12trial/s, best loss: 0.11794871794871786]\n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00,  1.05s/trial, best loss: 0.11794871794871786]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00,  1.05s/trial, best loss: 0.11794871794871786]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  1.35trial/s, best loss: 0.11794871794871786]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  1.34trial/s, best loss: 0.11794871794871786]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  1.88trial/s, best loss: 0.11794871794871786]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  1.87trial/s, best loss: 0.11794871794871786]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  1.79trial/s, best loss: 0.11794871794871786]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  1.79trial/s, best loss: 0.11794871794871786]\n[Tests/test]   | ./tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 27.56trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 30.81trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 33.25trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 27.68trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 30.24trial/s, best loss: 0.0]\n[Tests/test]   | ./tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuitCV())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 25.33trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuitCV())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | \r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 25.35trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuitCV())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 23.63trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuitCV())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 23.42trial/s, best loss: 0.0]\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuitCV())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n[Tests/test]   | If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n[Tests/test]   | \n[Tests/test]   | from sklearn.pipeline import make_pipeline\n[Tests/test]   | \n[Tests/test]   | model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuitCV())\n[Tests/test]   | \n[Tests/test]   | If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n[Tests/test]   | \n[Tests/test]   | kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n[Tests/test]   | model.fit(X, y, **kwargs)\n[Tests/test]   | \n[Tests/test]   | \n[Tests/test]   |   FutureWarning,\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | /tmp/de65f406-fe28-11ed-a890-af2cc187fc11/hyperopt-hyperopt-sklearn/.tox/py37/lib/python3.7/site-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n[Tests/test]   |   X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 24.89trial/s, best loss: 0.0]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 28.81trial/s, best loss: 0.010000000000000009]\n[Tests/test]   | .\r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 28.02trial/s, best loss: 0.0050000000000000044]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 29.03trial/s, best loss: 0.0050000000000000044]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 27.62trial/s, best loss: 0.0050000000000000044]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 27.92trial/s, best loss: 0.0050000000000000044]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 25.61trial/s, best loss: 0.0014170874532694056]\n[Tests/test]   | .\r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 29.15trial/s, best loss: 0.0003243401650846467]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 25.17trial/s, best loss: 0.00028841030389936684]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 24.42trial/s, best loss: 0.00028841030389936684]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 30.54trial/s, best loss: 0.00028841030389936684]\n[Tests/test]   | \r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 32.13trial/s, best loss: 0.06999999999999995]\n[Tests/test]   | .\r 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 24.57trial/s, best loss: 0.0]\n[Tests/test]   | \r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 27.91trial/s, best loss: 0.0]\n[Tests/test]   | \r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 31.85trial/s, best loss: 0.0]\n[Tests/test]   | \r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 25.44trial/s, best loss: 0.0]\n[Tests/test]   | .\r  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.45s/trial, best loss: 1.0659769141670206]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.45s/trial, best loss: 1.0659769141670206]\n[Tests/test]   \u274c  Failure - Main Test with tox\n[Tests/test] Error response from daemon: Could not find the file /var/run/act/workflow/pathcmd.txt in container 3090bf9cd0b7bffc6378351155f4b98dbf73e298913c581d3e58cbb3b9b1943f\n[Tests/test] failed to remove container: Error response from daemon: No such container: 3090bf9cd0b7bffc6378351155f4b98dbf73e298913c581d3e58cbb3b9b1943f\n[Tests/test] \ud83c\udfc1  Job failed\n",
    "actions_stderr": "Error: Job 'test' failed\n"
}